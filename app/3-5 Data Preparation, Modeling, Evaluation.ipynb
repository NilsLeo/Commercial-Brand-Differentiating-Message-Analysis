{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detecting BDM In Superbowl Commercials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "logging.basicConfig(\n",
    "    filename='log.log',\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S'\n",
    ")\n",
    "\n",
    "def update_df_with_csv(df, csv_filepath, merge_key):\n",
    "    \"\"\"\n",
    "    Update the dataframe by merging with data from a CSV file.\n",
    "    Prefers the incoming CSV's columns when there's a conflict.\n",
    "\n",
    "    Parameters:\n",
    "    - df: DataFrame to update.\n",
    "    - csv_filepath: Path to the CSV file to merge with.\n",
    "    - merge_key: Column name to merge on.\n",
    "\n",
    "    Returns:\n",
    "    - Updated DataFrame.\n",
    "    \"\"\"\n",
    "    # Read CSV file\n",
    "    incoming_df = pd.read_csv(csv_filepath)\n",
    "    \n",
    "    # Merge with suffixes to identify incoming columns\n",
    "    updated_df = df.merge(incoming_df, on=merge_key, how='left', suffixes=('', '_incoming'))\n",
    "    \n",
    "    # Handle conflicts: drop original columns and rename incoming columns\n",
    "    conflicting_columns = [col for col in incoming_df.columns if col in df.columns and col != merge_key]\n",
    "    for col in conflicting_columns:\n",
    "        updated_df.drop(col, axis=1, inplace=True)  # Drop the original column\n",
    "        updated_df.rename(columns={f'{col}_incoming': col}, inplace=True)  # Rename the incoming column\n",
    "    \n",
    "    return updated_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have some specific knowledge about the industry and brand, so we can use that to improve the model. This data only exists for a few brands and products. Activate or deactivate as needed.\n",
    "\n",
    "# Activate if you want to reduce the selection of commercials to 20 for debugging\n",
    "\n",
    "DEBUGGING = False\n",
    "\n",
    "CSV_PATH = 'csvs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activate if you want to completely rerun the notebook from scratch. This will delete all csvs/ saved data and start from scratch.\n",
    "RESTART_FROM_SCRATCH = False\n",
    "import shutil\n",
    "csv_dir = \"./csvs\"\n",
    "if RESTART_FROM_SCRATCH and os.path.exists(csv_dir):\n",
    "    shutil.rmtree(csv_dir)\n",
    "    logging.info(f\"Directory '{csv_dir}' has been deleted.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkpoints\n",
    "# Since the model takes very long to train, we save extracted features to csvs and only rerun the code if the csv \"checkpoint\" does not exist\n",
    "\n",
    "BASELINE_CHECKPOINT = os.path.exists(f'{CSV_PATH}/baseline.csv')\n",
    "logging.info(f\"Baseline checkpoint: {BASELINE_CHECKPOINT}\")\n",
    "TRANSCRIPT_CHECKPOINT = os.path.exists(f'{CSV_PATH}/transcript.csv')\n",
    "logging.info(f\"Transcript checkpoint: {TRANSCRIPT_CHECKPOINT}\")\n",
    "OCR_CHECKPOINT = os.path.exists(f'{CSV_PATH}/ocr.csv')\n",
    "logging.info(f\"OCR checkpoint: {OCR_CHECKPOINT}\")\n",
    "BDM_WORDS_CHECKPOINT = os.path.exists(f'{CSV_PATH}/bdm_words.csv')\n",
    "logging.info(f\"BDM words checkpoint: {BDM_WORDS_CHECKPOINT}\")\n",
    "ADJ_NOUN_PAIRS_CHECKPOINT = os.path.exists(f'{CSV_PATH}/adj_noun_pairs.csv')\n",
    "logging.info(f\"Adj noun pairs checkpoint: {ADJ_NOUN_PAIRS_CHECKPOINT}\")\n",
    "PRODUCT_SEMANTIC_SIMILARITY_CHECKPOINT = os.path.exists(f'{CSV_PATH}/product_semantic_similarity.csv')\n",
    "logging.info(f\"Product semantic similarity checkpoint: {PRODUCT_SEMANTIC_SIMILARITY_CHECKPOINT}\")\n",
    "BRAND_SEMANTIC_SIMILARITY_CHECKPOINT = os.path.exists(f'{CSV_PATH}/brand_semantic_similarity.csv')\n",
    "logging.info(f\"Brand semantic similarity checkpoint: {BRAND_SEMANTIC_SIMILARITY_CHECKPOINT}\")\n",
    "PERSONAL_PRONOUNS_CHECKPOINT = os.path.exists(f'{CSV_PATH}/personal_pronouns.csv')\n",
    "logging.info(f\"Personal pronouns checkpoint: {PERSONAL_PRONOUNS_CHECKPOINT}\")\n",
    "COMPARISONS_CHECKPOINT = os.path.exists(f'{CSV_PATH}/comparisons.csv')\n",
    "logging.info(f\"Comparisons checkpoint: {COMPARISONS_CHECKPOINT}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CRISP-DM 3: Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def download_videos():\n",
    "  url = 'https://box.fu-berlin.de/s/zwxKp8PXkCwAwGe/download'\n",
    "  download_filename = 'downloaded_archive.zip'\n",
    "  target_directory = 'ADs'\n",
    "  os.system(f'wget -O {download_filename} {url}')\n",
    "  os.makedirs(target_directory, exist_ok=True)\n",
    "  os.system(f'unzip -o {download_filename} -d {target_directory}')\n",
    "  os.remove(download_filename)\n",
    "  logging.info(f\"Archive extracted to {target_directory} and {download_filename} removed.\")\n",
    "\n",
    "if not BASELINE_CHECKPOINT:\n",
    "  pass\n",
    "  # download_videos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ad_df = pd.DataFrame()\n",
    "if not BASELINE_CHECKPOINT:\n",
    "    BDM_excel = pd.read_excel('BDM.xlsx')\n",
    "    final_excel = pd.read_excel('previous_project_results.xlsx')\n",
    "    final_excel = final_excel.merge(\n",
    "    BDM_excel[['AdNumber', 'BDM']], \n",
    "    on='AdNumber', \n",
    "    how='left',\n",
    "    suffixes=('_old', '')\n",
    "    ).drop('BDM_old', axis=1, errors='ignore')\n",
    "    ad_df = final_excel\n",
    "    ad_df = ad_df[['cont_primary_product_type', 'BRAND', 'AdNumber', 'cont_com_appeal', 'cont_csr_type', 'Emotion_from_Dialogue', 'BDM']]\n",
    "    ad_df = ad_df.rename(columns={'cont_primary_product_type': 'product_category', 'BRAND': 'brand', 'AdNumber': 'commercial_number', 'cont_com_appeal': 'commercial_appeal', 'cont_csr_type': 'csr_type', 'Emotion_from_Dialogue': 'emotion_from_dialogue'})  # Changed here\n",
    "    ad_df.head(10)\n",
    "    # Manual coding by the marketing team: 1 = rational, 2 = balanced, 3 = emotional\n",
    "    sentiment_columns = ad_df[\"commercial_appeal\"]\n",
    "\n",
    "    # Sentiment analysis from the last project group for audio transcription\n",
    "    # Encoding 0: p < 0.8 = neutral\n",
    "    # Encoding 1: p > 0.8 = emotional\n",
    "\n",
    "    emotion_columns = ad_df[\"emotion_from_dialogue\"]\n",
    "\n",
    "    # List of values to be encoded as 1\n",
    "    target_emotions = ['love', 'joy', 'surprise', 'sadness', 'anger', 'fear']\n",
    "    ad_df.loc[:, 'encoded_emotion'] = emotion_columns.apply(lambda x: 1 if x in target_emotions else 0)  # Changed here\n",
    "    ad_df = ad_df.drop(['emotion_from_dialogue', 'commercial_appeal'], axis=1)\n",
    "\n",
    "    # List of commercial numbers to update\n",
    "    commercial_numbers = ['AD0262', 'AD0284', 'AD0332', 'AD0348', 'AD0370', 'AD0375', 'AD0399', 'AD0482', 'AD0539', 'AD0749']\n",
    "    ad_df.loc[ad_df['commercial_number'].isin(commercial_numbers), 'BDM'] = 1.0\n",
    "\n",
    "    # drop all rows with no commercial number\n",
    "    ad_df = ad_df[ad_df['commercial_number'].notna()]\n",
    "    display(ad_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brand Keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not BASELINE_CHECKPOINT:\n",
    "        product_brand_df = pd.read_csv(\"product_brands.csv\")\n",
    "\n",
    "        product_brand_df['brand'] = product_brand_df['brand'].str.replace(' ', '').str.lower()\n",
    "        ad_df['brand_clean'] = ad_df['brand'].str.replace(' ', '').str.lower()\n",
    "\n",
    "        ad_df = ad_df.merge(\n",
    "            product_brand_df[['brand', 'product_brand_keywords']], \n",
    "            left_on='brand_clean',\n",
    "            right_on='brand',\n",
    "            how='left',\n",
    "            suffixes=('', '_brand')\n",
    "        )\n",
    "\n",
    "        ad_df.drop(['brand_clean', 'brand_brand'], axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Product Category Keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not BASELINE_CHECKPOINT:\n",
    "        product_brands_df = pd.read_csv(\"product_categories.csv\")\n",
    "        product_brands_df = product_brands_df.drop('product_cat_id', axis=1)\n",
    "        ad_df = ad_df.drop('product_category', axis=1)\n",
    "        display(product_brands_df)\n",
    "        display(ad_df)\n",
    "        brand_to_info = {}\n",
    "        for _, row in product_brands_df.iterrows():\n",
    "            brands = eval(row['product_cat_brands'])\n",
    "            for brand in brands:\n",
    "                brand = brand.replace(' ', '').lower()\n",
    "                brand_to_info[brand] = {col: row[col] for col in product_brands_df.columns}\n",
    "\n",
    "        def find_brand_info(brand):\n",
    "            if pd.isna(brand):\n",
    "                return None\n",
    "            clean_brand = brand.replace(' ', '').lower()\n",
    "            return brand_to_info.get(clean_brand)\n",
    "\n",
    "        for col in product_brands_df.columns:\n",
    "            ad_df[col] = ad_df['brand'].apply(lambda x: find_brand_info(x)[col] if find_brand_info(x) else None)\n",
    "\n",
    "        unmapped_brands = ad_df[ad_df['product_cat_name'].isna()]['brand'].unique()\n",
    "        if len(unmapped_brands) > 0:\n",
    "            logging.info(\"Brands without category mapping:\")\n",
    "            for brand in unmapped_brands:\n",
    "                logging.info(f\"- {brand}\")\n",
    "\n",
    "        ad_df.head(10)\n",
    "        # drop all where product_cat_name is nan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debugging\n",
    "if not BASELINE_CHECKPOINT:\n",
    "    if DEBUGGING:\n",
    "        ad_df = ad_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASELINE_COLUMNS = ad_df.columns\n",
    "if not BASELINE_CHECKPOINT:\n",
    "  os.makedirs(f'{CSV_PATH}', exist_ok=True)\n",
    "  ad_df[BASELINE_COLUMNS].to_csv(f'{CSV_PATH}/baseline.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ad_df = pd.read_csv(f'{CSV_PATH}/baseline.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature: Transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from pathlib import Path\n",
    "from transcript import transcribe_video\n",
    "from ocr import ocr\n",
    "TRANSCRIPT_COLUMNS = ['transcript']\n",
    "if not TRANSCRIPT_CHECKPOINT:\n",
    "    ads_dir = \"ADs\"\n",
    "    def find_video_file(commercial_number, ads_dir):\n",
    "        \"\"\"Find the video file path for a given commercial number.\"\"\"\n",
    "        # Search recursively for MP4 files\n",
    "        pattern = f\"{ads_dir}/**/{commercial_number}.mp4\"\n",
    "        matches = glob.glob(pattern, recursive=True)\n",
    "        return matches[0] if matches else None\n",
    "\n",
    "    # Initialize a list to collect indices of rows to drop\n",
    "    rows_to_drop = []\n",
    "\n",
    "    for idx, row in ad_df.iterrows():\n",
    "        transcript = ' '\n",
    "        commercial_number = row['commercial_number']\n",
    "        video_path = find_video_file(commercial_number, ads_dir)\n",
    "        \n",
    "        if video_path:\n",
    "            transcript = transcribe_video(video_path)\n",
    "            ad_df.at[idx, 'transcript'] = transcript\n",
    "        else:\n",
    "            logging.info(f\"Video not found for commercial {commercial_number}\")\n",
    "            # Collect index of row to drop\n",
    "            rows_to_drop.append(idx)\n",
    "\n",
    "    # Drop the rows where no video was found\n",
    "    ad_df = ad_df.drop(rows_to_drop)\n",
    "    ad_df[TRANSCRIPT_COLUMNS + ['commercial_number']].to_csv(f'{CSV_PATH}/transcript.csv', index=False)\n",
    "    ad_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ad_df = update_df_with_csv(ad_df, f'{CSV_PATH}/transcript.csv', 'commercial_number')\n",
    "ad_df['transcript'] = ad_df['transcript'].fillna('')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature: OCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from pathlib import Path\n",
    "from transcript import transcribe_video\n",
    "from ocr import ocr\n",
    "OCR_COLUMNS = ['ocr_text']\n",
    "if not OCR_CHECKPOINT:\n",
    "    ads_dir = \"ADs\"\n",
    "    def find_video_file(commercial_number, ads_dir):\n",
    "        \"\"\"Find the video file path for a given commercial number.\"\"\"\n",
    "        # Search recursively for MP4 files\n",
    "        pattern = f\"{ads_dir}/**/{commercial_number}.mp4\"\n",
    "        matches = glob.glob(pattern, recursive=True)\n",
    "        return matches[0] if matches else None\n",
    "    for idx, row in ad_df.iterrows():\n",
    "        ocr_text = ' '\n",
    "        commercial_number = row['commercial_number']\n",
    "        video_path = find_video_file(commercial_number, ads_dir)\n",
    "        \n",
    "        if video_path:\n",
    "            if DEBUGGING:\n",
    "                ocr_text = ' '\n",
    "            else:\n",
    "                ocr_text = ocr(video_path)\n",
    "        else:\n",
    "            logging.info(f\"Video not found for commercial {commercial_number}\")\n",
    "\n",
    "        ad_df.at[idx, 'ocr_text'] = ocr_text\n",
    "\n",
    "\n",
    "    ad_df[OCR_COLUMNS + ['commercial_number']].to_csv(f'{CSV_PATH}/ocr.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "ad_df = update_df_with_csv(ad_df, f'{CSV_PATH}/ocr.csv', 'commercial_number')\n",
    "\n",
    "ad_df['ocr_text'] = ad_df['ocr_text'].fillna('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature: Superlatives, Comparatives, Uniqueness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import text_analysis as ta\n",
    "\n",
    "BDM_WORDS_COLUMNS = ['transcript_superlatives', 'transcript_comparatives', 'transcript_unique_words', 'transcript_superlative_count', 'transcript_comparative_count', 'transcript_uniqueness_count', 'transcript_superlative_pct', 'transcript_comparative_pct', 'transcript_uniqueness_pct', 'transcript_total_bdm_terms_count', 'transcript_total_bdm_terms_pct', 'ocr_text_superlatives', 'ocr_text_comparatives', 'ocr_text_unique_words', 'ocr_text_superlative_count', 'ocr_text_comparative_count', 'ocr_text_uniqueness_count', 'ocr_text_superlative_pct', 'ocr_text_comparative_pct', 'ocr_text_uniqueness_pct', 'ocr_text_total_bdm_terms_count', 'ocr_text_total_bdm_terms_pct']\n",
    "\n",
    "\n",
    "if not BDM_WORDS_CHECKPOINT:\n",
    "    ad_df = ta.process_text_data(ad_df, 'transcript')\n",
    "    ad_df = ta.process_text_data(ad_df, 'ocr_text')\n",
    "\n",
    "    columns = ['commercial_number'] + BDM_WORDS_COLUMNS\n",
    "    ad_df[columns].to_csv(f'{CSV_PATH}/bdm_words.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "ad_df = update_df_with_csv(ad_df, f'{CSV_PATH}/bdm_words.csv', 'commercial_number')\n",
    "\n",
    "# fillna object columns with empty string, float columns with 0\n",
    "for column in ad_df[BDM_WORDS_COLUMNS].columns:\n",
    "    if ad_df[column].dtype == 'object':\n",
    "        ad_df[column] = ad_df[column].fillna('')\n",
    "    elif pd.api.types.is_numeric_dtype(ad_df[column]):\n",
    "        ad_df[column] = ad_df[column].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature: Nomen + Adjektive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "ADJ_NOUN_PAIRS_COLUMNS = ['transcript_adj_noun_pairs', 'transcript_num_adj_noun_pairs', 'ocr_text_adj_noun_pairs', 'ocr_text_num_adj_noun_pairs']\n",
    "if not ADJ_NOUN_PAIRS_CHECKPOINT:\n",
    "  ad_df[\"transcript_adj_noun_pairs\"] = ad_df[\"transcript\"].apply(ta.extract_adj_noun_pairs)\n",
    "  ad_df[\"transcript_num_adj_noun_pairs\"] = ad_df[\"transcript_adj_noun_pairs\"].apply(len)\n",
    "  ad_df[\"ocr_text_adj_noun_pairs\"] = ad_df[\"ocr_text\"].apply(ta.extract_adj_noun_pairs)\n",
    "  ad_df[\"ocr_text_num_adj_noun_pairs\"] = ad_df[\"ocr_text_adj_noun_pairs\"].apply(len)\n",
    "  ad_df[ADJ_NOUN_PAIRS_COLUMNS + ['commercial_number']].to_csv(f'{CSV_PATH}/adj_noun_pairs.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "ad_df = update_df_with_csv(ad_df, f'{CSV_PATH}/adj_noun_pairs.csv', 'commercial_number')\n",
    "for column in ad_df[ADJ_NOUN_PAIRS_COLUMNS].columns:\n",
    "    if ad_df[column].dtype == 'object':\n",
    "        ad_df[column] = ad_df[column].fillna('')\n",
    "    elif pd.api.types.is_numeric_dtype(ad_df[column]):\n",
    "        ad_df[column] = ad_df[column].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature: Semantische Nähe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(ad_df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import logging\n",
    "import text_analysis as ta\n",
    "\n",
    "# Function to calculate semantic similarities and log top keywords\n",
    "PRODUCT_SEMANTIC_SIMILARITY_COLUMNS = ['transcript_product_cat_keywords_similarity', 'transcript_product_cat_keywords_top_keywords', 'ocr_text_product_cat_keywords_similarity', 'ocr_text_product_cat_keywords_top_keywords']\n",
    "BRAND_SEMANTIC_SIMILARITY_COLUMNS = ['transcript_product_brand_keywords_similarity', 'transcript_product_brand_keywords_top_keywords', 'ocr_text_product_brand_keywords_similarity', 'ocr_text_product_brand_keywords_top_keywords']\n",
    "\n",
    "if not PRODUCT_SEMANTIC_SIMILARITY_CHECKPOINT:\n",
    "       ad_df = ta.calculate_semantic_similarities(ad_df, 'transcript', 'product_cat_keywords')\n",
    "       ad_df = ta.calculate_semantic_similarities(ad_df, 'ocr_text',  'product_cat_keywords')\n",
    "if not BRAND_SEMANTIC_SIMILARITY_CHECKPOINT:\n",
    "       ad_df = ta.calculate_semantic_similarities(ad_df, 'transcript',  'product_brand_keywords')\n",
    "       ad_df = ta.calculate_semantic_similarities(ad_df, 'ocr_text', 'product_brand_keywords')\n",
    "\n",
    "\n",
    "\n",
    "       ad_df[PRODUCT_SEMANTIC_SIMILARITY_COLUMNS + ['commercial_number']].to_csv(f'{CSV_PATH}/product_semantic_similarity.csv', index=False)\n",
    "       ad_df[BRAND_SEMANTIC_SIMILARITY_COLUMNS + ['commercial_number']].to_csv(f'{CSV_PATH}/brand_semantic_similarity.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "ad_df = update_df_with_csv(ad_df, f'{CSV_PATH}/product_semantic_similarity.csv', 'commercial_number')\n",
    "for column in ad_df[PRODUCT_SEMANTIC_SIMILARITY_COLUMNS].columns:\n",
    "    if ad_df[column].dtype == 'object':\n",
    "        ad_df[column] = ad_df[column].fillna('')\n",
    "    elif pd.api.types.is_numeric_dtype(ad_df[column]):\n",
    "        ad_df[column] = ad_df[column].fillna(0)\n",
    "ad_df = update_df_with_csv(ad_df, f'{CSV_PATH}/brand_semantic_similarity.csv', 'commercial_number')\n",
    "for column in ad_df[BRAND_SEMANTIC_SIMILARITY_COLUMNS].columns:\n",
    "    if ad_df[column].dtype == 'object':\n",
    "        ad_df[column] = ad_df[column].fillna('')\n",
    "\n",
    "output_df = ad_df.copy()\n",
    "# drop empty values\n",
    "output_df = output_df.dropna()\n",
    "output_df.to_csv(f'{CSV_PATH}/ad_df.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature: Zahlenvergleiche"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test---------------------------\n",
    "#text = \"\"\"Our product is 10 times faster and 50% more efficient. \"\"\"\n",
    "COMPARISONS_COLUMNS = ['transcript_comparisons', 'ocr_text_comparisons', 'transcript_num_comparisons', 'ocr_text_num_comparisons']\n",
    "if not COMPARISONS_CHECKPOINT:\n",
    "    ad_df[\"transcript_comparisons\"] = ad_df[\"transcript\"].apply(ta.apply_on_transcript)\n",
    "    ad_df[\"ocr_text_comparisons\"] = ad_df[\"ocr_text\"].apply(ta.apply_on_transcript)\n",
    "    ad_df[\"transcript_num_comparisons\"] = ad_df[\"transcript_comparisons\"].apply(len)\n",
    "    ad_df[\"ocr_text_num_comparisons\"] = ad_df[\"ocr_text_comparisons\"].apply(len)\n",
    "    ad_df[COMPARISONS_COLUMNS + ['commercial_number']].to_csv(f'{CSV_PATH}/comparisons.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "ad_df = update_df_with_csv(ad_df, f'{CSV_PATH}/comparisons.csv', 'commercial_number')\n",
    "for column in ad_df[COMPARISONS_COLUMNS].columns:\n",
    "    if ad_df[column].dtype == 'object':\n",
    "        ad_df[column] = ad_df[column].fillna('')\n",
    "    elif pd.api.types.is_numeric_dtype(ad_df[column]):\n",
    "        ad_df[column] = ad_df[column].fillna(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature: Personalpronomen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import text_analysis as ta\n",
    "PERSONAL_PRONOUNS_COLUMNS = [\n",
    "               'commercial_number', \n",
    "               'transcript_contains_i', \n",
    "               'ocr_text_contains_i', \n",
    "               'transcript_contains_we', \n",
    "               'ocr_text_contains_we', \n",
    "               'transcript_contains_you', \n",
    "               'ocr_text_contains_you', \n",
    "               'transcript_contains_he', \n",
    "               'ocr_text_contains_he', \n",
    "               'transcript_contains_she', \n",
    "               'ocr_text_contains_she', \n",
    "                'transcript_contains_it', \n",
    "                'ocr_text_contains_it', \n",
    "                'transcript_contains_they', \n",
    "                'ocr_text_contains_they', \n",
    "                ]\n",
    "\n",
    "if not PERSONAL_PRONOUNS_CHECKPOINT:\n",
    "    ad_df['transcript_contains_i'] = ad_df['transcript'].apply(ta.contains_i)\n",
    "    ad_df['ocr_text_contains_i'] = ad_df['ocr_text'].apply(ta.contains_i)\n",
    "    ad_df['transcript_contains_we'] = ad_df['transcript'].apply(ta.contains_we)\n",
    "    ad_df['ocr_text_contains_we'] = ad_df['ocr_text'].apply(ta.contains_we)\n",
    "    ad_df['transcript_contains_you'] = ad_df['transcript'].apply(ta.contains_you)\n",
    "    ad_df['ocr_text_contains_you'] = ad_df['ocr_text'].apply(ta.contains_you)\n",
    "    ad_df['transcript_contains_he'] = ad_df['transcript'].apply(ta.contains_he)\n",
    "    ad_df['ocr_text_contains_he'] = ad_df['ocr_text'].apply(ta.contains_he)\n",
    "    ad_df['transcript_contains_she'] = ad_df['transcript'].apply(ta.contains_she)\n",
    "    ad_df['ocr_text_contains_she'] = ad_df['ocr_text'].apply(ta.contains_she)\n",
    "    ad_df['transcript_contains_it'] = ad_df['transcript'].apply(ta.contains_it)\n",
    "    ad_df['ocr_text_contains_it'] = ad_df['ocr_text'].apply(ta.contains_it)\n",
    "    ad_df['transcript_contains_they'] = ad_df['transcript'].apply(ta.contains_they)\n",
    "    ad_df['ocr_text_contains_they'] = ad_df['ocr_text'].apply(ta.contains_they)\n",
    "\n",
    "    ad_df[PERSONAL_PRONOUNS_COLUMNS + ['commercial_number']].to_csv(f'{CSV_PATH}/personal_pronouns.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "ad_df = update_df_with_csv(ad_df, f'{CSV_PATH}/personal_pronouns.csv', 'commercial_number')\n",
    "for column in ad_df[PERSONAL_PRONOUNS_COLUMNS].columns:\n",
    "  ad_df[column] = ad_df[column].fillna(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic function to check for missing values\n",
    "def check_missing_values(df, check_for_empty_string=False):\n",
    "    missing_summary = df.isnull().sum()\n",
    "    display(missing_summary[missing_summary > 0])  # Display only columns with missing values\n",
    "    if df.isnull().sum().sum() == 0:\n",
    "        print(\"✅ No missing values found\")\n",
    "    else:\n",
    "        print(\"❌\")\n",
    "        display(df[df.isnull().any(axis=1)])\n",
    "        if check_for_empty_string:\n",
    "            missing_rows = df[df.isnull().any(axis=1)]\n",
    "            display(missing_rows[['commercial_number'] + list(missing_summary[missing_summary > 0].index)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "BRAND_SPECIFIC_COLUMNS = [\n",
    "                         'transcript_product_brand_keywords_similarity',\n",
    "                         'ocr_text_product_brand_keywords_similarity',                         \n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "INDUSTRY_SPECIFIC_COLUMNS = [\n",
    "                             'transcript_product_cat_keywords_similarity', \n",
    "                             'ocr_text_product_cat_keywords_similarity', \n",
    "                             ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "ad_df.loc[ad_df['ocr_text'].isna(), 'ocr_text'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import models as m\n",
    "ad_df = m.prepare_df_for_modeling(ad_df)\n",
    "# get rid of all brand and industry specific columns \n",
    "vanilla_df = ad_df.drop(columns=BRAND_SPECIFIC_COLUMNS + INDUSTRY_SPECIFIC_COLUMNS)\n",
    "\n",
    "brand_specific_df = ad_df[ad_df['transcript_product_brand_keywords_similarity'].notna()]\n",
    "brand_specific_df = brand_specific_df.drop(columns=INDUSTRY_SPECIFIC_COLUMNS)\n",
    "\n",
    "industry_specific_df = ad_df[ad_df['transcript_product_cat_keywords_similarity'].notna()]\n",
    "industry_specific_df = industry_specific_df.drop(columns=BRAND_SPECIFIC_COLUMNS)\n",
    "\n",
    "industry_and_brand_specific_df = ad_df[ad_df['transcript_product_cat_keywords_similarity'].notna() & ad_df['transcript_product_brand_keywords_similarity'].notna()]\n",
    "\n",
    "datasets = [\n",
    "  {\"name\": \"vanilla\", \"df\": vanilla_df, \"brand_specific\": False, \"industry_specific\": False},\n",
    "  {\"name\": \"industry_specific\", \"df\": industry_specific_df, \"brand_specific\": False, \"industry_specific\": True},\n",
    "  {\"name\": \"brand_specific\", \"df\": brand_specific_df, \"brand_specific\": True, \"industry_specific\": False},\n",
    "  {\"name\": \"industry_and_brand_specific\", \"df\": industry_and_brand_specific_df, \"brand_specific\": True, \"industry_specific\": True}\n",
    "]\n",
    "\n",
    "for dataset in datasets:\n",
    "  dataset[\"df\"].to_csv(f'{CSV_PATH}/{dataset[\"name\"]}.csv', index=False)\n",
    "for dataset in datasets:\n",
    "  check_missing_values(dataset[\"df\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CRISP-DM 4: Modeling, 5 Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from IPython.display import Markdown\n",
    "import models as m\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for dataset in datasets:\n",
    "    display(Markdown(f\"# {dataset['name']} Model\"))\n",
    "    check_missing_values(dataset[\"df\"])\n",
    "    ad_df = dataset[\"df\"]\n",
    "    commercial_numbers = ad_df['commercial_number']\n",
    "    ad_df = ad_df.drop(columns=['commercial_number'])\n",
    "    original_data = ad_df.copy()\n",
    "\n",
    "    target = ad_df['BDM']\n",
    "    data = ad_df\n",
    "    data.drop(columns=['BDM'], inplace=True)\n",
    "    display(data.head(10))\n",
    "\n",
    "\n",
    "    X_train, X_test, y_train, y_test = m.train_test_split(data, target, test_size=0.2, random_state=42)\n",
    "\n",
    "    base_models = m.get_base_models()\n",
    "    param_distributions = m.get_param_distributions()\n",
    "    tuned_models = m.tune_models(data, target, base_models, param_distributions)\n",
    "\n",
    "\n",
    "\n",
    "    trained_models = m.train_models(data, target, tuned_models, dataset[\"industry_specific\"], dataset[\"brand_specific\"])\n",
    "\n",
    "    display(Markdown(f\"## 5 Evaluation\"))\n",
    "    \n",
    "    results_df, predictions = m.evaluate_models(data, target, trained_models)\n",
    "    m.display_model_results(data, target, trained_models, results_df, predictions)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
