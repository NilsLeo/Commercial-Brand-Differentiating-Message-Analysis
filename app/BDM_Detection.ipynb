{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detecting BDM In Superbowl Commercials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "logging.basicConfig(\n",
    "    filename='log.log',\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have some specific knowledge about the industry and brand, so we can use that to improve the model. This data only exists for a few brands and products. Activate or deactivate as needed.\n",
    "INDUSTRY_SPECIFIC_AWARENESS = True\n",
    "BRAND_SPECIFIC_AWARENESS = True\n",
    "\n",
    "# Activate if you want to reduce the selection of commercials to 20 for debugging\n",
    "REDUCED_SELECTION = True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkpoints\n",
    "# Since the model takes very long to train, we save extracted features to csvs and only rerun the code if the csv \"checkpoint\" does not exist\n",
    "\n",
    "BASELINE_CHECKPOINT = os.path.exists('csvs/baseline.csv')\n",
    "logging.info(f\"Baseline checkpoint: {BASELINE_CHECKPOINT}\")\n",
    "TRANSCRIPT_CHECKPOINT = os.path.exists('csvs/transcript.csv')\n",
    "logging.info(f\"Transcript checkpoint: {TRANSCRIPT_CHECKPOINT}\")\n",
    "OCR_CHECKPOINT = os.path.exists('csvs/ocr.csv')\n",
    "logging.info(f\"OCR checkpoint: {OCR_CHECKPOINT}\")\n",
    "BDM_WORDS_CHECKPOINT = os.path.exists('csvs/bdm_words.csv')\n",
    "logging.info(f\"BDM words checkpoint: {BDM_WORDS_CHECKPOINT}\")\n",
    "ADJ_NOUN_PAIRS_CHECKPOINT = os.path.exists('csvs/adj_noun_pairs.csv')\n",
    "logging.info(f\"Adj noun pairs checkpoint: {ADJ_NOUN_PAIRS_CHECKPOINT}\")\n",
    "SEMANTIC_SIMILARITY_CHECKPOINT = os.path.exists('csvs/semantic_similarity.csv')\n",
    "logging.info(f\"Semantic similarity checkpoint: {SEMANTIC_SIMILARITY_CHECKPOINT}\")\n",
    "PERSONAL_PRONOUNS_CHECKPOINT = os.path.exists('csvs/personal_pronouns.csv')\n",
    "logging.info(f\"Personal pronouns checkpoint: {PERSONAL_PRONOUNS_CHECKPOINT}\")\n",
    "# Activate if you want to completely rerun the notebook from scratch. This will delete all csvs/ saved data and start from scratch.\n",
    "RESTART_FROM_SCRATCH = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "csv_dir = \"./csvs\"\n",
    "if RESTART_FROM_SCRATCH and os.path.exists(csv_dir):\n",
    "    shutil.rmtree(csv_dir)\n",
    "    logging.info(f\"Directory '{csv_dir}' has been deleted.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CRISP-DM 3: Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def download_videos():\n",
    "  url = 'https://box.fu-berlin.de/s/zwxKp8PXkCwAwGe/download'\n",
    "  download_filename = 'downloaded_archive.zip'\n",
    "  target_directory = 'ADs'\n",
    "  os.system(f'wget -O {download_filename} {url}')\n",
    "  os.makedirs(target_directory, exist_ok=True)\n",
    "  os.system(f'unzip -o {download_filename} -d {target_directory}')\n",
    "  os.remove(download_filename)\n",
    "  logging.info(f\"Archive extracted to {target_directory} and {download_filename} removed.\")\n",
    "\n",
    "if not BASELINE_CHECKPOINT:\n",
    "  download_videos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not BASELINE_CHECKPOINT:\n",
    "    BDM_excel = pd.read_excel('BDM.xlsx')\n",
    "    final_excel = pd.read_excel('previous_project_results.xlsx')\n",
    "    final_excel = final_excel.merge(\n",
    "    BDM_excel[['AdNumber', 'BDM']], \n",
    "    on='AdNumber', \n",
    "    how='left',\n",
    "    suffixes=('_old', '')\n",
    "    ).drop('BDM_old', axis=1, errors='ignore')\n",
    "\n",
    "    ad_df = final_excel.groupby(['cont_primary_product_type', 'BRAND', 'AdNumber', \"BDM\"]).size().reset_index(name='count')\n",
    "    ad_df.rename(columns={'cont_primary_product_type': 'product_category', 'BRAND': 'brand', 'AdNumber': 'commercial_number'}, inplace=True)\n",
    "    ad_df.drop(columns=['count'], inplace=True)\n",
    "    ad_df.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brand Keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not BASELINE_CHECKPOINT:\n",
    "    if BRAND_SPECIFIC_AWARENESS:\n",
    "        product_brand_df = pd.read_csv(\"product_brands.csv\")\n",
    "\n",
    "        product_brand_df['brand'] = product_brand_df['brand'].str.replace(' ', '').str.lower()\n",
    "        ad_df['brand_clean'] = ad_df['brand'].str.replace(' ', '').str.lower()\n",
    "\n",
    "        ad_df = ad_df.merge(\n",
    "            product_brand_df[['brand', 'product_brand_keywords']], \n",
    "            left_on='brand_clean',\n",
    "            right_on='brand',\n",
    "            how='left',\n",
    "            suffixes=('', '_brand')\n",
    "        )\n",
    "\n",
    "        ad_df.drop(['brand_clean', 'brand_brand'], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "        ad_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Product Category Keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not BASELINE_CHECKPOINT:\n",
    "    if INDUSTRY_SPECIFIC_AWARENESS:\n",
    "        product_brands_df = pd.read_csv(\"product_categories.csv\")\n",
    "        product_brands_df.head(40)\n",
    "        product_brands_df = product_brands_df.drop('product_cat_id', axis=1)\n",
    "        ad_df = ad_df.drop('product_category', axis=1)\n",
    "        display(product_brands_df)\n",
    "        display(ad_df)\n",
    "        brand_to_info = {}\n",
    "        for _, row in product_brands_df.iterrows():\n",
    "            brands = eval(row['product_cat_brands'])\n",
    "            for brand in brands:\n",
    "                brand = brand.replace(' ', '').lower()\n",
    "                brand_to_info[brand] = {col: row[col] for col in product_brands_df.columns}\n",
    "\n",
    "        def find_brand_info(brand):\n",
    "            clean_brand = brand.replace(' ', '').lower()\n",
    "            return brand_to_info.get(clean_brand)\n",
    "\n",
    "        for col in product_brands_df.columns:\n",
    "            ad_df[col] = ad_df['brand'].apply(lambda x: find_brand_info(x)[col] if find_brand_info(x) else None)\n",
    "\n",
    "        unmapped_brands = ad_df[ad_df['product_cat_name'].isna()]['brand'].unique()\n",
    "        if len(unmapped_brands) > 0:\n",
    "            logging.info(\"Brands without category mapping:\")\n",
    "            for brand in unmapped_brands:\n",
    "                logging.info(f\"- {brand}\")\n",
    "\n",
    "        ad_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not BASELINE_CHECKPOINT:\n",
    "  if REDUCED_SELECTION:\n",
    "    ad_df = ad_df.head(20)\n",
    "  directory = 'csvs'\n",
    "  if not os.path.exists(directory):\n",
    "      os.makedirs(directory)\n",
    "  ad_df.to_csv('csvs/baseline.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature: Transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ad_df = pd.read_csv('csvs/baseline.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from pathlib import Path\n",
    "from transcript import transcribe_video\n",
    "from ocr import ocr\n",
    "\n",
    "if not TRANSCRIPT_CHECKPOINT:\n",
    "    ads_dir = \"ADs\"\n",
    "    def find_video_file(commercial_number, ads_dir):\n",
    "        \"\"\"Find the video file path for a given commercial number.\"\"\"\n",
    "        # Search recursively for MP4 files\n",
    "        pattern = f\"{ads_dir}/**/{commercial_number}.mp4\"\n",
    "        matches = glob.glob(pattern, recursive=True)\n",
    "        return matches[0] if matches else None\n",
    "\n",
    "    ad_df['transcript'] = ''\n",
    "\n",
    "    for idx, row in ad_df.iterrows():\n",
    "        commercial_number = row['commercial_number']\n",
    "        video_path = find_video_file(commercial_number, ads_dir)\n",
    "        \n",
    "        if video_path:\n",
    "            transcript = transcribe_video(video_path)\n",
    "            ad_df.at[idx, 'transcript'] = transcript\n",
    "        else:\n",
    "            logging.info(f\"Video not found for commercial {commercial_number}\")\n",
    "    ad_df[['commercial_number', 'transcript']].to_csv('csvs/transcript.csv', index=False)\n",
    "    ad_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature: OCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "transcript_df = pd.read_csv('csvs/transcript.csv')\n",
    "ad_df = ad_df.merge(transcript_df, on='commercial_number', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from pathlib import Path\n",
    "from transcript import transcribe_video\n",
    "from ocr import ocr\n",
    "\n",
    "if not OCR_CHECKPOINT:\n",
    "    ads_dir = \"ADs\"\n",
    "    def find_video_file(commercial_number, ads_dir):\n",
    "        \"\"\"Find the video file path for a given commercial number.\"\"\"\n",
    "        # Search recursively for MP4 files\n",
    "        pattern = f\"{ads_dir}/**/{commercial_number}.mp4\"\n",
    "        matches = glob.glob(pattern, recursive=True)\n",
    "        return matches[0] if matches else None\n",
    "    ad_df['ocr_text'] = ''\n",
    "    for idx, row in ad_df.iterrows():\n",
    "        commercial_number = row['commercial_number']\n",
    "        video_path = find_video_file(commercial_number, ads_dir)\n",
    "        \n",
    "        if video_path:\n",
    "            ocr_text = ocr(video_path)\n",
    "            ad_df.at[idx, 'ocr_text'] = ocr_text\n",
    "        else:\n",
    "            logging.info(f\"Video not found for commercial {commercial_number}\")\n",
    "\n",
    "    ad_df[['commercial_number', 'ocr_text']].to_csv('csvs/ocr.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature: Superlatives, Comparatives, Uniqueness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the transcript data\n",
    "ocr_df = pd.read_csv('csvs/ocr.csv')\n",
    "ad_df = ad_df.merge(ocr_df, on='commercial_number', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import text_analysis as ta\n",
    "if not BDM_WORDS_CHECKPOINT:\n",
    "\n",
    "\n",
    "    ad_df['word_count'] = 0\n",
    "    ad_df['superlative_count'] = 0\n",
    "    ad_df['superlative_pct'] = 0.0\n",
    "    ad_df['comparative_count'] = 0\n",
    "    ad_df['comparative_pct'] = 0.0\n",
    "    ad_df['uniqueness_count'] = 0\n",
    "    ad_df['uniqueness_pct'] = 0.0\n",
    "    ad_df['total_bdm_terms_count'] = 0\n",
    "    ad_df['total_bdm_terms_pct'] = 0.0\n",
    "\n",
    "    for idx, row in ad_df.iterrows():\n",
    "        word_count = len(ta.get_tokens(row['transcript']))\n",
    "        ad_df.at[idx, 'word_count'] = word_count\n",
    "\n",
    "        superlatives = ta.get_superlatives(row['transcript'])\n",
    "        ad_df.at[idx, 'superlatives'] = ', '.join(superlatives) if superlatives else ''\n",
    "        superlative_count = len(superlatives) if superlatives else 0\n",
    "        ad_df.at[idx, 'superlative_count'] = superlative_count\n",
    "\n",
    "        comparatives = ta.get_comparatives(row['transcript'])\n",
    "        ad_df.at[idx, 'comparatives'] = ', '.join(comparatives) if comparatives else ''\n",
    "        comparative_count = len(comparatives) if comparatives else 0\n",
    "        ad_df.at[idx, 'comparative_count'] = comparative_count\n",
    "        \n",
    "        unique_words = ta.get_unique_words(row['transcript'])\n",
    "        ad_df.at[idx, 'unique_words'] = ', '.join(unique_words) if unique_words else ''\n",
    "        uniqueness_count = len(unique_words) if unique_words else 0\n",
    "        ad_df.at[idx, 'uniqueness_count'] = uniqueness_count\n",
    "\n",
    "        if word_count > 0:\n",
    "            ad_df.at[idx, 'superlative_pct'] = superlative_count / word_count * 100\n",
    "            ad_df.at[idx, 'comparative_pct'] = comparative_count / word_count * 100\n",
    "            ad_df.at[idx, 'uniqueness_pct'] = uniqueness_count / word_count * 100\n",
    "            \n",
    "            total_bdm_terms = superlative_count + comparative_count + uniqueness_count\n",
    "            ad_df.at[idx, 'total_bdm_terms_count'] = total_bdm_terms\n",
    "            ad_df.at[idx, 'total_bdm_terms_pct'] = total_bdm_terms / word_count * 100\n",
    "\n",
    "    ad_df = ad_df.sort_values(\n",
    "        by=['superlative_count', 'comparative_count', 'superlative_pct', 'comparative_pct', 'uniqueness_pct'],\n",
    "        ascending=[False, False, False, False, False]\n",
    "    )\n",
    "\n",
    "    ad_df[['commercial_number', 'superlatives', 'comparatives', 'unique_words', 'superlative_count', 'comparative_count', 'uniqueness_count', 'superlative_pct', 'comparative_pct', 'uniqueness_pct', 'total_bdm_terms_count', 'total_bdm_terms_pct']].to_csv('csvs/bdm_words.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature: Nomen + Adjektive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "bdm_words_df = pd.read_csv('csvs/bdm_words.csv')\n",
    "ad_df = ad_df.merge(bdm_words_df, on='commercial_number', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not ADJ_NOUN_PAIRS_CHECKPOINT:\n",
    "  ad_df[\"adj_noun_pairs\"] = ad_df[\"transcript\"].apply(ta.extract_adj_noun_pairs)\n",
    "  ad_df[\"num_adj_noun_pairs\"] = ad_df[\"adj_noun_pairs\"].apply(len)\n",
    "  ad_df[['commercial_number', 'adj_noun_pairs', 'num_adj_noun_pairs']].to_csv('csvs/adj_noun_pairs.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature: Semantische Nähe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_noun_pairs_df = pd.read_csv('csvs/adj_noun_pairs.csv')\n",
    "ad_df = ad_df.merge(adj_noun_pairs_df, on='commercial_number', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if INDUSTRY_SPECIFIC_AWARENESS and not SEMANTIC_SIMILARITY_CHECKPOINT:\n",
    "    import nltk\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    from nltk.corpus import stopwords\n",
    "    from nltk.probability import FreqDist\n",
    "    from collections import defaultdict\n",
    "    nltk.download('all')\n",
    "    import numpy as np\n",
    "    for idx, row in ad_df.iterrows():\n",
    "        transcript = row['transcript']\n",
    "        product_cat_keyword_similarities = {}\n",
    "        for keyword in row['product_cat_keywords'][1:-1].replace(\"'\", \"\").split(\", \"):\n",
    "            similarity = round(float(ta.get_semantic_similarity(transcript, keyword)), 3)\n",
    "            product_cat_keyword_similarities[keyword] = similarity\n",
    "        \n",
    "        sorted_keywords = sorted(product_cat_keyword_similarities.items(), key=lambda x: x[1], reverse=True)\n",
    "        top_3_keywords = sorted_keywords[:3]\n",
    "        top_3_average = round(float(np.mean([sim for _, sim in top_3_keywords])), 3)\n",
    "        \n",
    "\n",
    "        logging.info(f\"Top 3 keywords for {row['commercial_number']}:\")\n",
    "        for keyword, similarity in top_3_keywords:\n",
    "            logging.info(f\"- {keyword}: {similarity}\")\n",
    "        logging.info(f\"Top 3 average similarity: {top_3_average}\")\n",
    "        \n",
    "        ad_df.at[idx, 'product_cat_keyword_similarity'] = top_3_average\n",
    "        ad_df.at[idx, 'product_cat_top_keywords'] = ', '.join([keyword for keyword, _ in top_3_keywords])\n",
    "if BRAND_SPECIFIC_AWARENESS and not SEMANTIC_SIMILARITY_CHECKPOINT:\n",
    "    for idx, row in ad_df.iterrows():\n",
    "        transcript = row['transcript']\n",
    "        product_brand_keyword_similarities = {}\n",
    "        \n",
    "        for keyword in row['product_brand_keywords'][1:-1].replace(\"'\", \"\").split(\", \"):\n",
    "            similarity = round(float(ta.get_semantic_similarity(transcript, keyword)), 3)\n",
    "            product_brand_keyword_similarities[keyword] = similarity\n",
    "        \n",
    "        sorted_keywords = sorted(product_brand_keyword_similarities.items(), key=lambda x: x[1], reverse=True)\n",
    "        top_3_keywords = sorted_keywords[:3]\n",
    "        top_3_average = round(float(np.mean([sim for _, sim in top_3_keywords])), 3)\n",
    "        \n",
    "        logging.info(f\"Top 3 brand keywords for {row['commercial_number']}:\")\n",
    "        for keyword, similarity in top_3_keywords:\n",
    "            logging.info(f\"- {keyword}: {similarity}\")\n",
    "        logging.info(f\"Top 3 average brand similarity: {top_3_average}\")\n",
    "        \n",
    "        ad_df.at[idx, 'product_brand_keyword_similarity'] = top_3_average\n",
    "        ad_df.at[idx, 'product_brand_top_keywords'] = ', '.join([keyword for keyword, _ in top_3_keywords])\n",
    "if not SEMANTIC_SIMILARITY_CHECKPOINT:\n",
    "    columns = ['commercial_number']\n",
    "    if INDUSTRY_SPECIFIC_AWARENESS:\n",
    "        columns.extend(['product_cat_keyword_similarity', 'product_cat_top_keywords'])\n",
    "    if BRAND_SPECIFIC_AWARENESS:\n",
    "        columns.extend(['product_brand_keyword_similarity', 'product_brand_top_keywords'])\n",
    "    ad_df[columns].to_csv('csvs/semantic_similarity.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature: Personalpronomen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic_similarity_df = pd.read_csv('csvs/semantic_similarity.csv')\n",
    "ad_df = ad_df.merge(semantic_similarity_df, on='commercial_number', how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not PERSONAL_PRONOUNS_CHECKPOINT:\n",
    "    for idx, row in ad_df.iterrows():\n",
    "        transcript = row['transcript']\n",
    "        most_common_pronoun, most_common_pronoun_count, most_common_pronoun_pct = ta.get_dominant_pronoun_stats(transcript)\n",
    "        ad_df.at[idx, 'personal_pronouns'] = most_common_pronoun\n",
    "        ad_df.at[idx, 'num_personal_pronouns'] = most_common_pronoun_count\n",
    "        ad_df.at[idx, 'personal_pronoun_pct'] = most_common_pronoun_pct\n",
    "        ad_df[['commercial_number', 'personal_pronouns', 'num_personal_pronouns', 'personal_pronoun_pct']].to_csv('csvs/personal_pronouns.csv', index=False)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "personal_pronouns_df = pd.read_csv('csvs/personal_pronouns.csv')\n",
    "ad_df = ad_df.merge(personal_pronouns_df, on='commercial_number', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "display(ad_df[ad_df.isnull().any(axis=1)])\n",
    "display(ad_df[ad_df.isna().any(axis=1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(f\"Rows with BDM = 1.0: {len(ad_df[ad_df['BDM'] == 1.0])}\")\n",
    "logging.info(f\"Rows with BDM = 0.0: {len(ad_df[ad_df['BDM'] == 0.0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "commercial_numbers = ad_df['commercial_number']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CRISP-DM 4: Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import models as m\n",
    "\n",
    "data = m.prepare_model_data(ad_df, INDUSTRY_SPECIFIC_AWARENESS, BRAND_SPECIFIC_AWARENESS)\n",
    "target = ad_df['BDM']\n",
    "base_models = m.get_base_models()\n",
    "param_distributions = m.get_param_distributions()\n",
    "tuned_models = m.tune_models(data, target, base_models, param_distributions)\n",
    "\n",
    "trained_models = m.train_models(data, target, tuned_models, INDUSTRY_SPECIFIC_AWARENESS, BRAND_SPECIFIC_AWARENESS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CRISP-DM 5: Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df, predictions = m.evaluate_models(data, target, trained_models)\n",
    "original_data = ad_df.copy()\n",
    "display(original_data.head(10))\n",
    "original_data = pd.concat([original_data, predictions], axis=1)\n",
    "m.display_model_results(data, target, trained_models, results_df)\n",
    "predicted_data = original_data[['commercial_number', 'BDM', 'Logistic Regression_result', 'Random Forest_result', 'Support Vector Machine_result']]\n",
    "predicted_data['majority_vote'] = predicted_data[['Logistic Regression_result', 'Random Forest_result', 'Support Vector Machine_result']].mode(axis=1)[0]\n",
    "display(predicted_data.head(10))\n",
    "m.analyze_decision_tree(data, target, tuned_models)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
