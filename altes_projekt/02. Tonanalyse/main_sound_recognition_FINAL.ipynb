{"cells":[{"cell_type":"markdown","metadata":{"id":"dE96Mw7hUjlE"},"source":["#**Automatic Audio Recognition**\n","This script includes 3 different models which analyze different parts of audio in super bowl ads.\n","1. *Gender specific speaking time* (and durations of speaking parts)\n","2. *Emotion recognition from Transcription* (uses only transcription from WhisperAI for analysis)\n","3. *Acoustic Indizes* (many different indicators like min/max_energy, db and tempo)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"54LsLpyqT-lM"},"source":["# *1. Gender specific speaking time*"]},{"cell_type":"markdown","metadata":{"id":"jLzGh7XjtR9V"},"source":["Initially, the audio file will be segmented before gender recognition can take place\n","\n","Audio segmentation: https://github.com/pyannote/pyannote-audio\n","\n","Gender recognition: https://github.com/x4nth055/gender-recognition-by-voice"]},{"cell_type":"markdown","metadata":{"id":"tckHJKZnYnp7"},"source":["### Installation"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":115024,"status":"ok","timestamp":1708638592563,"user":{"displayName":"Thorben","userId":"17213772383075988098"},"user_tz":-60},"id":"IL25VsVSgzOw","outputId":"39d1663c-2a19-4910-d1cb-72952e68ea61"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m208.7/208.7 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.5/58.5 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.1/48.1 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.4/51.4 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.6/118.6 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m630.6/630.6 kB\u001b[0m \u001b[31m55.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.1/50.1 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m840.4/840.4 kB\u001b[0m \u001b[31m33.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m800.9/800.9 kB\u001b[0m \u001b[31m35.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m413.4/413.4 kB\u001b[0m \u001b[31m31.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.6/59.6 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.4/233.4 kB\u001b[0m \u001b[31m28.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.8/117.8 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.7/78.7 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m526.7/526.7 kB\u001b[0m \u001b[31m34.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for julius (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","Reading package lists... Done\n","Building dependency tree... Done\n","Reading state information... Done\n","The following NEW packages will be installed:\n","  libportaudio2\n","0 upgraded, 1 newly installed, 0 to remove and 35 not upgraded.\n","Need to get 65.3 kB of archives.\n","After this operation, 223 kB of additional disk space will be used.\n","Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libportaudio2 amd64 19.6.0-1.1 [65.3 kB]\n","Fetched 65.3 kB in 1s (127 kB/s)\n","debconf: unable to initialize frontend: Dialog\n","debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 1.)\n","debconf: falling back to frontend: Readline\n","debconf: unable to initialize frontend: Readline\n","debconf: (This frontend requires a controlling tty.)\n","debconf: falling back to frontend: Teletype\n","dpkg-preconfigure: unable to re-open stdin: \n","Selecting previously unselected package libportaudio2:amd64.\n","(Reading database ... 121749 files and directories currently installed.)\n","Preparing to unpack .../libportaudio2_19.6.0-1.1_amd64.deb ...\n","Unpacking libportaudio2:amd64 (19.6.0-1.1) ...\n","Setting up libportaudio2:amd64 (19.6.0-1.1) ...\n","Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n","/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n","\n","Reading package lists... Done\n","Building dependency tree... Done\n","Reading state information... Done\n","Suggested packages:\n","  python-pyaudio-doc\n","The following NEW packages will be installed:\n","  python3-pyaudio\n","0 upgraded, 1 newly installed, 0 to remove and 35 not upgraded.\n","Need to get 25.9 kB of archives.\n","After this operation, 117 kB of additional disk space will be used.\n","Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 python3-pyaudio amd64 0.2.11-1.3ubuntu1 [25.9 kB]\n","Fetched 25.9 kB in 0s (52.8 kB/s)\n","debconf: unable to initialize frontend: Dialog\n","debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 1.)\n","debconf: falling back to frontend: Readline\n","debconf: unable to initialize frontend: Readline\n","debconf: (This frontend requires a controlling tty.)\n","debconf: falling back to frontend: Teletype\n","dpkg-preconfigure: unable to re-open stdin: \n","Selecting previously unselected package python3-pyaudio.\n","(Reading database ... 121755 files and directories currently installed.)\n","Preparing to unpack .../python3-pyaudio_0.2.11-1.3ubuntu1_amd64.deb ...\n","Unpacking python3-pyaudio (0.2.11-1.3ubuntu1) ...\n","Setting up python3-pyaudio (0.2.11-1.3ubuntu1) ...\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for utils (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.2)\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/pyannote/audio/core/io.py:43: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.\n","  torchaudio.set_audio_backend(\"soundfile\")\n"]}],"source":["!pip install -qq pyannote.audio==3.1.1\n","!pip install -qq ipython==7.34.0\n","!pip install -qq tensorflow\n","!pip install -qq scikit-learn\n","!pip install -qq numpy\n","!pip install -qq pandas\n","!pip install -qq tqdm\n","!sudo apt-get install libportaudio2\n","!sudo apt-get install python3-pyaudio\n","!pip install -qq librosa\n","!pip install -qq utils\n","\n","!pip -qq install pydub\n","!pip install tqdm\n","from pydub import AudioSegment\n","from tqdm import tqdm\n","from pyannote.audio import Pipeline\n","import torch\n","\n","import pyaudio\n","import os\n","import wave\n","import librosa\n","import numpy as np\n","import pandas as pd\n","import tqdm\n","import locale\n","\n","\n","\n","def getpreferredencoding(do_setlocale = True):\n","    return \"UTF-8\"\n","locale.getpreferredencoding = getpreferredencoding"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22883,"status":"ok","timestamp":1708638620870,"user":{"displayName":"Thorben","userId":"17213772383075988098"},"user_tz":-60},"id":"YPbio2axS-_m","outputId":"4a7aa1b1-dbdc-4e6b-8c6b-b01c6a8174e3"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["#Optional\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"5MclWK2GYnp_"},"source":["### Mandatory Login\n","\n","To load the speaker diarization pipeline,\n","\n","* accept the user conditions on [hf.co/pyannote/speaker-diarization-3.1](https://hf.co/pyannote/speaker-diarization-3.1)\n","* accept the user conditions on [hf.co/pyannote/segmentation-3.0](https://hf.co/pyannote/segmentation-3.0)\n","* login using `notebook_login` below"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jh3EV9Y2wFXC"},"outputs":[],"source":["# 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"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":145,"referenced_widgets":["d0647072ab8044c7a1ff53b9e01f7189","2d52b4a045c14f44903b010261527ed6","3ff01220d0d449f99775a49369910f63","34222225ba97456694d7298c879b4b19","d9bce56fe264426b85e729cf46a3973f","6c2729e3f4b84d349a112da2ccf77b25","35ae3d30cf45414a98a606b2c9ba40be","504a095f33214bd4b36646d088b49e5e","a9bf397a62944000ae68a482a8ae20c1","aa5d8610fb084fa59991f262fed05bad","a5865557a7474b0ebeed270b592f38ac","45d6c82b2ec74893ae2b5a86eeb1daff","e9be5a21ad9046d181c258ea1487de5c","8a7f1963cc7848a48b2c8a5ab1ff17a3"]},"executionInfo":{"elapsed":284,"status":"ok","timestamp":1708638630457,"user":{"displayName":"Thorben","userId":"17213772383075988098"},"user_tz":-60},"id":"r5u7VMb-YnqB","outputId":"1ddafd98-b25a-4ac0-b544-0b2a0419522e"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d0647072ab8044c7a1ff53b9e01f7189","version_major":2,"version_minor":0},"text/plain":["VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"]},"metadata":{},"output_type":"display_data"}],"source":["from huggingface_hub import notebook_login\n","notebook_login(\"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\")"]},{"cell_type":"markdown","metadata":{"id":"tjNgvcnOWRIF"},"source":["## Audio Splitter Method Definition\n","\n"]},{"cell_type":"markdown","metadata":{"id":"4eqTRtgxgc0A"},"source":["### Utils\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tRqtCGZegfjr"},"outputs":[],"source":["from sys import byteorder\n","from array import array\n","from struct import pack\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, LSTM, Dropout\n","from sklearn.model_selection import train_test_split\n","\n","\n","label2int = {\n","    \"male\": 1,\n","    \"female\": 0\n","}\n","\n","\n","def load_data(vector_length=128):\n","    \"\"\"A function to load gender recognition dataset from `data` folder\n","    After the second run, this will load from results/features.npy and results/labels.npy files\n","    as it is much faster!\"\"\"\n","    # make sure results folder exists\n","    if not os.path.isdir(\"results\"):\n","        os.mkdir(\"results\")\n","    # if features & labels already loaded individually and bundled, load them from there instead\n","    if os.path.isfile(\"/content/drive/MyDrive/SuperBowl_Project_FUB/Colab_Notebooks/audio_gender_notebooks/results/features.npy\") and os.path.isfile(\"results/labels.npy\"):\n","        X = np.load(\"/content/drive/MyDrive/SuperBowl_Project_FUB/Colab_Notebooks/audio_gender_notebooks/results/features.npy\")\n","        y = np.load(\"/content/drive/MyDrive/SuperBowl_Project_FUB/Colab_Notebooks/audio_gender_notebooks/results/labels.npy\")\n","        return X, y\n","    # read dataframe\n","    df = pd.read_csv(\"/content/drive/MyDrive/SuperBowl_Project_FUB/Colab_Notebooks/audio_gender_notebooks/balanced-all.csv\")\n","    # get total samples\n","    n_samples = len(df)\n","    # get total male samples\n","    n_male_samples = len(df[df['gender'] == 'male'])\n","    # get total female samples\n","    n_female_samples = len(df[df['gender'] == 'female'])\n","    print(\"Total samples:\", n_samples)\n","    print(\"Total male samples:\", n_male_samples)\n","    print(\"Total female samples:\", n_female_samples)\n","    # initialize an empty array for all audio features\n","    X = np.zeros((n_samples, vector_length))\n","    # initialize an empty array for all audio labels (1 for male and 0 for female)\n","    y = np.zeros((n_samples, 1))\n","    for i, (filename, gender) in tqdm.tqdm(enumerate(zip(df['filename'], df['gender'])), \"Loading data\", total=n_samples):\n","        features = np.load(filename)\n","        X[i] = features\n","        y[i] = label2int[gender]\n","    # save the audio features and labels into files\n","    # so we won't load each one of them next run\n","    np.save(\"/content/drive/MyDrive/SuperBowl_Project_FUB/Colab_Notebooks/audio_gender_notebooks/results/features\", X)\n","    np.save(\"/content/drive/MyDrive/SuperBowl_Project_FUB/Colab_Notebooks/audio_gender_notebooks/results\", y)\n","    return X, y\n","\n","\n","def split_data(X, y, test_size=0.1, valid_size=0.1):\n","    # split training set and testing set\n","    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=7)\n","    # split training set and validation set\n","    X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=valid_size, random_state=7)\n","    # return a dictionary of values\n","    return {\n","        \"X_train\": X_train,\n","        \"X_valid\": X_valid,\n","        \"X_test\": X_test,\n","        \"y_train\": y_train,\n","        \"y_valid\": y_valid,\n","        \"y_test\": y_test\n","    }\n","\n","\n","def create_model(vector_length=128):\n","    \"\"\"5 hidden dense layers from 256 units to 64, not the best model, but not bad.\"\"\"\n","    model = Sequential()\n","    model.add(Dense(256, input_shape=(vector_length,)))\n","    model.add(Dropout(0.3))\n","    model.add(Dense(256, activation=\"relu\"))\n","    model.add(Dropout(0.3))\n","    model.add(Dense(128, activation=\"relu\"))\n","    model.add(Dropout(0.3))\n","    model.add(Dense(128, activation=\"relu\"))\n","    model.add(Dropout(0.3))\n","    model.add(Dense(64, activation=\"relu\"))\n","    model.add(Dropout(0.3))\n","    # one output neuron with sigmoid activation function, 0 means female, 1 means male\n","    model.add(Dense(1, activation=\"sigmoid\"))\n","    # using binary crossentropy as it's male/female classification (binary)\n","    model.compile(loss=\"binary_crossentropy\", metrics=[\"accuracy\"], optimizer=\"adam\")\n","    # print summary of the model\n","    model.summary()\n","    return model"]},{"cell_type":"markdown","metadata":{"id":"jkH61xIZgWXp"},"source":["### Test Definition"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7gWd9jx8WX3r"},"outputs":[],"source":["THRESHOLD = 500\n","CHUNK_SIZE = 1024\n","FORMAT = pyaudio.paInt16\n","RATE = 16000\n","\n","SILENCE = 30\n","\n","def is_silent(snd_data):\n","    \"Returns 'True' if below the 'silent' threshold\"\n","    return max(snd_data) < THRESHOLD\n","\n","def normalize(snd_data):\n","    \"Average the volume out\"\n","    MAXIMUM = 16384\n","    times = float(MAXIMUM)/max(abs(i) for i in snd_data)\n","\n","    r = array('h')\n","    for i in snd_data:\n","        r.append(int(i*times))\n","    return r\n","\n","def trim(snd_data):\n","    \"Trim the blank spots at the start and end\"\n","    def _trim(snd_data):\n","        snd_started = False\n","        r = array('h')\n","\n","        for i in snd_data:\n","            if not snd_started and abs(i)>THRESHOLD:\n","                snd_started = True\n","                r.append(i)\n","\n","            elif snd_started:\n","                r.append(i)\n","        return r\n","\n","    # Trim to the left\n","    snd_data = _trim(snd_data)\n","\n","    # Trim to the right\n","    snd_data.reverse()\n","    snd_data = _trim(snd_data)\n","    snd_data.reverse()\n","    return snd_data\n","\n","def add_silence(snd_data, seconds):\n","    \"Add silence to the start and end of 'snd_data' of length 'seconds' (float)\"\n","    r = array('h', [0 for i in range(int(seconds*RATE))])\n","    r.extend(snd_data)\n","    r.extend([0 for i in range(int(seconds*RATE))])\n","    return r\n","\n","\n","def extract_feature(file_name, **kwargs):\n","    \"\"\"\n","    Extract feature from audio file `file_name`\n","        Features supported:\n","            - MFCC (mfcc)\n","            - Chroma (chroma)\n","            - MEL Spectrogram Frequency (mel)\n","            - Contrast (contrast)\n","            - Tonnetz (tonnetz)\n","        e.g:\n","        `features = extract_feature(path, mel=True, mfcc=True)`\n","    \"\"\"\n","    mfcc = kwargs.get(\"mfcc\")\n","    chroma = kwargs.get(\"chroma\")\n","    mel = kwargs.get(\"mel\")\n","    contrast = kwargs.get(\"contrast\")\n","    tonnetz = kwargs.get(\"tonnetz\")\n","    X, sample_rate = librosa.core.load(file_name)\n","    if chroma or contrast:\n","        stft = np.abs(librosa.stft(X))\n","    result = np.array([])\n","    if mfcc:\n","        mfccs = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=40).T, axis=0)\n","        result = np.hstack((result, mfccs))\n","    if chroma:\n","        chroma = np.mean(librosa.feature.chroma_stft(S=stft, sr=sample_rate).T,axis=0)\n","        result = np.hstack((result, chroma))\n","    if mel:\n","        mel = np.mean(librosa.feature.melspectrogram(y=X, sr=sample_rate).T,axis=0)\n","        result = np.hstack((result, mel))\n","    if contrast:\n","        contrast = np.mean(librosa.feature.spectral_contrast(S=stft, sr=sample_rate).T,axis=0)\n","        result = np.hstack((result, contrast))\n","    if tonnetz:\n","        tonnetz = np.mean(librosa.feature.tonnetz(y=librosa.effects.harmonic(X), sr=sample_rate).T,axis=0)\n","        result = np.hstack((result, tonnetz))\n","    return result"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ikiAX5Mm29yi"},"outputs":[],"source":["from openpyxl import load_workbook\n","\n","def audio_splitter(input_folder, output_base_folder, output_base_excel_folder):\n","  # Construct the model\n","  model = create_model()\n","  # Load the saved/trained weights\n","  model.load_weights(\"/content/drive/MyDrive/SuperBowl_Project_FUB/Colab_Notebooks/audio_gender_notebooks/results/model.h5\")\n","\n","\n","  # Iterate through each folder from ADs_IG_2013_wav to ADs_IG_2022_wav\n","  for folder_name in range(2013, 2023):\n","      input_path = f\"{input_folder}/ADs_IG_{folder_name}_wav\"\n","      output_folder = f\"{output_base_folder}/ADs_IG_{folder_name}\"#.wav\n","\n","      # output_folder_excel\n","      excel_output_folder = f\"{output_base_excel_folder}/ADs_IG_{folder_name}\" # .wav\n","\n","      # Create output folder if it doesn't exist\n","      os.makedirs(output_folder, exist_ok=True)\n","\n","      # Iterate through each file in the current folder\n","      for file_name in os.listdir(input_path):\n","          if file_name.endswith(\".wav\"):\n","              audio_path = f\"{input_path}/{file_name}\"  # Path to the audio file\n","              audio = AudioSegment.from_wav(audio_path)\n","\n","              result_per_audio = []\n","\n","              # Initialize an empty list to store the segmented audio\n","              voice_timestamp = []\n","              segment_index = 0\n","\n","              # Load audio for diarization\n","              own_file = {'audio': audio_path}  # Provide the audio file path\n","              pipeline = Pipeline.from_pretrained('pyannote/speaker-diarization-3.1', use_auth_token=True)\n","              if torch.cuda.is_available():\n","                  pipeline.to(torch.device('cuda'))\n","\n","              from pyannote.audio.pipelines.utils.hook import ProgressHook\n","              with ProgressHook() as hook:\n","                  diarization = pipeline(own_file, hook=hook)\n","\n","\n","              # Segmentation logic\n","              for turn, _, speaker in diarization.itertracks(yield_label=True):\n","                  segment_id = f\"segment_{segment_index}\"\n","                  voice_timestamp.append((segment_id, turn.start, turn.end, speaker))\n","                  segment_index += 1\n","\n","\n","              # Loop through the voice_timestamp list and split the audio\n","              for idx, (_, start, stop, speaker) in enumerate(voice_timestamp):\n","                  segment = audio[start * 1000: stop * 1000]  # Extract the segment in milliseconds\n","                  output_subfolder = f\"{output_folder}/{os.path.splitext(file_name)[0]}\"  # Create subfolder based on file name\n","                  os.makedirs(output_subfolder, exist_ok=True)  # Create subfolder if it doesn't exist\n","\n","                  if((stop - start) < 0.05):\n","                    print(f\"segment_{idx}.wav is too short!\")\n","                    continue\n","                  else:\n","                    segment.export(f\"{output_subfolder}/segment_{idx}.wav\", format=\"wav\")\n","\n","                  # Extract features and reshape it (assuming you have an extract_feature function)\n","                  features = extract_feature(f\"{output_subfolder}/segment_{idx}.wav\", format=\"wav\", mel=True).reshape(1, -1)\n","\n","                  # Predict the gender\n","                  male_prob = model.predict(features)[0][0]\n","                  female_prob = 1 - male_prob\n","                  gender = \"male\" if male_prob > female_prob else \"female\"\n","\n","\n","                  # Store segment information in the list\n","                  duration = stop - start\n","                  segID = f\"segment_{idx}.wav\"\n","                  result_per_audio.append((file_name, segID, start,stop, duration, speaker, gender, male_prob, female_prob))\n","\n","\n","              for result_tuple in result_per_audio:\n","                file_name, segID, turn_start, turn_end, duration, speaker, gender, male_prob, female_prob = result_tuple\n","                print(f\"file_name: {file_name}, segment_id: {segID}, turn.start: {turn_start}, turn.end: {turn_end}, duration: {duration}, speaker: {speaker}, gender: {gender}, male_prob: {male_prob}, female_prob: {female_prob}\")\n","\n","\n","\n","              # Create DataFrame from result_per_audio\n","              df = pd.DataFrame(result_per_audio, columns=['file_name', 'segment_id', 'start', 'end', 'duration', 'speaker', 'gender', 'male_prob', 'female_prob'])\n","\n","              # Create directory for saving Excel file\n","              excel_output_folder = f\"{output_base_excel_folder}/ADs_IG_{folder_name}\" #.wav\n","              os.makedirs(excel_output_folder, exist_ok=True)  # Create output directory if it doesn't exist\n","\n","              # Save DataFrame to Excel\n","              excel_file_path = f\"{excel_output_folder}/{os.path.splitext(file_name)[0]}.xlsx\"  # Excel file path\n","\n","              # Check if Excel file already exists\n","              if os.path.isfile(excel_file_path):\n","\n","                 book = load_workbook(excel_file_path)\n","\n","                 # Create a Pandas Excel writer using openpyxl\n","                 writer = pd.ExcelWriter(excel_file_path, engine='openpyxl')\n","                 writer.book = book\n","\n","                  # Add DataFrame to existing Excel file in a new sheet\n","                 df.to_excel(writer, sheet_name='Gender_speaking_time', index=False)\n","\n","                  # Save the changes\n","                 writer.save()\n","              else:\n","                  df.to_excel(excel_file_path, sheet_name=\"Gender_speaking_time\", index=False)  # Export DataFrame to Excel without index\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"7V6Uy8EU-umc"},"source":["### Main\n","\n","---\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["c9ff155a579140e2bb19bfbadc4e032f","4cbb11e0ee474f2eaf7ffa7e3583e9f8"]},"executionInfo":{"elapsed":14257,"status":"ok","timestamp":1708647221477,"user":{"displayName":"Thorben","userId":"17213772383075988098"},"user_tz":-60},"id":"Mow9w3JKCPXh","outputId":"e68d7dc1-90c8-4ad0-dbe6-b161d17297fb"},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"sequential_3\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," dense_18 (Dense)            (None, 256)               33024     \n","                                                                 \n"," dropout_15 (Dropout)        (None, 256)               0         \n","                                                                 \n"," dense_19 (Dense)            (None, 256)               65792     \n","                                                                 \n"," dropout_16 (Dropout)        (None, 256)               0         \n","                                                                 \n"," dense_20 (Dense)            (None, 128)               32896     \n","                                                                 \n"," dropout_17 (Dropout)        (None, 128)               0         \n","                                                                 \n"," dense_21 (Dense)            (None, 128)               16512     \n","                                                                 \n"," dropout_18 (Dropout)        (None, 128)               0         \n","                                                                 \n"," dense_22 (Dense)            (None, 64)                8256      \n","                                                                 \n"," dropout_19 (Dropout)        (None, 64)                0         \n","                                                                 \n"," dense_23 (Dense)            (None, 1)                 65        \n","                                                                 \n","=================================================================\n","Total params: 156545 (611.50 KB)\n","Trainable params: 156545 (611.50 KB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c9ff155a579140e2bb19bfbadc4e032f","version_major":2,"version_minor":0},"text/plain":["Output()"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"],"text/plain":[]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n","</pre>\n"],"text/plain":["\n"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["1/1 [==============================] - 0s 130ms/step\n","1/1 [==============================] - 0s 52ms/step\n","1/1 [==============================] - 0s 38ms/step\n","1/1 [==============================] - 0s 32ms/step\n","1/1 [==============================] - 0s 40ms/step\n","1/1 [==============================] - 0s 19ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 31ms/step\n","1/1 [==============================] - 0s 33ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 32ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 24ms/step\n","segment_16.wav is too short!\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 27ms/step\n","file_name: AD0299.wav, segment_id: segment_0.wav, turn.start: 2.6230899830220715, turn.end: 3.505942275042445, duration: 0.8828522920203734, speaker: SPEAKER_01, gender: male, male_prob: 0.9192330241203308, female_prob: 0.08076697587966919\n","file_name: AD0299.wav, segment_id: segment_1.wav, turn.start: 3.5908319185059425, turn.end: 4.6943972835314085, duration: 1.103565365025466, speaker: SPEAKER_01, gender: female, male_prob: 0.4197688102722168, female_prob: 0.5802311897277832\n","file_name: AD0299.wav, segment_id: segment_2.wav, turn.start: 4.949066213921901, turn.end: 8.480475382003396, duration: 3.5314091680814954, speaker: SPEAKER_01, gender: male, male_prob: 0.9572740793228149, female_prob: 0.04272592067718506\n","file_name: AD0299.wav, segment_id: segment_3.wav, turn.start: 8.972835314091682, turn.end: 10.280135823429543, duration: 1.3073005093378605, speaker: SPEAKER_01, gender: male, male_prob: 0.9313696622848511, female_prob: 0.06863033771514893\n","file_name: AD0299.wav, segment_id: segment_4.wav, turn.start: 11.129032258064518, turn.end: 14.117147707979628, duration: 2.98811544991511, speaker: SPEAKER_01, gender: male, male_prob: 0.5936049818992615, female_prob: 0.4063950181007385\n","file_name: AD0299.wav, segment_id: segment_5.wav, turn.start: 15.560271646859084, turn.end: 16.341256366723258, duration: 0.7809847198641737, speaker: SPEAKER_01, gender: male, male_prob: 0.904643177986145, female_prob: 0.09535682201385498\n","file_name: AD0299.wav, segment_id: segment_6.wav, turn.start: 16.59592529711375, turn.end: 19.040747028862476, duration: 2.444821731748725, speaker: SPEAKER_01, gender: male, male_prob: 0.8347542881965637, female_prob: 0.16524571180343628\n","file_name: AD0299.wav, segment_id: segment_7.wav, turn.start: 19.668930390492363, turn.end: 20.212224108658745, duration: 0.5432937181663817, speaker: SPEAKER_01, gender: male, male_prob: 0.6795572638511658, female_prob: 0.32044273614883423\n","file_name: AD0299.wav, segment_id: segment_8.wav, turn.start: 20.874363327674025, turn.end: 31.09507640067912, duration: 10.220713073005093, speaker: SPEAKER_01, gender: male, male_prob: 0.9312126040458679, female_prob: 0.06878739595413208\n","file_name: AD0299.wav, segment_id: segment_9.wav, turn.start: 31.09507640067912, turn.end: 32.02886247877759, duration: 0.9337860780984713, speaker: SPEAKER_00, gender: male, male_prob: 0.9610421061515808, female_prob: 0.03895789384841919\n","file_name: AD0299.wav, segment_id: segment_10.wav, turn.start: 32.38539898132428, turn.end: 33.72665534804754, duration: 1.3412563667232575, speaker: SPEAKER_00, gender: male, male_prob: 0.9339573383331299, female_prob: 0.06604266166687012\n","file_name: AD0299.wav, segment_id: segment_11.wav, turn.start: 34.16808149405772, turn.end: 35.42444821731749, duration: 1.2563667232597666, speaker: SPEAKER_01, gender: male, male_prob: 0.6981319785118103, female_prob: 0.3018680214881897\n","file_name: AD0299.wav, segment_id: segment_12.wav, turn.start: 36.00169779286927, turn.end: 38.02207130730051, duration: 2.0203735144312347, speaker: SPEAKER_01, gender: male, male_prob: 0.9474207162857056, female_prob: 0.052579283714294434\n","file_name: AD0299.wav, segment_id: segment_13.wav, turn.start: 38.225806451612904, turn.end: 41.99490662139219, duration: 3.7691001697792856, speaker: SPEAKER_01, gender: male, male_prob: 0.8070628643035889, female_prob: 0.19293713569641113\n","file_name: AD0299.wav, segment_id: segment_14.wav, turn.start: 41.99490662139219, turn.end: 42.35144312393888, duration: 0.3565365025466889, speaker: SPEAKER_02, gender: female, male_prob: 0.18526001274585724, female_prob: 0.8147399872541428\n","file_name: AD0299.wav, segment_id: segment_15.wav, turn.start: 42.35144312393888, turn.end: 42.45331069609507, duration: 0.1018675721561948, speaker: SPEAKER_01, gender: male, male_prob: 0.806157648563385, female_prob: 0.193842351436615\n","file_name: AD0299.wav, segment_id: segment_17.wav, turn.start: 43.08149405772496, turn.end: 43.99830220713073, duration: 0.9168081494057674, speaker: SPEAKER_02, gender: female, male_prob: 0.3118652403354645, female_prob: 0.6881347596645355\n","file_name: AD0299.wav, segment_id: segment_18.wav, turn.start: 45.135823429541595, turn.end: 52.14770797962649, duration: 7.011884550084893, speaker: SPEAKER_02, gender: male, male_prob: 0.93407142162323, female_prob: 0.06592857837677002\n","file_name: AD0299.wav, segment_id: segment_19.wav, turn.start: 52.249575551782684, turn.end: 54.33786078098472, duration: 2.088285229202036, speaker: SPEAKER_02, gender: male, male_prob: 0.7403955459594727, female_prob: 0.25960445404052734\n","file_name: AD0299.wav, segment_id: segment_20.wav, turn.start: 54.49066213921902, turn.end: 56.29032258064516, duration: 1.7996604414261412, speaker: SPEAKER_02, gender: male, male_prob: 0.9012676477432251, female_prob: 0.0987323522567749\n"]}],"source":["# Input and output folder paths\n","input_folder = '/content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Input_audio'\n","output_base_folder = '/content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/splitted_audios'\n","output_base_excel_folder = '/content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/output_lists/output_lists'\n","splitted_audios= audio_splitter(input_folder, output_base_folder, output_base_excel_folder)"]},{"cell_type":"markdown","metadata":{"id":"8ed4CsrdFsXR"},"source":["### Total speaking time (Conclusion)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3572,"status":"ok","timestamp":1708647270619,"user":{"displayName":"Thorben","userId":"17213772383075988098"},"user_tz":-60},"id":"EO353s5XJLkn","outputId":"5130ae55-3dcc-4558-923a-537751a935a2"},"outputs":[{"name":"stdout","output_type":"stream","text":["Processing: /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/testtest/ADs_IG_2013_wav/AD0299.wav\n","Total Duration: 56.875 seconds\n"]}],"source":["import os\n","import pandas as pd\n","from pydub import AudioSegment\n","\n","# Define input and output base folders\n","input_folder = '/content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Input_audio'\n","output_base_excel_folder = '/content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/output_lists/output_lists'\n","\n","# Iterate through each folder from ADs_IG_2013_wav to ADs_IG_2022_wav\n","\n","for folder_name in range(2013, 2023):\n","    input_path = f\"{input_folder}/ADs_IG_{folder_name}_wav\"\n","    output_folder = f\"{output_base_excel_folder}/ADs_IG_{folder_name}\" #_wav\n","\n","    # Iterate through each WAV file in the current folder\n","    for wav_file_name in os.listdir(input_path):\n","        if wav_file_name.endswith(\".wav\"):\n","            # Load the WAV file using pydub\n","            wav_file_path = os.path.join(input_path, wav_file_name)\n","            audio = AudioSegment.from_wav(wav_file_path)\n","\n","            # Get the total duration in seconds\n","            total_duration = len(audio) / 1000.0  # Convert milliseconds to seconds\n","\n","            print(\"Processing:\", wav_file_path)\n","            print(\"Total Duration:\", total_duration, \"seconds\")\n","\n","            # Read Excel file into a pandas DataFrame\n","            excel_file_path = os.path.join(output_folder, f\"{os.path.splitext(wav_file_name)[0]}.xlsx\")\n","            # df = pd.read_excel(excel_file_path)\n","\n","            # Check if 'Gender_speaking_time' sheet exists in the Excel file\n","            with pd.ExcelWriter(excel_file_path, engine='openpyxl', mode='a', if_sheet_exists='replace') as writer:\n","                if 'Gender_speaking_time' in writer.sheets:\n","                    # Append to existing 'Gender_speaking_time' sheet\n","                    df = pd.read_excel(excel_file_path, sheet_name='Gender_speaking_time')  # Read data from the correct sheet\n","                else:\n","                    # Create a new 'Gender_speaking_time' sheet\n","                    df = pd.DataFrame()  # Create an empty DataFrame if the sheet doesn't exist\n","\n","            # Calculate total speaking time in seconds\n","            total_speaking_time = df['duration'].sum()\n","\n","            # Calculate the percentage of total speaking time against the total duration\n","            percentage_of_total_duration = total_speaking_time / total_duration * 100\n","\n","            # Calculate percentages\n","            df['Value_Percent'] = df['duration'] / total_speaking_time * 100\n","\n","            # Create a DataFrame with the results\n","            results_df = pd.DataFrame({\n","                'Metric': ['Total Speaking Time', 'Male Speaking Time', 'Female Speaking Time'],\n","                'Value_Seconds': [total_speaking_time, df[df['gender'] == 'male']['duration'].sum(), df[df['gender'] == 'female']['duration'].sum()],\n","                'Value_Percent': [percentage_of_total_duration, df[df['gender'] == 'male']['Value_Percent'].sum(), df[df['gender'] == 'female']['Value_Percent'].sum()]\n","            })\n","\n","            sheet_name = 'Gender_speaking_time'\n","\n","            existing_data = pd.read_excel(excel_file_path, sheet_name)\n","            existing_data[''] = ''\n","\n","            # Step 3: Concatenate the existing data, the empty column, and the new DataFrame\n","            combined_data = pd.concat([existing_data, results_df], axis=1)\n","\n","            # Step 3: Load the Excel file\n","            book = load_workbook(excel_file_path)\n","\n","            # Step 4: Get the writer object for the Excel file\n","            writer = pd.ExcelWriter(excel_file_path, engine='openpyxl')\n","            writer.book = book\n","\n","            # Step 5: Write the combined data to the fourth sheet\n","            combined_data.to_excel(writer, sheet_name, index=False, header=True)\n","\n","            # Step 6: Save the changes\n","            writer.save()\n","            writer.close()"]},{"cell_type":"markdown","metadata":{"id":"k49Dx-u6qMwE"},"source":["\n","\n","---\n","\n"]},{"cell_type":"markdown","metadata":{"id":"tDMwn1BCiYWW"},"source":["# *2. Emotion Recognition from Transcription*"]},{"cell_type":"markdown","metadata":{"id":"PP0b_vg7rrze"},"source":["WhisperAI transcribes all audio files before distilbert analyzes the emotions\n","\n","\n","WhisperAI: https://github.com/openai/whisper\n","\n","distilbert-base-uncased-emotion: https://huggingface.co/bhadresh-savani/distilbert-base-uncased-emotion?text=I+feel+a+bit+let+down"]},{"cell_type":"markdown","metadata":{"id":"AvqbjDgQn94R"},"source":["## Whisper AI for Transcription\n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":55409,"status":"ok","timestamp":1708602848456,"user":{"displayName":"Thorben Schlieffen","userId":"06537630395058694731"},"user_tz":-60},"id":"21_SRoXnoXlR","outputId":"9f5ed74f-d6bf-4427-dfb1-0f44cfc4b0b9"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting openai-whisper\n","  Downloading openai-whisper-20231117.tar.gz (798 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m798.6/798.6 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: triton<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (2.1.0)\n","Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (0.58.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (1.25.2)\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (2.1.0+cu121)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (4.66.2)\n","Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (10.1.0)\n","Collecting tiktoken (from openai-whisper)\n","  Downloading tiktoken-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from triton<3,>=2.0.0->openai-whisper) (3.13.1)\n","Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->openai-whisper) (0.41.1)\n","Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper) (2023.12.25)\n","Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper) (2.31.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (4.9.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (3.2.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (3.1.3)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (2023.6.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2024.2.2)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->openai-whisper) (2.1.5)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->openai-whisper) (1.3.0)\n","Building wheels for collected packages: openai-whisper\n","  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for openai-whisper: filename=openai_whisper-20231117-py3-none-any.whl size=801356 sha256=29d457bd281e8bf04ae623071474f0f10fb4d40d04c59365f77eae16d34990d6\n","  Stored in directory: /root/.cache/pip/wheels/d0/85/e1/9361b4cbea7dd4b7f6702fa4c3afc94877952eeb2b62f45f56\n","Successfully built openai-whisper\n","Installing collected packages: tiktoken, openai-whisper\n","Successfully installed openai-whisper-20231117 tiktoken-0.6.0\n","Collecting git+https://github.com/openai/whisper.git\n","  Cloning https://github.com/openai/whisper.git to /tmp/pip-req-build-v5yrr7c8\n","  Running command git clone --filter=blob:none --quiet https://github.com/openai/whisper.git /tmp/pip-req-build-v5yrr7c8\n","  Resolved https://github.com/openai/whisper.git to commit ba3f3cd54b0e5b8ce1ab3de13e32122d0d5f98ab\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (0.58.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (1.25.2)\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (2.1.0+cu121)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (4.66.2)\n","Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (10.1.0)\n","Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (0.6.0)\n","Requirement already satisfied: triton<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (2.1.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from triton<3,>=2.0.0->openai-whisper==20231117) (3.13.1)\n","Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->openai-whisper==20231117) (0.41.1)\n","Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper==20231117) (2023.12.25)\n","Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper==20231117) (2.31.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (4.9.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (3.2.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (3.1.3)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (2023.6.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (2024.2.2)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->openai-whisper==20231117) (2.1.5)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->openai-whisper==20231117) (1.3.0)\n","Collecting git+https://github.com/openai/whisper.git\n","  Cloning https://github.com/openai/whisper.git to /tmp/pip-req-build-utgd5w_u\n","  Running command git clone --filter=blob:none --quiet https://github.com/openai/whisper.git /tmp/pip-req-build-utgd5w_u\n","  Resolved https://github.com/openai/whisper.git to commit ba3f3cd54b0e5b8ce1ab3de13e32122d0d5f98ab\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Building wheels for collected packages: openai-whisper\n","  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for openai-whisper: filename=openai_whisper-20231117-py3-none-any.whl size=802825 sha256=5121e3a4a9cedbd9eeff03b08c0eb8351a6901a86d39ecb609f757b4bef52e54\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-1hi7rs0p/wheels/8b/6c/d0/622666868c179f156cf595c8b6f06f88bc5d80c4b31dccaa03\n","Successfully built openai-whisper\n","Installing collected packages: openai-whisper\n","  Attempting uninstall: openai-whisper\n","    Found existing installation: openai-whisper 20231117\n","    Uninstalling openai-whisper-20231117:\n","      Successfully uninstalled openai-whisper-20231117\n","Successfully installed openai-whisper-20231117\n"]}],"source":["# if not installed\n","!pip install -U openai-whisper\n","\n","!pip install git+https://github.com/openai/whisper.git\n","\n","!pip install --upgrade --no-deps --force-reinstall git+https://github.com/openai/whisper.git"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6625,"status":"ok","timestamp":1708602915572,"user":{"displayName":"Thorben Schlieffen","userId":"06537630395058694731"},"user_tz":-60},"id":"sGJ8gRNzobgc","outputId":"688e1b38-02bd-47d9-f98c-42bdba5f70af"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting setuptools-rust\n","  Downloading setuptools_rust-1.8.1-py3-none-any.whl (26 kB)\n","Requirement already satisfied: setuptools>=62.4 in /usr/local/lib/python3.10/dist-packages (from setuptools-rust) (67.7.2)\n","Collecting semantic-version<3,>=2.8.2 (from setuptools-rust)\n","  Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n","Requirement already satisfied: tomli>=1.2.1 in /usr/local/lib/python3.10/dist-packages (from setuptools-rust) (2.0.1)\n","Installing collected packages: semantic-version, setuptools-rust\n","Successfully installed semantic-version-2.10.0 setuptools-rust-1.8.1\n"]}],"source":["# if not installed\n","!pip install setuptools-rust"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20751,"status":"ok","timestamp":1708602079178,"user":{"displayName":"Thorben Schlieffen","userId":"06537630395058694731"},"user_tz":-60},"id":"GTmPWuaioekS","outputId":"c4d49d44-b55e-4aeb-ca32-d67dcdeebef9"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["# Optional\n","# from google.colab import drive\n","# drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"d8KH9_OQrbFE"},"source":["### Main\n","\n","---"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":605541,"status":"ok","timestamp":1708603530986,"user":{"displayName":"Thorben Schlieffen","userId":"06537630395058694731"},"user_tz":-60},"id":"e4zYYidAo0aC","outputId":"db925520-3f2d-4042-aede-f3c9074e1301"},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|█████████████████████████████████████| 2.88G/2.88G [00:46<00:00, 66.8MiB/s]\n","/usr/local/lib/python3.10/dist-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n","  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"]},{"name":"stdout","output_type":"stream","text":["Transcription for AD0431.wav:  Oh, you look good. Thank you. Hey. See the guy taking my little girl out, huh? Yep. Huh. You know what? Why don't you go ahead and take my new car? Thanks, Pop. Go ahead, baby. Watch this. Boom! Let's go! Boom! Let's go! Favorite spot, favorite girl. Are you ready? Hey, are you ready for this? You messing with the wrong daddy! I'm taking you home. Why? Car Finder on the Hyundai Genesis. Back so soon? Here you go, sir. Because a dad's got to do what a dad's got to do. Honey, what did you guys do tonight? Hey!\n","Output will be saved in /content/drive/MyDrive/Fanny/Audio/ADs_IG_2016_wav/AD0431.txt .\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n","  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"]},{"name":"stdout","output_type":"stream","text":["Transcription for AD0252.wav:  Look at you, so dashing. Come on. Nowadays, lots of people go by themselves. No, they don't. Yeah. Hey, son. Have fun tonight. Rob! Hey! Six play drive. Mentioned Baldwin.\n","Output will be saved in /content/drive/MyDrive/Fanny/Audio/ADs_IG_2013_wav/AD0252.txt .\n"]}],"source":["import os\n","import whisper\n","import torch\n","\n","# Input\n","input_folder_path = '/content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Input_audio'\n","\n","# Set model to CPU\n","device = torch.device('cpu')\n","model = whisper.load_model(\"large\", device=device)\n","\n","# Iterate through each folder\n","for folder_name in os.listdir(input_folder_path):\n","    folder_path = os.path.join(input_folder_path, folder_name)\n","\n","    # Iterate to find .wav files\n","    if os.path.isdir(folder_path):\n","        for file_name in os.listdir(folder_path):\n","            if file_name.endswith(\".wav\"):\n","                audio_file_path = os.path.join(folder_path, file_name)\n","\n","                # Transcription of audio file\n","                result = model.transcribe(audio_file_path)\n","\n","                # Outputs here are only TXT files. WhisperAI can also create SRT, VTT, TSV and JSON files. You can change it here.\n","\n","                # Create output\n","                output_txt_path = os.path.join(folder_path, f\"{file_name.split('.')[0]}.txt\")\n","\n","                # Save Output in .txt file\n","                with open(output_txt_path, 'w') as txt_file:\n","                    txt_file.write(result['text'])\n","\n","                print(f\"Transcription for {file_name}: {result['text']}\")\n","                print(f\"Output will be saved in {output_txt_path} .\")\n"]},{"cell_type":"markdown","metadata":{"id":"8d20XiSLo_FY"},"source":["\n","## Emotion Recognition (1)"]},{"cell_type":"markdown","source":["### Set up"],"metadata":{"id":"zZH1G38Y6AvO"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Lai6BjVVo_ue"},"outputs":[],"source":["from transformers import pipeline\n","import pandas as pd\n","import os"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rkjeJW2XiXhb"},"outputs":[],"source":["# If not installed:\n","#!pip install transformers\n","#!pip install xlsxwriter"]},{"cell_type":"markdown","source":["### Main"],"metadata":{"id":"EVfTBXNV7jCH"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"o-jZ1-Fjkgpd"},"outputs":[],"source":["# Define emotionclassifier\n","\n","from transformers import pipeline\n","\n","# Create a text classification pipeline object\n","classifier = pipeline(\"text-classification\", model='bhadresh-savani/distilbert-base-uncased-emotion', top_k=None)\n","\n","def classify_emotion_from_file(file_path):\n","    with open(file_path, 'r', encoding='utf-8') as file:\n","        content = file.read()\n","\n","    # Split the text into words\n","    words = content.split()\n","\n","    def classify_and_check(predictions, text):\n","        if any(emotion['score'] > 0.8 for emotion in predictions[0]):\n","            return {'Emotion': predictions[0][0]['label'], 'Probability': predictions[0][0]['score']}\n","        else:\n","            return {'Emotion': 'neutral', 'Probability': 1.0}\n","\n","    # Due to the model's limitation in analyzing coherent texts with more than 657 tokens (approximately 320 words),\n","    # such lengthy texts will be split in half and analyzed independently. In our collection of Super Bowl ads from 2013 to 2022,\n","    # only one ad (AD0290) exceeds this limit. Our AI model classified the first part as 'fear' and the second part as 'joy.'\n","    # After a manual analysis of both parts, we would categorize the entire ad as 'joy.'\n","\n","    if len(words) > 320:\n","        # Split the first row into two parts, each containing half of the words\n","        half_length = len(words) // 2\n","        first_row_part1 = ' '.join(words[:half_length])\n","        first_row_part2 = ' '.join(words[half_length:])\n","\n","        # Classify emotions for the first part of the first row\n","        first_row_part1_predictions = classifier(first_row_part1)\n","\n","        # Append information for the first part of the first row\n","        emotions_and_scores_part1 = {\n","            'AD-Number': file_path.split('/')[-1].split('.')[0],\n","            'Transcription': first_row_part1,\n","            'Word range': f'1-{half_length}',\n","            **classify_and_check(first_row_part1_predictions, first_row_part1)\n","        }\n","\n","        # Classify emotions for the second part of the first row\n","        first_row_part2_predictions = classifier(first_row_part2)\n","\n","        # Append information for the second part of the first row\n","        emotions_and_scores_part2 = {\n","            'AD-Number': file_path.split('/')[-1].split('.')[0],\n","            'Transcription': first_row_part2,\n","            'Word range': f'{half_length + 1}-{len(words)}',\n","            **classify_and_check(first_row_part2_predictions, first_row_part2)\n","        }\n","\n","        # Combine information for the first row\n","        emotions_and_scores = [emotions_and_scores_part1, emotions_and_scores_part2]\n","    else:\n","\n","        # Classify emotions for the entire content\n","        full_content_predictions = classifier(content)\n","\n","        # Append information for the first row (full content)\n","        emotions_and_scores = [{\n","            'AD-Number': file_path.split('/')[-1].split('.')[0],\n","            'Transcription': content,\n","            'Word range': f'1-{len(words)}',\n","            **classify_and_check(full_content_predictions, content)\n","        }]\n","\n","    # Classify emotions for each 20-word segment starting from the 2nd row with a 5-word shift\n","    for start in range(0, len(words)-20, 5):\n","        end = min(start + 20, len(words))\n","        word_range = f'{start+1}-{end}'  # Adjust to avoid index out of range\n","        text_segment = ' '.join(words[start:end])\n","\n","        # Classify emotions for the current segment\n","        segment_predictions = classifier(text_segment)\n","\n","        # Append information for each segment\n","        emotions_and_scores.append({\n","            'AD-Number': file_path.split('/')[-1].split('.')[0],\n","            'Transcription': text_segment,\n","            'Word range': word_range,\n","            **classify_and_check(segment_predictions, text_segment)\n","        })\n","\n","\n","    # Check for missing words\n","    remaining_start = max(len(words) - 20, 0)  # Startpunkt für die letzten 20 Wörter\n","    if remaining_start < len(words):\n","        remaining_word_range = f'{remaining_start + 1}-{len(words)}'\n","        remaining_text_segment = ' '.join(words[remaining_start:])\n","        remaining_predictions = classifier(remaining_text_segment)\n","\n","        # Verwendung der classify_and_check-Funktion für die Emotionsklassifikation\n","        remaining_emotion_info = classify_and_check(remaining_predictions, remaining_text_segment)\n","\n","        # Append information for remaining words directly\n","        emotions_and_scores.append({\n","          'AD-Number': file_path.split('/')[-1].split('.')[0],\n","          'Transcription': remaining_text_segment,\n","          'Word range': remaining_word_range,\n","          'Emotion': remaining_emotion_info['Emotion'],\n","          'Probability': remaining_emotion_info['Probability']\n","        })\n","\n","\n","    return emotions_and_scores, emotions_and_scores[0]['AD-Number']\n","\n","\n","def extract_emotions_and_scores(text, predictions, ad_number):\n","    # Extract emotions + probabilities and add \"Word range\" and \"Text segment\" information\n","    emotions_and_scores = []\n","\n","    # Split text into words\n","    words = text.split()\n","    segment_size = 20\n","\n","    for emotion in predictions[0]:\n","        if emotion['score'] > 0.8:\n","            for start in range(0, len(words), segment_size):  # Adjust to 20 words per segment\n","                end = min(start + segment_size, len(words))\n","                word_range = f'{start + 1}-{end}'  # Adjust to avoid index out of range\n","                text_segment = ' '.join(words[start:end])\n","\n","                # Classify emotions for the current segment using the global classifier\n","                segment_predictions = classifier(text_segment)\n","\n","                # Only add relevant information for the first row\n","                if start == 0:\n","                    emotions_and_scores.append({\n","                        'AD-Number': ad_number.split('/')[-1].split('.')[0],\n","                        'Transcription': text,\n","                        'Word range': f'1-{len(words)}',\n","                        'Emotion': emotion['label'],\n","                        'Probability': emotion['score']\n","                    })\n","\n","                # Add information for subsequent rows\n","                emotions_and_scores.append({\n","                    'AD-Number': ad_number.split('/')[-1].split('.')[0],\n","                    'Transcription': text_segment,\n","                    'Word range': word_range,\n","                    'Emotion': segment_predictions[0][0]['label'],  # Assuming top emotion from the model\n","                    'Probability': segment_predictions[0][0]['score']\n","\n","                     })\n","\n","\n","\n","    return emotions_and_scores\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mm1uQuTXkkiy"},"outputs":[],"source":["def process_text_file(file_path, output_base_folder):\n","\n","    predictions, file_name = classify_emotion_from_file(file_path)\n","\n","    # Create directory for output\n","    output_folder = os.path.join(output_base_folder, file_name)\n","    output_folder_name = os.path.basename(output_folder)\n","    output_folder_name = output_folder_name.replace(\".wav\", \"\")\n","    output_folder = os.path.join(os.path.dirname(output_folder), output_folder_name)\n","\n","    # Go through all subfolders\n","    for root, dirs, files in os.walk(output_base_folder):\n","        for dir_name in dirs:\n","            if dir_name not in file_path:\n","                continue\n","\n","            # Create Excel directory\n","            excel_file_path = os.path.join(root, dir_name, f\"{file_name}.xlsx\")\n","\n","            try:\n","                # Try to open existing excel file\n","                with pd.ExcelFile(excel_file_path) as xls:\n","\n","                    # If file exists, add new sheet with predicitions\n","                    result_df_existing = pd.read_excel(xls)\n","                    result_df_new = pd.DataFrame(predictions, columns=['AD-Number', 'Transcription', 'Word range', 'Emotion', 'Probability'])\n","                    result_df_existing = pd.concat([result_df_new], ignore_index=True)\n","\n","                    # save updated data in excel file\n","                    with pd.ExcelWriter(excel_file_path, engine='openpyxl', mode='a') as writer:\n","                        result_df_existing.to_excel(writer, sheet_name=\"Transcription_and_Mood\", index=False)\n","                        print(f\"File {excel_file_path} is updated.\")\n","                        return\n","            except FileNotFoundError:\n","\n","                   # If there is no matching excel file, create a new one\n","\n","                    with pd.ExcelWriter(excel_file_path, engine='openpyxl', mode='w') as writer:\n","                        # Create excel file\n","                        result_df_new = pd.DataFrame(predictions, columns=['AD-Number', 'Transcription', 'Word range', 'Emotion', 'Probability'])\n","                        result_df_new.to_excel(writer, sheet_name=\"Transcription_and_Mood\", index=False)\n","                        print(f\"File {excel_file_path} is created.\")\n","                        break\n"]},{"cell_type":"markdown","source":["### Input and Output"],"metadata":{"id":"4FuVNTED7pKZ"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4312,"status":"ok","timestamp":1708647348242,"user":{"displayName":"Thorben","userId":"17213772383075988098"},"user_tz":-60},"id":"Qk6tnP-Akn_j","outputId":"4ef1cdaf-3096-4e6c-aada-fdff31b13a99"},"outputs":[{"name":"stdout","output_type":"stream","text":["File /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/output_lists/output_lists/ADs_IG_2013/AD0299.xlsx is updated.\n"]}],"source":["# Input\n","input_folder_path = '/content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Input_audio'\n","output_base_folder = '/content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/output_lists/output_lists'\n","\n","# Loop through all files in the folder\n","for root, dirs, files in os.walk(input_folder_path):\n","    for file_name in files:\n","        if file_name.endswith(\".txt\"):\n","            file_path = os.path.join(root, file_name)\n","            process_text_file(file_path, output_base_folder)\n"]},{"cell_type":"markdown","metadata":{"id":"wZJr6yGgqF6f"},"source":["\n","\n","---\n","\n"]},{"cell_type":"markdown","source":["## Combination of emotion from image and audio (2)\n","\n","\n"],"metadata":{"id":"j-tItGZ9W5IH"}},{"cell_type":"markdown","source":["## Emotion from WhisperAI Intervalls"],"metadata":{"id":"0Jefnv7pX004"}},{"cell_type":"markdown","source":["### Set Up"],"metadata":{"id":"9SdbABSwwcSz"}},{"cell_type":"code","source":["import pandas as pd\n","import json\n","import os\n","from transformers import pipeline\n","from openpyxl import load_workbook\n","from openpyxl.utils.dataframe import dataframe_to_rows"],"metadata":{"id":"u6wKKv3zIpvM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Optional\n","# from google.colab import drive\n","# drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hwbaqOc41Ydh","executionInfo":{"status":"ok","timestamp":1709415738181,"user_tz":-60,"elapsed":1629,"user":{"displayName":"Flavio Kuka","userId":"09654499042520950340"}},"outputId":"317679be-e946-435b-cfc1-c267981072b3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"markdown","source":["### Functions"],"metadata":{"id":"X8KcwHSh0k1r"}},{"cell_type":"code","source":["# Funktion zum Emotionsklassifikation und Überprüfung\n","def classify_and_check(predictions):\n","    if any(emotion['score'] > 0.8 for emotion in predictions[0]):\n","        return {'Emotion': predictions[0][0]['label'], 'Probability': predictions[0][0]['score']}\n","    else:\n","        return {'Emotion': 'neutral', 'Probability': 1.0}\n","\n","# Funktion zum Emotionsklassifikation\n","def classify_emotion(text):\n","    classifier = pipeline(\"text-classification\", model='bhadresh-savani/distilbert-base-uncased-emotion', top_k=None)\n","    result = classifier(text)\n","    return result"],"metadata":{"id":"dUku8083IvH0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def process_json_file(json_file_path, excel_file_path):\n","    # Create empty list\n","    all_excel_data = []\n","\n","    # Existing table\n","    existing_df = pd.read_excel(excel_file_path, sheet_name='Transcription_and_Mood')\n","\n","    # load all JSON data\n","    with open(json_file_path) as f:\n","        data = json.load(f)\n","\n","    # extract data name\n","    ad_number = os.path.splitext(os.path.basename(json_file_path))[0]\n","\n","    # extract whole text\n","    full_text = data.get(\"text\", \"\")\n","\n","    # extract relevant information\n","    data_list = data.get(\"segments\", [])\n","\n","    # loop through all segments\n","    for segment in data_list:\n","        row = {\n","            'AD-Number': ad_number,\n","            'ID-Number': segment.get(\"id\", \"\"),\n","            'Start': segment.get(\"start\", \"\"),\n","            'End': segment.get(\"end\", \"\"),\n","            'Transcription': segment.get(\"text\", \"\")\n","        }\n","\n","        # classify emotion\n","        emotion_result = classify_emotion(row['Transcription'])\n","        row.update(classify_and_check(emotion_result))\n","\n","        all_excel_data.append(row)\n","\n","    # add first row\n","    first_row = {\n","        'AD-Number': ad_number,\n","        'ID-Number': '',\n","        'Start': '',\n","        'End': '',\n","        'Transcription': full_text\n","    }\n","\n","    if len(full_text) > 320:\n","      # Copy the emotion from the first table\n","      first_row.update({'Emotion': existing_df.at[0, 'Emotion'], 'Probability': 1.0})\n","\n","    else:\n","      # classify emotion + add results\n","      emotion_result_first_row = classify_emotion(first_row['Transcription'])\n","      first_row.update(classify_and_check(emotion_result_first_row))\n","\n","    all_excel_data.insert(0, first_row)\n","\n","    # Create a new DataFrame with your new data\n","    new_data_df = pd.DataFrame(all_excel_data)\n","\n","    # Insert the new DataFrame\n","    existing_df[' '] = ''\n","    existing_df = pd.concat([existing_df, new_data_df], axis=1)\n","\n","    # Load the existing workbook using openpyxl\n","    workbook = load_workbook(excel_file_path)\n","\n","    # get the 'Transcription_and_Mood' sheet\n","    sheet = workbook['Transcription_and_Mood']\n","\n","    # Write the updated DataFrame to the Excel sheet\n","    for r_idx, row in enumerate(dataframe_to_rows(existing_df, index=False, header=True), 1):\n","        for c_idx, value in enumerate(row, 1):\n","            sheet.cell(row=r_idx, column=c_idx, value=value)\n","\n","    # Save the updated workbook\n","    workbook.save(excel_file_path)"],"metadata":{"id":"4xdz4hR6I7Mz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Run Code"],"metadata":{"id":"QWeqtLQ20paw"}},{"cell_type":"code","source":["# Path to the folder containing the frames and the excel lists\n","json_file_folder_path = '/content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/JSON_Dateien'\n","excel_file_folder_path = '/content/drive/MyDrive/SuperBowl_Project_FUB/output_lists'"],"metadata":{"id":"Ka2AVThyb5xc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# OPTIONAL: So that the loops starts with the years in an alphabetical order\n","years = []\n","for year in os.listdir(excel_file_folder_path):\n","  years.append(year)\n","years.sort()"],"metadata":{"id":"6eANlqg81VzB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for year in years:\n","    # Set paths\n","    json_files_year_path = os.path.join(json_file_folder_path, f'{year}_json')\n","    excel_file_year_path = os.path.join(excel_file_folder_path, year)\n","\n","    # Create sets of base file names\n","    json_files_year_set = {os.path.splitext(file)[0] for file in os.listdir(json_files_year_path)}\n","    excel_files_year_set = {os.path.splitext(file)[0] for file in os.listdir(excel_file_year_path)}\n","\n","    # Find common base names\n","    common_base_names = json_files_year_set.intersection(excel_files_year_set)\n","\n","    # Iterate over common base names\n","    for base_name in common_base_names:\n","        json_file_path = os.path.join(json_files_year_path, f'{base_name}.json')\n","        excel_file_path = os.path.join(excel_file_year_path, f'{base_name}.xlsx')\n","        try:\n","          process_json_file(json_file_path, excel_file_path)\n","        except:\n","          print(f\"fail: {base_name}\")"],"metadata":{"id":"8_wd05jv1xMS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Analysis Emotion Image & Audio"],"metadata":{"id":"V0kMYUJ5YDoG"}},{"cell_type":"markdown","source":["### Set Up"],"metadata":{"id":"q52BXAJzYn8R"}},{"cell_type":"code","source":["!pip install XlsxWriter"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8YU1Z5y3q-yh","executionInfo":{"status":"ok","timestamp":1709458592187,"user_tz":-60,"elapsed":5128,"user":{"displayName":"Flavio Kuka","userId":"09654499042520950340"}},"outputId":"d4190464-069a-4fcd-e522-671536d77e26"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting XlsxWriter\n","  Downloading XlsxWriter-3.2.0-py3-none-any.whl (159 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/159.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m159.9/159.9 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: XlsxWriter\n","Successfully installed XlsxWriter-3.2.0\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","import re\n","from collections import Counter\n","import os\n","from openpyxl import load_workbook\n","from openpyxl.utils.dataframe import dataframe_to_rows\n","import math\n","import xlsxwriter"],"metadata":{"id":"uRA3k5WtvvVn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Optional\n","# from google.colab import drive\n","# drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WjftN_ykvu4H","executionInfo":{"status":"ok","timestamp":1709458457633,"user_tz":-60,"elapsed":27444,"user":{"displayName":"Flavio Kuka","userId":"09654499042520950340"}},"outputId":"adb49219-d6ef-4c7e-9eec-d529f9baa424"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["# Create the mapping dictionary for frames to seconds\n","mapping_dict = {}\n","for i in range(0, 4800, 10):\n","    new_number = ((i // 10) // 3) + 1\n","    mapping_dict[i] = new_number"],"metadata":{"id":"ZFldY_ruwruD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create the mapping for the emotions from text to image\n","emotion_mapping_dict = {\n","    'joy': 'happy',\n","    'love': 'happy',\n","    'neutral': 'neutral',\n","    'anger': 'angry',\n","    'surprise': 'surprise',\n","    'fear': 'fear',\n","    'sadness': 'sad'\n","}"],"metadata":{"id":"3Gl45arPgKy3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Find corresponding Frame_Nr for the second"],"metadata":{"id":"4rEo3MezwgEO"}},{"cell_type":"code","source":["def comparison_emotions_image_audio(excel_file_path):\n","  # Read the Sheets\n","  emotion_image = pd.read_excel(f'{excel_file_path}', sheet_name='Predictions')\n","  emotion_audio = pd.read_excel(f'{excel_file_path}', sheet_name='Transcription_and_Mood')\n","\n","  # Create new columns\n","  emotion_audio['Frame_Nr'] = ''\n","  emotion_audio['Emotions'] = ''\n","  emotion_audio['Dominant_Emotion'] = ''\n","  emotion_audio['Correct_%'] = ''\n","  emotion_audio['Equal_Emotions'] = ''\n","\n","  for index, start_second in emotion_audio['Start'].items():\n","    # Check if start_second is not NaN\n","    if pd.notna(start_second):\n","        # Define the seconds in the interval\n","        end_second = emotion_audio.at[index, 'End']\n","        seconds_interval = list(range(int(start_second)+1, int(end_second)+1))\n","\n","        # Create a list of corresponding frames\n","        frames = []\n","        for second in seconds_interval:\n","            for key, value in mapping_dict.items():\n","                if value == round(second):\n","                    frames.append(key)\n","        # Save the corresponding frames to the dataframe\n","        emotion_audio.at[index, 'Frame_Nr'] = frames\n","\n","  # Find corresponding emotion\n","  for index_audio, frame_audio in emotion_audio['Frame_Nr'].items():\n","    corresponding_emotions = []\n","    if str(frame_audio) != '':\n","      # Iterate for each frame_nr\n","      for frame_nr_audio in frame_audio:\n","        for index_image, frame_video in emotion_image['video_frame'].items():\n","          # Find the corresponding frame_nr from the image analysis\n","          frame_nr_video = frame_video[13:]\n","          frame_nr_video = frame_nr_video.split('.')[0]\n","          if str(frame_nr_audio) == str(frame_nr_video):\n","            # Find the corresponding emotion\n","            corresponding_emotion = emotion_image.at[index_image, 'emotion_prediction']\n","            if corresponding_emotion != '-':\n","              corresponding_emotions.append(corresponding_emotion)\n","      # Save the identified emotions\n","      emotion_audio.at[index_audio, 'Emotions'] = corresponding_emotions\n","\n","  # Find dominant emotion\n","  for index_audio, emotions_image in emotion_audio['Emotions'].items():\n","    if len(emotions_image) > 0:\n","      dominant_emotion = Counter(emotions_image).most_common(1)[0][0]\n","      emotion_audio.at[index_audio, 'Dominant_Emotion'] = dominant_emotion\n","\n","  # Find Correct % and if both Emotions are equal\n","  for index_audio, total_emotion_audio in emotion_audio['Emotion.1'].items():\n","    if str(total_emotion_audio) != 'nan':\n","      transformed_emotion = emotion_mapping_dict[str(total_emotion_audio)]\n","      emotions_from_image = emotion_audio.at[index_audio, 'Emotions']\n","      correct_emotion_count = emotions_from_image.count(transformed_emotion)\n","      total_emotions = len(emotions_from_image)\n","      if total_emotions != 0:\n","        emotion_audio.at[index_audio, 'Correct_%'] = (correct_emotion_count/ total_emotions)\n","      else:\n","        emotion_audio.at[index_audio, 'Correct_%'] = 0\n","\n","      dominant_emotion_image = emotion_audio.at[index_audio, 'Dominant_Emotion']\n","      if  transformed_emotion == dominant_emotion_image:\n","        emotion_audio.at[index_audio, 'Equal_Emotions'] = 1\n","      else:\n","        emotion_audio.at[index_audio, 'Equal_Emotions'] = 0\n","\n","  # Insert summary information\n","  emotion_audio['   '] = ''\n","  emotion_audio['Average_Correct_%'] = ''\n","  emotion_audio['Average_Equal_Emotions'] = ''\n","  if len(list(emotion_audio['Emotion.1'].items())) > 0:\n","      emotion_audio['Correct_%'] = pd.to_numeric(emotion_audio['Correct_%'], errors='coerce')\n","      emotion_audio['Equal_Emotions'] = pd.to_numeric(emotion_audio['Equal_Emotions'], errors='coerce')\n","      emotion_audio.at[0, 'Average_Correct_%'] = emotion_audio['Correct_%'].mean()\n","      emotion_audio.at[0, 'Average_Equal_Emotions'] = emotion_audio['Equal_Emotions'].mean()\n","\n","  # Read exel file\n","  excel_sheets = pd.read_excel(excel_file_path, sheet_name=None)\n","\n","  # Replace \"Unnamed\" columns with empty strings in all dataframes\n","  for sheet_name, df in excel_sheets.items():\n","      df.columns = [col if 'Unnamed' not in str(col) else '' for col in df.columns]\n","\n","  # Update the 'Transcription_and_Mood' sheet in the dictionary\n","  excel_sheets['Transcription_and_Mood'] = emotion_audio\n","\n","  # Save the modified dictionary of dataframes back to the Excel file\n","  with pd.ExcelWriter(excel_file_path, engine='xlsxwriter') as writer:\n","      for sheet_name, df in excel_sheets.items():\n","          df.to_excel(writer, sheet_name=sheet_name, index=False)"],"metadata":{"id":"YYieyE2CwPOH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Run"],"metadata":{"id":"bC-h8259yoPl"}},{"cell_type":"code","source":["excel_file_folder_path = '/content/drive/MyDrive/SuperBowl_Project_FUB/output_lists'"],"metadata":{"id":"TkGyHorj_lJQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# OPTIONAL: So that the loops starts with the years in an alphabetical order\n","years = []\n","for year in os.listdir(excel_file_folder_path):\n","  years.append(year)\n","years.sort()"],"metadata":{"id":"-oFx1bf2Yn8k"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for year in years:\n","    # Set path\n","    excel_file_year_path = os.path.join(excel_file_folder_path, year)\n","\n","    # Create list of files\n","    excel_files = os.listdir(excel_file_year_path)\n","\n","    # Iterate over files\n","    for excel_file in excel_files:\n","      excel_file_path = os.path.join(excel_file_year_path, excel_file)\n","\n","      try:\n","        comparison_emotions_image_audio(excel_file_path)\n","      except:\n","        print(excel_file)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1709459334450,"user_tz":-60,"elapsed":445207,"user":{"displayName":"Flavio Kuka","userId":"09654499042520950340"}},"outputId":"8927ad7b-6b8a-476d-8b43-0d16592a9f24","id":"wIKg409kYn8l"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["AD0347.xlsx\n","AD0471_03.xlsx\n","AD0471_01.xlsx\n","AD0471_02.xlsx\n","AD0731_FROMadsoftheworld.xlsx\n","AD0754.xlsx\n"]}]},{"cell_type":"markdown","metadata":{"id":"dOSIkq0O2vI5"},"source":["# *3. Acoustic Indices (1.0.1)*\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"gB0pxe3s2-Gd"},"source":["This module aims to extract audio features of the given ads (audio-files)\n","\n","Acoustic Indices: https://github.com/patriceguyot/Acoustic_Indices\n","\n","Pydub: https://github.com/jiaaro/pydub"]},{"cell_type":"markdown","metadata":{"id":"FTjB61sx3Hx3"},"source":["### Set up"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Po2USJPg3WOj"},"outputs":[],"source":["#!/usr/bin/env python"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"9gd3_BUK3a5t","outputId":"2f3e149e-944a-4adc-ea4a-d29acdb9aaef"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.25.2)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (1.11.4)\n","Requirement already satisfied: numpy<1.28.0,>=1.21.6 in /usr/local/lib/python3.10/dist-packages (from scipy) (1.25.2)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.2.0)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.49.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\n","Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.25.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (23.2)\n","Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.4.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.1)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (6.0.1)\n","Requirement already satisfied: librosa in /usr/local/lib/python3.10/dist-packages (0.10.1)\n","Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.10/dist-packages (from librosa) (3.0.1)\n","Requirement already satisfied: numpy!=1.22.0,!=1.22.1,!=1.22.2,>=1.20.3 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.25.2)\n","Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.11.4)\n","Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.2.2)\n","Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.3.2)\n","Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (4.4.2)\n","Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.58.1)\n","Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.12.1)\n","Requirement already satisfied: pooch>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.8.0)\n","Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.3.7)\n","Requirement already satisfied: typing-extensions>=4.1.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (4.9.0)\n","Requirement already satisfied: lazy-loader>=0.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.3)\n","Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.0.7)\n","Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.0->librosa) (0.41.1)\n","Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.0->librosa) (4.2.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.0->librosa) (23.2)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.0->librosa) (2.31.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->librosa) (3.3.0)\n","Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from soundfile>=0.12.1->librosa) (1.16.0)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.21)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa) (2024.2.2)\n","Collecting python_speech_features\n","  Downloading python_speech_features-0.6.tar.gz (5.6 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Building wheels for collected packages: python_speech_features\n","  Building wheel for python_speech_features (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for python_speech_features: filename=python_speech_features-0.6-py3-none-any.whl size=5869 sha256=6db0817ba674ed3d7380307e6f718cf7fd195bbc3a5716de2e4d526ce7c767cd\n","  Stored in directory: /root/.cache/pip/wheels/5a/9e/68/30bad9462b3926c29e315df16b562216d12bdc215f4d240294\n","Successfully built python_speech_features\n","Installing collected packages: python_speech_features\n","Successfully installed python_speech_features-0.6\n","Collecting pydub\n","  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n","Installing collected packages: pydub\n","Successfully installed pydub-0.25.1\n"]}],"source":["!pip install numpy\n","!pip install scipy\n","!pip install matplotlib\n","!pip install pyyaml\n","\n","!pip install librosa\n","!pip install python_speech_features\n","\n","!pip install pydub"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"CGxhU3OO3tz8"},"outputs":[],"source":["import yaml\n","from scipy import signal\n","from csv import writer\n","import argparse\n","import os\n","\n","import cv2\n","import librosa\n","import librosa.display\n","from python_speech_features import mfcc\n","import wave\n","import audioop\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","# compression rate\n","from pydub import AudioSegment\n","\n","# Excel Export\n","import pandas as pd\n","from openpyxl.styles import Font"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18683,"status":"ok","timestamp":1708809950260,"user":{"displayName":"Thorben Schlieffen","userId":"06537630395058694731"},"user_tz":-60},"id":"A_uNmIVF3eSl","outputId":"19df4cc5-2f87-4a59-be7f-afd15c8d9044"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["  # Optional\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"JuvMI44hotqJ"},"source":["### Copy & import relevant files"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dqqAeHkK4BC9"},"outputs":[],"source":["!cp /content/drive/MyDrive/SuperBowl_Project_FUB/Colab_Notebooks/Acoustic_Indices/compute_indice.py .\n","!cp /content/drive/MyDrive/SuperBowl_Project_FUB/Colab_Notebooks/Acoustic_Indices/acoustic_index.py .\n","!cp /content/drive/MyDrive/SuperBowl_Project_FUB/Colab_Notebooks/Acoustic_Indices/yaml/config_014_butter.yaml ."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6OyVSFqEo69Q"},"outputs":[],"source":["# import sys\n","# sys.path.append('/content/drive/MyDrive/SuperBowl_Project_FUB/Colab_Notebooks/Acoustic_Indices')\n","\n","from compute_indice import *\n","from acoustic_index import *"]},{"cell_type":"markdown","metadata":{"id":"HHJzI9D7pJH9"},"source":["### Run the Code"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MHdPXCL64V_m"},"outputs":[],"source":["# If True, only one file \"outputs.xlsx\" is created.\n","# If False, a separate output file is created for each ad.\n","single_output_file = False\n","\n","config_file = f\"/content/drive/MyDrive/SuperBowl_Project_FUB/Colab_Notebooks/Acoustic_Indices/yaml/config_014_butter.yaml\"\n","audio_dir = f\"/content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Input_audio\"\n","# output_csv_file = f\"/content/drive/MyDrive/SuperBowl_Project_FUB/Colab_Notebooks/Acoustic_Indices-1.0.1/outputs.csv\"\n","output_dir = \"/content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/output_lists/output_lists\""]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":193},"executionInfo":{"elapsed":270,"status":"error","timestamp":1708809965539,"user":{"displayName":"Thorben Schlieffen","userId":"06537630395058694731"},"user_tz":-60},"id":"bD_Pds7h4XrV","outputId":"13cff867-5690-4225-936d-23d10f5d67d0"},"outputs":[{"name":"stdout","output_type":"stream","text":["Config file:  /content/drive/MyDrive/SuperBowl_Project_FUB/Colab_Notebooks/Acoustic_Indices/yaml/config_014_butter.yaml\n"]},{"ename":"NameError","evalue":"name 'yaml' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-7-a41d9e9c0373>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Config file: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myml_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myml_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mdata_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0myaml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLoader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0myaml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFullLoader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'yaml' is not defined"]}],"source":["# Set config file\n","yml_file = config_file\n","print(\"Config file: \", yml_file)\n","with open(yml_file, 'r') as stream:\n","    data_config = yaml.load(stream, Loader=yaml.FullLoader)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":299,"status":"ok","timestamp":1708808977152,"user":{"displayName":"Thorben Schlieffen","userId":"06537630395058694731"},"user_tz":-60},"id":"_Ly6BYmJ4aiF","outputId":"a9b7f926-c3d3-47ed-cc80-23bb0c3de9c5"},"outputs":[{"name":"stdout","output_type":"stream","text":["- 53 files found in the directory /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav :\n","\n"]}],"source":["# Get audio files\n","all_audio_file_path = []\n","for path, subdirs, files in os.walk(audio_dir):\n","    for name in files:\n","        if name.endswith(\".wav\") and not name.startswith(\".\"):\n","            all_audio_file_path.append(os.path.join(path, name))\n","\n","all_audio_file_path = sorted(all_audio_file_path)\n","\n","print(\"-\", len(all_audio_file_path), \"files found in the directory\", audio_dir, ':\\n')"]},{"cell_type":"markdown","metadata":{"id":"ay5j1mBspcaz"},"source":["### In case the ad list needs to be modified, use the list below.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uHSMrIdPpoYu"},"outputs":[],"source":["#all_audio_file_path = []\n","#print(all_audio_file_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z8ps4GcE4eFk"},"outputs":[],"source":["# Initialize an empty DataFrame for all ads\n","all_data = pd.DataFrame()"]},{"cell_type":"markdown","metadata":{"id":"A3um4wUgpsng"},"source":["### additional values"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"69jImfOx4hCI"},"outputs":[],"source":["def calculate_additional_values(y):\n","    duration = librosa.get_duration(y=y)\n","    tempo, _ = librosa.beat.beat_track(y=y)\n","    db_values = librosa.amplitude_to_db(np.abs(librosa.stft(y)), ref=np.max)\n","    avg_db = np.mean(db_values)\n","    min_db = np.min(db_values)\n","    max_db = np.max(db_values)\n","    max_db_value = np.max(db_values)\n","    return duration, tempo, avg_db, min_db, max_db, max_db_value"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EpIplz6wqOYq"},"outputs":[],"source":["def compress_wav_to_mp3(input_wav_path, output_mp3_path, bitrate='192k'):\n","    audio = AudioSegment.from_wav(input_wav_path)\n","    audio.export(output_mp3_path, format='mp3', bitrate=bitrate)\n","\n","def measure_compression_ratio(original_size, compressed_size, original_duration):\n","    compression_ratio = ((original_size - compressed_size) / original_size) * 100\n","    return compression_ratio\n","\n","def measure_compression_ratio_per_second(compression_ratio, original_duration):\n","    compression_ratio_per_second = compression_ratio / original_duration\n","    return compression_ratio_per_second\n","\n","def delete_file(file_path):\n","    \"\"\"\n","    Delete a file if it exists.\n","\n","    Parameters:\n","    - file_path: Path to the file to be deleted.\n","    \"\"\"\n","    if os.path.exists(file_path):\n","        os.remove(file_path)\n","        print(f\"File {file_path} deleted.\")\n","    else:\n","        print(f\"File {file_path} does not exist.\")"]},{"cell_type":"markdown","metadata":{"id":"MNVJCe7tqWYS"},"source":["### Main"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IgEqSfcs4p77"},"outputs":[],"source":["#parser = argparse.ArgumentParser()\n","#parser.add_argument(\"config_file\", help='yaml config file', nargs='?', const='yaml/config_014_butter.yaml', default='yaml/config_014_butter.yaml', type=str)\n","#parser.add_argument(\"audio_dir\", help='audio directory', nargs='?', const='audio_files', default='audio_files', type=str)\n","#parser.add_argument(\"output_csv_file\", help='output csv file', nargs='?', const='dict_all.csv', default='dict_all.csv', type=str)\n","#args =parser.parse_args()\n","\n","if single_output_file:\n","\n","  print(\"audio directory: \", audio_dir)\n","  print(\"output_excel_file: \", output_excel_file)\n","\n","  for idx_file, filename in enumerate(all_audio_file_path):\n","\n","      print(f'###### CURRENT AD: {filename} ######')\n","      print(f'###### - {all_audio_file_path.index(filename)} / {len(all_audio_file_path)} - ######')\n","\n","      # Read signal -------------------------------------\n","      file = AudioFile(filename, verbose=True)\n","\n","      # Pre-processing -----------------------------------------------------------------------------------\n","      if 'Filtering' in data_config:\n","          if data_config['Filtering']['type'] == 'butterworth':\n","              print('- Pre-processing - High-Pass Filtering:', data_config['Filtering'])\n","              freq_filter = data_config['Filtering']['frequency']\n","              Wn = freq_filter/float(file.niquist)\n","              order = data_config['Filtering']['order']\n","              [b,a] = signal.butter(order, Wn, btype='highpass')\n","              # to plot the frequency response\n","              #w, h = signal.freqz(b, a, worN=2000)\n","              #plt.plot((file.sr * 0.5 / np.pi) * w, abs(h))\n","              #plt.show()\n","              file.process_filtering(signal.filtfilt(b, a, file.sig_float))\n","          elif data_config['Filtering']['type'] == 'windowed_sinc':\n","              print('- Pre-processing - High-Pass Filtering:', data_config['Filtering'])\n","              freq_filter = data_config['Filtering']['frequency']\n","              fc = freq_filter / float(file.sr)\n","              roll_off = data_config['Filtering']['roll_off']\n","              b = roll_off / float(file.sr)\n","              N = int(np.ceil((4 / b)))\n","              if not N % 2: N += 1  # Make sure that N is odd.\n","              n = np.arange(N)\n","              # Compute a low-pass filter.\n","              h = np.sinc(2 * fc * (n - (N - 1) / 2.))\n","              w = np.blackman(N)\n","              h = h * w\n","              h = h / np.sum(h)\n","              # Create a high-pass filter from the low-pass filter through spectral inversion.\n","              h = -h\n","              h[(N - 1) / 2] += 1\n","              file.process_filtering(np.convolve(file.sig_float, h))\n","\n","      # Compute Indices -----------------------------------------------------------------------------------\n","      print('- Compute Indices')\n","      ci = data_config['Indices']  # use to simplify the notation\n","      for index_name in ci:  # iterate over the index names (key of dictionary in the yml file)\n","\n","          if index_name == 'Acoustic_Complexity_Index':\n","              print('\\tCompute', index_name)\n","              spectro, _ = compute_spectrogram(file, **ci[index_name]['spectro'])\n","              methodToCall = globals().get(ci[index_name]['function'])\n","              j_bin = int(ci[index_name]['arguments']['j_bin'] * file.sr / ci[index_name]['spectro']['windowHop'])  # transform j_bin in samples\n","              main_value, temporal_values = methodToCall(spectro, j_bin)\n","              file.indices[index_name] = Index(index_name, temporal_values=temporal_values, main_value=main_value)\n","\n","          elif index_name == 'Acoustic_Diversity_Index':\n","              print('\\tCompute', index_name)\n","              methodToCall = globals().get(ci[index_name]['function'])\n","              freq_band_Hz = ci[index_name]['arguments']['max_freq'] / ci[index_name]['arguments']['freq_step']\n","              windowLength = int(file.sr / freq_band_Hz)\n","              spectro, _ = compute_spectrogram(file, windowLength=windowLength, windowHop=windowLength, scale_audio=True, square=False, windowType='hamming', centered=False, normalized=False)\n","              main_value = methodToCall(spectro, freq_band_Hz, **ci[index_name]['arguments'])\n","              file.indices[index_name] = Index(index_name, main_value=main_value)\n","\n","          elif index_name == 'Acoustic_Evenness_Index':\n","              print('\\tCompute', index_name)\n","              methodToCall = globals().get(ci[index_name]['function'])\n","              freq_band_Hz = ci[index_name]['arguments']['max_freq'] / ci[index_name]['arguments']['freq_step']\n","              windowLength = int(file.sr / freq_band_Hz)\n","              spectro, _ = compute_spectrogram(file, windowLength=windowLength, windowHop=windowLength, scale_audio=True, square=False, windowType='hamming', centered=False, normalized=False)\n","              main_value = methodToCall(spectro, freq_band_Hz, **ci[index_name]['arguments'])\n","              file.indices[index_name] = Index(index_name, main_value=main_value)\n","\n","          elif index_name == 'Bio_acoustic_Index':\n","              print('\\tCompute', index_name)\n","              spectro, frequencies = compute_spectrogram(file, **ci[index_name]['spectro'])\n","              methodToCall = globals().get(ci[index_name]['function'])\n","              main_value = methodToCall(spectro, frequencies, **ci[index_name]['arguments'])\n","              file.indices[index_name] = Index(index_name, main_value=main_value)\n","\n","          elif index_name == 'Normalized_Difference_Sound_Index':\n","              print('\\tCompute', index_name)\n","              methodToCall = globals().get(ci[index_name]['function'])\n","              main_value = methodToCall(file, **ci[index_name]['arguments'])\n","              file.indices[index_name] = Index(index_name, main_value=main_value)\n","\n","          elif index_name == 'RMS_energy':\n","              print('\\tCompute', index_name)\n","              methodToCall = globals().get(ci[index_name]['function'])\n","              temporal_values = methodToCall(file, **ci[index_name]['arguments'])\n","              file.indices[index_name] = Index(index_name, temporal_values=temporal_values)\n","\n","          elif index_name == 'Spectral_centroid':\n","              print('\\tCompute', index_name)\n","              spectro, frequencies = compute_spectrogram(file, **ci[index_name]['spectro'])\n","              methodToCall = globals().get(ci[index_name]['function'])\n","              temporal_values = methodToCall(spectro, frequencies)\n","              file.indices[index_name] = Index(index_name, temporal_values=temporal_values)\n","\n","          elif index_name == 'Spectral_Entropy':\n","              print('\\tCompute', index_name)\n","              spectro, _ = compute_spectrogram(file, **ci[index_name]['spectro'])\n","              methodToCall = globals().get(ci[index_name]['function'])\n","              main_value = methodToCall(spectro)\n","              file.indices[index_name] = Index(index_name, main_value=main_value)\n","\n","          elif index_name == 'Temporal_Entropy':\n","              print('\\tCompute', index_name)\n","              methodToCall = globals().get(ci[index_name]['function'])\n","              main_value = methodToCall(file, **ci[index_name]['arguments'])\n","              file.indices[index_name] = Index(index_name, main_value=main_value)\n","\n","          elif index_name == 'ZCR':\n","              print('\\tCompute', index_name)\n","              methodToCall = globals().get(ci[index_name]['function'])\n","              temporal_values = methodToCall(file, **ci[index_name]['arguments'])\n","              file.indices[index_name] = Index(index_name, temporal_values=temporal_values)\n","\n","          elif index_name == 'Wave_SNR':\n","              print('\\tCompute', index_name)\n","              methodToCall = globals().get(ci[index_name]['function'])\n","              values = methodToCall(file, **ci[index_name]['arguments'])\n","              file.indices[index_name] = Index(index_name, values=values)\n","\n","          elif index_name == 'NB_peaks':\n","              print('\\tCompute', index_name)\n","              spectro, frequencies = compute_spectrogram(file, **ci[index_name]['spectro'])\n","              methodToCall = globals().get(ci[index_name]['function'])\n","              main_value = methodToCall(spectro, frequencies, **ci[index_name]['arguments'])\n","              file.indices[index_name] = Index(index_name, main_value=main_value)\n","\n","          elif index_name == 'Acoustic_Diversity_Index_NR': # Acoustic_Diversity_Index with Noise Removed spectrograms\n","              print('\\tCompute', index_name)\n","              methodToCall = globals().get(ci[index_name]['function'])\n","              freq_band_Hz = ci[index_name]['arguments']['max_freq'] / ci[index_name]['arguments']['freq_step']\n","              windowLength = int(file.sr / freq_band_Hz)\n","              spectro,_ = compute_spectrogram(file, windowLength=windowLength, windowHop= windowLength, scale_audio=True, square=False, windowType='hamming', centered=False, normalized= False )\n","              spectro_noise_removed = remove_noiseInSpectro(spectro, **ci[index_name]['remove_noiseInSpectro'])\n","              main_value = methodToCall(spectro_noise_removed, freq_band_Hz, **ci[index_name]['arguments'])\n","              file.indices[index_name] = Index(index_name, main_value=main_value)\n","\n","          elif index_name == 'Acoustic_Evenness_Index_NR': # Acoustic_Evenness_Index with Noise Removed spectrograms\n","              print('\\tCompute', index_name)\n","              methodToCall = globals().get(ci[index_name]['function'])\n","              freq_band_Hz = ci[index_name]['arguments']['max_freq'] / ci[index_name]['arguments']['freq_step']\n","              windowLength = int(file.sr / freq_band_Hz)\n","              spectro,_ = compute_spectrogram(file, windowLength=windowLength, windowHop= windowLength, scale_audio=True, square=False, windowType='hamming', centered=False, normalized= False )\n","              spectro_noise_removed = remove_noiseInSpectro(spectro, **ci[index_name]['remove_noiseInSpectro'])\n","              main_value = methodToCall(spectro_noise_removed, freq_band_Hz, **ci[index_name]['arguments'])\n","              file.indices[index_name] = Index(index_name, main_value=main_value)\n","\n","          elif index_name == 'Bio_acoustic_Index_NR': # Bio_acoustic_Index with Noise Removed spectrograms\n","              print('\\tCompute', index_name)\n","              spectro, frequencies = compute_spectrogram(file, **ci[index_name]['spectro'])\n","              spectro_noise_removed = remove_noiseInSpectro(spectro, **ci[index_name]['remove_noiseInSpectro'])\n","              methodToCall = globals().get(ci[index_name]['function'])\n","              main_value = methodToCall(spectro_noise_removed, frequencies, **ci[index_name]['arguments'])\n","              file.indices[index_name] = Index(index_name, main_value=main_value)\n","\n","          elif index_name == 'Spectral_Entropy_NR': # Spectral_Entropy with Noise Removed spectrograms\n","              print('\\tCompute', index_name)\n","              spectro, _ = compute_spectrogram(file, **ci[index_name]['spectro'])\n","              spectro_noise_removed = remove_noiseInSpectro(spectro, **ci[index_name]['remove_noiseInSpectro'])\n","              methodToCall = globals().get(ci[index_name]['function'])\n","              main_value = methodToCall(spectro_noise_removed)\n","              file.indices[index_name] = Index(index_name, main_value=main_value)\n","\n","      # Output Indices -----------------------------------------------------------------------------------\n","      #if idx_file == 0: # wenn ertse Datei\n","      #    with open(output_csv_file, 'w') as f_object:\n","      #        writer_object = writer(f_object)\n","      #        keys = ['filename']\n","      #        values = [file.file_name]\n","      #        for idx, current_index in file.indices.items():\n","      #            for key, value in current_index.__dict__.items():\n","      #                if key != 'name':\n","      #                    keys.append(idx + '__' + key)\n","      #                    values.append(value)\n","      #        writer_object.writerow(keys)\n","      #        writer_object.writerow(values)\n","      #        f_object.close()\n","      #else: # alles nach der ersten Datei\n","      #    with open(output_csv_file, 'a') as f_object:\n","      #        writer_object = writer(f_object)\n","      #        values = [file.file_name]\n","      #        for idx, current_index in file.indices.items():\n","      #            for key, value in current_index.__dict__.items():\n","      #                if key != 'name':\n","      #                    values.append(value)\n","      #        writer_object.writerow(values)\n","      #        f_object.close()\n","      #print(\"\\n\")\n","      # Create a dictionary to store data for the current file\n","\n","      file_data = {'filename': file.file_name}\n","\n","      for idx, current_index in file.indices.items():\n","          for key, value in current_index.__dict__.items():\n","              if key != 'name':\n","                  file_data[idx + '__' + key] = value\n","\n","      # Calculate additional values\n","      additional_values = calculate_additional_values(file.sig_float)\n","\n","      # compression rate\n","      original_size = os.path.getsize(filename)\n","      audio = AudioSegment.from_wav(filename)\n","      original_duration = audio.duration_seconds\n","      mp3_path = os.path.join(filename.replace(\".wav\", \".mp3\"))\n","      compress_wav_to_mp3(filename, mp3_path)\n","      compressed_size = os.path.getsize(mp3_path)\n","      compression_ratio = measure_compression_ratio(original_size, compressed_size, original_duration)\n","      compression_ratio_per_second = measure_compression_ratio_per_second(compression_ratio, original_duration)\n","      delete_file(mp3_path)\n","\n","      # Append additional values to the file_data dictionary\n","      duration, tempo, avg_db, min_db, max_db, max_db_value = additional_values\n","      file_data['duration'] = duration\n","      file_data['tempo'] = tempo\n","      file_data['avg_db'] = avg_db\n","      file_data['min_db'] = min_db\n","      file_data['max_db'] = max_db\n","      file_data['max_db_value'] = max_db_value\n","      file_data['compression_ratio'] = compression_ratio\n","      file_data['compression_ratio_per_second'] = compression_ratio_per_second\n","\n","      # Append the data for the current file to the DataFrame\n","      all_data = all_data.append(file_data, ignore_index=True)\n","\n","  #To Excel\n","  with pd.ExcelWriter(output_excel_file, engine='openpyxl') as writer:\n","    all_data.to_excel(writer, sheet_name='Acoustic_Indices', index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":765261,"status":"ok","timestamp":1708809758797,"user":{"displayName":"Thorben Schlieffen","userId":"06537630395058694731"},"user_tz":-60},"id":"REsHzfJY4rGb","outputId":"414a3890-bff3-445e-f8ae-da9ca87c79f8"},"outputs":[{"name":"stdout","output_type":"stream","text":["audio directory:  /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav\n","output_directory:  /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/output_lists/output_lists\n","###### CURRENT AD: /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0688.wav ######\n","###### - 0 / 53 - ######\n","Read the audio file: /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0688.wav\n","\tSuccessful read of the audio file: /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0688.wav\n","\tThe audio file contains more than one channel. Only the channel 0 will be used.\n","- Pre-processing - High-Pass Filtering: {'type': 'butterworth', 'order': 8, 'frequency': 300}\n","- Compute Indices\n","\tCompute Acoustic_Complexity_Index\n","\tCompute Acoustic_Diversity_Index\n","\tCompute Acoustic_Evenness_Index\n","\tCompute Bio_acoustic_Index\n","\tCompute Normalized_Difference_Sound_Index\n","\tCompute RMS_energy\n","\tCompute Spectral_centroid\n","\tCompute Spectral_Entropy\n","\tCompute Temporal_Entropy\n","\tCompute ZCR\n","\tCompute Wave_SNR\n","\tCompute NB_peaks\n","\tCompute Acoustic_Diversity_Index_NR\n","\tCompute Acoustic_Evenness_Index_NR\n","\tCompute Bio_acoustic_Index_NR\n","\tCompute Spectral_Entropy_NR\n","File /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0688.mp3 deleted.\n","/content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/output_lists/output_lists/ADs_IG_2021/AD0688.xlsx\n"]},{"name":"stderr","output_type":"stream","text":["<ipython-input-46-020a70f8a660>:238: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  all_data = all_data.append(file_data, ignore_index=True)\n"]},{"name":"stdout","output_type":"stream","text":["###### CURRENT AD: /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0689.wav ######\n","###### - 1 / 53 - ######\n","Read the audio file: /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0689.wav\n","\tSuccessful read of the audio file: /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0689.wav\n","\tThe audio file contains more than one channel. Only the channel 0 will be used.\n","- Pre-processing - High-Pass Filtering: {'type': 'butterworth', 'order': 8, 'frequency': 300}\n","- Compute Indices\n","\tCompute Acoustic_Complexity_Index\n","\tCompute Acoustic_Diversity_Index\n","\tCompute Acoustic_Evenness_Index\n","\tCompute Bio_acoustic_Index\n","\tCompute Normalized_Difference_Sound_Index\n","\tCompute RMS_energy\n","\tCompute Spectral_centroid\n","\tCompute Spectral_Entropy\n","\tCompute Temporal_Entropy\n","\tCompute ZCR\n","\tCompute Wave_SNR\n","\tCompute NB_peaks\n","\tCompute Acoustic_Diversity_Index_NR\n","\tCompute Acoustic_Evenness_Index_NR\n","\tCompute Bio_acoustic_Index_NR\n","\tCompute Spectral_Entropy_NR\n","File /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0689.mp3 deleted.\n","/content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/output_lists/output_lists/ADs_IG_2021/AD0689.xlsx\n"]},{"name":"stderr","output_type":"stream","text":["<ipython-input-46-020a70f8a660>:238: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  all_data = all_data.append(file_data, ignore_index=True)\n"]},{"name":"stdout","output_type":"stream","text":["###### CURRENT AD: /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0690.wav ######\n","###### - 2 / 53 - ######\n","Read the audio file: /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0690.wav\n","\tSuccessful read of the audio file: /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0690.wav\n","\tThe audio file contains more than one channel. Only the channel 0 will be used.\n","- Pre-processing - High-Pass Filtering: {'type': 'butterworth', 'order': 8, 'frequency': 300}\n","- Compute Indices\n","\tCompute Acoustic_Complexity_Index\n","\tCompute Acoustic_Diversity_Index\n","\tCompute Acoustic_Evenness_Index\n","\tCompute Bio_acoustic_Index\n","\tCompute Normalized_Difference_Sound_Index\n","\tCompute RMS_energy\n","\tCompute Spectral_centroid\n","\tCompute Spectral_Entropy\n","\tCompute Temporal_Entropy\n","\tCompute ZCR\n","\tCompute Wave_SNR\n","\tCompute NB_peaks\n","\tCompute Acoustic_Diversity_Index_NR\n","\tCompute Acoustic_Evenness_Index_NR\n","\tCompute Bio_acoustic_Index_NR\n","\tCompute Spectral_Entropy_NR\n","File /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0690.mp3 deleted.\n","/content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/output_lists/output_lists/ADs_IG_2021/AD0690.xlsx\n"]},{"name":"stderr","output_type":"stream","text":["<ipython-input-46-020a70f8a660>:238: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  all_data = all_data.append(file_data, ignore_index=True)\n"]},{"name":"stdout","output_type":"stream","text":["###### CURRENT AD: /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0691.wav ######\n","###### - 3 / 53 - ######\n","Read the audio file: /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0691.wav\n","\tSuccessful read of the audio file: /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0691.wav\n","\tThe audio file contains more than one channel. Only the channel 0 will be used.\n","- Pre-processing - High-Pass Filtering: {'type': 'butterworth', 'order': 8, 'frequency': 300}\n","- Compute Indices\n","\tCompute Acoustic_Complexity_Index\n","\tCompute Acoustic_Diversity_Index\n","\tCompute Acoustic_Evenness_Index\n","\tCompute Bio_acoustic_Index\n","\tCompute Normalized_Difference_Sound_Index\n","\tCompute RMS_energy\n","\tCompute Spectral_centroid\n","\tCompute Spectral_Entropy\n","\tCompute Temporal_Entropy\n","\tCompute ZCR\n","\tCompute Wave_SNR\n","\tCompute NB_peaks\n","\tCompute Acoustic_Diversity_Index_NR\n","\tCompute Acoustic_Evenness_Index_NR\n","\tCompute Bio_acoustic_Index_NR\n","\tCompute Spectral_Entropy_NR\n","File /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0691.mp3 deleted.\n","/content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/output_lists/output_lists/ADs_IG_2021/AD0691.xlsx\n"]},{"name":"stderr","output_type":"stream","text":["<ipython-input-46-020a70f8a660>:238: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  all_data = all_data.append(file_data, ignore_index=True)\n"]},{"name":"stdout","output_type":"stream","text":["###### CURRENT AD: /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0692.wav ######\n","###### - 4 / 53 - ######\n","Read the audio file: /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0692.wav\n","\tSuccessful read of the audio file: /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0692.wav\n","\tThe audio file contains more than one channel. Only the channel 0 will be used.\n","- Pre-processing - High-Pass Filtering: {'type': 'butterworth', 'order': 8, 'frequency': 300}\n","- Compute Indices\n","\tCompute Acoustic_Complexity_Index\n","\tCompute Acoustic_Diversity_Index\n","\tCompute Acoustic_Evenness_Index\n","\tCompute Bio_acoustic_Index\n","\tCompute Normalized_Difference_Sound_Index\n","\tCompute RMS_energy\n","\tCompute Spectral_centroid\n","\tCompute Spectral_Entropy\n","\tCompute Temporal_Entropy\n","\tCompute ZCR\n","\tCompute Wave_SNR\n","\tCompute NB_peaks\n","\tCompute Acoustic_Diversity_Index_NR\n","\tCompute Acoustic_Evenness_Index_NR\n","\tCompute Bio_acoustic_Index_NR\n","\tCompute Spectral_Entropy_NR\n","File /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0692.mp3 deleted.\n","/content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/output_lists/output_lists/ADs_IG_2021/AD0692.xlsx\n"]},{"name":"stderr","output_type":"stream","text":["<ipython-input-46-020a70f8a660>:238: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  all_data = all_data.append(file_data, ignore_index=True)\n"]},{"name":"stdout","output_type":"stream","text":["###### CURRENT AD: /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0693.wav ######\n","###### - 5 / 53 - ######\n","Read the audio file: /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0693.wav\n","\tSuccessful read of the audio file: /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0693.wav\n","\tThe audio file contains more than one channel. Only the channel 0 will be used.\n","- Pre-processing - High-Pass Filtering: {'type': 'butterworth', 'order': 8, 'frequency': 300}\n","- Compute Indices\n","\tCompute Acoustic_Complexity_Index\n","\tCompute Acoustic_Diversity_Index\n","\tCompute Acoustic_Evenness_Index\n","\tCompute Bio_acoustic_Index\n","\tCompute Normalized_Difference_Sound_Index\n","\tCompute RMS_energy\n","\tCompute Spectral_centroid\n","\tCompute Spectral_Entropy\n","\tCompute Temporal_Entropy\n","\tCompute ZCR\n","\tCompute Wave_SNR\n","\tCompute NB_peaks\n","\tCompute Acoustic_Diversity_Index_NR\n","\tCompute Acoustic_Evenness_Index_NR\n","\tCompute Bio_acoustic_Index_NR\n","\tCompute Spectral_Entropy_NR\n","File /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0693.mp3 deleted.\n","/content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/output_lists/output_lists/ADs_IG_2021/AD0693.xlsx\n"]},{"name":"stderr","output_type":"stream","text":["<ipython-input-46-020a70f8a660>:238: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  all_data = all_data.append(file_data, ignore_index=True)\n"]},{"name":"stdout","output_type":"stream","text":["###### CURRENT AD: /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0694.wav ######\n","###### - 6 / 53 - ######\n","Read the audio file: /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0694.wav\n","\tSuccessful read of the audio file: /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0694.wav\n","\tThe audio file contains more than one channel. Only the channel 0 will be used.\n","- Pre-processing - High-Pass Filtering: {'type': 'butterworth', 'order': 8, 'frequency': 300}\n","- Compute Indices\n","\tCompute Acoustic_Complexity_Index\n","\tCompute Acoustic_Diversity_Index\n","\tCompute Acoustic_Evenness_Index\n","\tCompute Bio_acoustic_Index\n","\tCompute Normalized_Difference_Sound_Index\n","\tCompute RMS_energy\n","\tCompute Spectral_centroid\n","\tCompute Spectral_Entropy\n","\tCompute Temporal_Entropy\n","\tCompute ZCR\n","\tCompute Wave_SNR\n","\tCompute NB_peaks\n","\tCompute Acoustic_Diversity_Index_NR\n","\tCompute Acoustic_Evenness_Index_NR\n","\tCompute Bio_acoustic_Index_NR\n","\tCompute Spectral_Entropy_NR\n","File /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0694.mp3 deleted.\n","/content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/output_lists/output_lists/ADs_IG_2021/AD0694.xlsx\n"]},{"name":"stderr","output_type":"stream","text":["<ipython-input-46-020a70f8a660>:238: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  all_data = all_data.append(file_data, ignore_index=True)\n"]},{"name":"stdout","output_type":"stream","text":["###### CURRENT AD: /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0695.wav ######\n","###### - 7 / 53 - ######\n","Read the audio file: /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0695.wav\n","\tSuccessful read of the audio file: /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0695.wav\n","\tThe audio file contains more than one channel. Only the channel 0 will be used.\n","- Pre-processing - High-Pass Filtering: {'type': 'butterworth', 'order': 8, 'frequency': 300}\n","- Compute Indices\n","\tCompute Acoustic_Complexity_Index\n","\tCompute Acoustic_Diversity_Index\n","\tCompute Acoustic_Evenness_Index\n","\tCompute Bio_acoustic_Index\n","\tCompute Normalized_Difference_Sound_Index\n","\tCompute RMS_energy\n","\tCompute Spectral_centroid\n","\tCompute Spectral_Entropy\n","\tCompute Temporal_Entropy\n","\tCompute ZCR\n","\tCompute Wave_SNR\n","\tCompute NB_peaks\n","\tCompute Acoustic_Diversity_Index_NR\n","\tCompute Acoustic_Evenness_Index_NR\n","\tCompute Bio_acoustic_Index_NR\n","\tCompute Spectral_Entropy_NR\n","File /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0695.mp3 deleted.\n","/content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/output_lists/output_lists/ADs_IG_2021/AD0695.xlsx\n"]},{"name":"stderr","output_type":"stream","text":["<ipython-input-46-020a70f8a660>:238: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  all_data = all_data.append(file_data, ignore_index=True)\n"]},{"name":"stdout","output_type":"stream","text":["###### CURRENT AD: /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0696.wav ######\n","###### - 8 / 53 - ######\n","Read the audio file: /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0696.wav\n","\tSuccessful read of the audio file: /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0696.wav\n","\tThe audio file contains more than one channel. Only the channel 0 will be used.\n","- Pre-processing - High-Pass Filtering: {'type': 'butterworth', 'order': 8, 'frequency': 300}\n","- Compute Indices\n","\tCompute Acoustic_Complexity_Index\n","\tCompute Acoustic_Diversity_Index\n","\tCompute Acoustic_Evenness_Index\n","\tCompute Bio_acoustic_Index\n","\tCompute Normalized_Difference_Sound_Index\n","\tCompute RMS_energy\n","\tCompute Spectral_centroid\n","\tCompute Spectral_Entropy\n","\tCompute Temporal_Entropy\n","\tCompute ZCR\n","\tCompute Wave_SNR\n","\tCompute NB_peaks\n","\tCompute Acoustic_Diversity_Index_NR\n","\tCompute Acoustic_Evenness_Index_NR\n","\tCompute Bio_acoustic_Index_NR\n","\tCompute Spectral_Entropy_NR\n","File /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0696.mp3 deleted.\n","/content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/output_lists/output_lists/ADs_IG_2021/AD0696.xlsx\n"]},{"name":"stderr","output_type":"stream","text":["<ipython-input-46-020a70f8a660>:238: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  all_data = all_data.append(file_data, ignore_index=True)\n"]},{"name":"stdout","output_type":"stream","text":["###### CURRENT AD: /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0697.wav ######\n","###### - 9 / 53 - ######\n","Read the audio file: /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0697.wav\n","\tSuccessful read of the audio file: /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0697.wav\n","\tThe audio file contains more than one channel. Only the channel 0 will be used.\n","- Pre-processing - High-Pass Filtering: {'type': 'butterworth', 'order': 8, 'frequency': 300}\n","- Compute Indices\n","\tCompute Acoustic_Complexity_Index\n","\tCompute Acoustic_Diversity_Index\n","\tCompute Acoustic_Evenness_Index\n","\tCompute Bio_acoustic_Index\n","\tCompute Normalized_Difference_Sound_Index\n","\tCompute RMS_energy\n","\tCompute Spectral_centroid\n","\tCompute Spectral_Entropy\n","\tCompute Temporal_Entropy\n","\tCompute ZCR\n","\tCompute Wave_SNR\n","\tCompute NB_peaks\n","\tCompute Acoustic_Diversity_Index_NR\n","\tCompute Acoustic_Evenness_Index_NR\n","\tCompute Bio_acoustic_Index_NR\n","\tCompute Spectral_Entropy_NR\n","File /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0697.mp3 deleted.\n","/content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/output_lists/output_lists/ADs_IG_2021/AD0697.xlsx\n"]},{"name":"stderr","output_type":"stream","text":["<ipython-input-46-020a70f8a660>:238: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  all_data = all_data.append(file_data, ignore_index=True)\n"]},{"name":"stdout","output_type":"stream","text":["###### CURRENT AD: /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0698.wav ######\n","###### - 10 / 53 - ######\n","Read the audio file: /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0698.wav\n","\tSuccessful read of the audio file: /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0698.wav\n","\tThe audio file contains more than one channel. Only the channel 0 will be used.\n","- Pre-processing - High-Pass Filtering: {'type': 'butterworth', 'order': 8, 'frequency': 300}\n","- Compute Indices\n","\tCompute Acoustic_Complexity_Index\n","\tCompute Acoustic_Diversity_Index\n","\tCompute Acoustic_Evenness_Index\n","\tCompute Bio_acoustic_Index\n","\tCompute Normalized_Difference_Sound_Index\n","\tCompute RMS_energy\n","\tCompute Spectral_centroid\n","\tCompute Spectral_Entropy\n","\tCompute Temporal_Entropy\n","\tCompute ZCR\n","\tCompute Wave_SNR\n","\tCompute NB_peaks\n","\tCompute Acoustic_Diversity_Index_NR\n","\tCompute Acoustic_Evenness_Index_NR\n","\tCompute Bio_acoustic_Index_NR\n","\tCompute Spectral_Entropy_NR\n","File /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0698.mp3 deleted.\n","/content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/output_lists/output_lists/ADs_IG_2021/AD0698.xlsx\n"]},{"name":"stderr","output_type":"stream","text":["<ipython-input-46-020a70f8a660>:238: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  all_data = all_data.append(file_data, ignore_index=True)\n"]},{"name":"stdout","output_type":"stream","text":["###### CURRENT AD: /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0699.wav ######\n","###### - 11 / 53 - ######\n","Read the audio file: /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0699.wav\n","\tSuccessful read of the audio file: /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0699.wav\n","\tThe audio file contains more than one channel. Only the channel 0 will be used.\n","- Pre-processing - High-Pass Filtering: {'type': 'butterworth', 'order': 8, 'frequency': 300}\n","- Compute Indices\n","\tCompute Acoustic_Complexity_Index\n","\tCompute Acoustic_Diversity_Index\n","\tCompute Acoustic_Evenness_Index\n","\tCompute Bio_acoustic_Index\n","\tCompute Normalized_Difference_Sound_Index\n","\tCompute RMS_energy\n","\tCompute Spectral_centroid\n","\tCompute Spectral_Entropy\n","\tCompute Temporal_Entropy\n","\tCompute ZCR\n","\tCompute Wave_SNR\n","\tCompute NB_peaks\n","\tCompute Acoustic_Diversity_Index_NR\n","\tCompute Acoustic_Evenness_Index_NR\n","\tCompute Bio_acoustic_Index_NR\n","\tCompute Spectral_Entropy_NR\n","File /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0699.mp3 deleted.\n","/content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/output_lists/output_lists/ADs_IG_2021/AD0699.xlsx\n"]},{"name":"stderr","output_type":"stream","text":["<ipython-input-46-020a70f8a660>:238: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  all_data = all_data.append(file_data, ignore_index=True)\n"]},{"name":"stdout","output_type":"stream","text":["###### CURRENT AD: /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0700.wav ######\n","###### - 12 / 53 - ######\n","Read the audio file: /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0700.wav\n","\tSuccessful read of the audio file: /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0700.wav\n","\tThe audio file contains more than one channel. Only the channel 0 will be used.\n","- Pre-processing - High-Pass Filtering: {'type': 'butterworth', 'order': 8, 'frequency': 300}\n","- Compute Indices\n","\tCompute Acoustic_Complexity_Index\n","\tCompute Acoustic_Diversity_Index\n","\tCompute Acoustic_Evenness_Index\n","\tCompute Bio_acoustic_Index\n","\tCompute Normalized_Difference_Sound_Index\n","\tCompute RMS_energy\n","\tCompute Spectral_centroid\n","\tCompute Spectral_Entropy\n","\tCompute Temporal_Entropy\n","\tCompute ZCR\n","\tCompute Wave_SNR\n","\tCompute NB_peaks\n","\tCompute Acoustic_Diversity_Index_NR\n","\tCompute Acoustic_Evenness_Index_NR\n","\tCompute Bio_acoustic_Index_NR\n","\tCompute Spectral_Entropy_NR\n","File /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0700.mp3 deleted.\n","/content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/output_lists/output_lists/ADs_IG_2021/AD0700.xlsx\n"]},{"name":"stderr","output_type":"stream","text":["<ipython-input-46-020a70f8a660>:238: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  all_data = all_data.append(file_data, ignore_index=True)\n"]},{"name":"stdout","output_type":"stream","text":["###### CURRENT AD: /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0701.wav ######\n","###### - 13 / 53 - ######\n","Read the audio file: /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0701.wav\n","\tSuccessful read of the audio file: /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0701.wav\n","\tThe audio file contains more than one channel. Only the channel 0 will be used.\n","- Pre-processing - High-Pass Filtering: {'type': 'butterworth', 'order': 8, 'frequency': 300}\n","- Compute Indices\n","\tCompute Acoustic_Complexity_Index\n","\tCompute Acoustic_Diversity_Index\n","\tCompute Acoustic_Evenness_Index\n","\tCompute Bio_acoustic_Index\n","\tCompute Normalized_Difference_Sound_Index\n","\tCompute RMS_energy\n","\tCompute Spectral_centroid\n","\tCompute Spectral_Entropy\n","\tCompute Temporal_Entropy\n","\tCompute ZCR\n","\tCompute Wave_SNR\n","\tCompute NB_peaks\n","\tCompute Acoustic_Diversity_Index_NR\n","\tCompute Acoustic_Evenness_Index_NR\n","\tCompute Bio_acoustic_Index_NR\n","\tCompute Spectral_Entropy_NR\n","File /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0701.mp3 deleted.\n","/content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/output_lists/output_lists/ADs_IG_2021/AD0701.xlsx\n"]},{"name":"stderr","output_type":"stream","text":["<ipython-input-46-020a70f8a660>:238: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  all_data = all_data.append(file_data, ignore_index=True)\n"]},{"name":"stdout","output_type":"stream","text":["###### CURRENT AD: /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0702.wav ######\n","###### - 14 / 53 - ######\n","Read the audio file: /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0702.wav\n","\tSuccessful read of the audio file: /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0702.wav\n","\tThe audio file contains more than one channel. Only the channel 0 will be used.\n","- Pre-processing - High-Pass Filtering: {'type': 'butterworth', 'order': 8, 'frequency': 300}\n","- Compute Indices\n","\tCompute Acoustic_Complexity_Index\n","\tCompute Acoustic_Diversity_Index\n","\tCompute Acoustic_Evenness_Index\n","\tCompute Bio_acoustic_Index\n","\tCompute Normalized_Difference_Sound_Index\n","\tCompute RMS_energy\n","\tCompute Spectral_centroid\n","\tCompute Spectral_Entropy\n","\tCompute Temporal_Entropy\n","\tCompute ZCR\n","\tCompute Wave_SNR\n","\tCompute NB_peaks\n","\tCompute Acoustic_Diversity_Index_NR\n","\tCompute Acoustic_Evenness_Index_NR\n","\tCompute Bio_acoustic_Index_NR\n","\tCompute Spectral_Entropy_NR\n","File /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0702.mp3 deleted.\n","/content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/output_lists/output_lists/ADs_IG_2021/AD0702.xlsx\n"]},{"name":"stderr","output_type":"stream","text":["<ipython-input-46-020a70f8a660>:238: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  all_data = all_data.append(file_data, ignore_index=True)\n"]},{"name":"stdout","output_type":"stream","text":["###### CURRENT AD: /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0703.wav ######\n","###### - 15 / 53 - ######\n","Read the audio file: /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0703.wav\n","\tSuccessful read of the audio file: /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0703.wav\n","\tThe audio file contains more than one channel. Only the channel 0 will be used.\n","- Pre-processing - High-Pass Filtering: {'type': 'butterworth', 'order': 8, 'frequency': 300}\n","- Compute Indices\n","\tCompute Acoustic_Complexity_Index\n","\tCompute Acoustic_Diversity_Index\n","\tCompute Acoustic_Evenness_Index\n","\tCompute Bio_acoustic_Index\n","\tCompute Normalized_Difference_Sound_Index\n","\tCompute RMS_energy\n","\tCompute Spectral_centroid\n","\tCompute Spectral_Entropy\n","\tCompute Temporal_Entropy\n","\tCompute ZCR\n","\tCompute Wave_SNR\n","\tCompute NB_peaks\n","\tCompute Acoustic_Diversity_Index_NR\n","\tCompute Acoustic_Evenness_Index_NR\n","\tCompute Bio_acoustic_Index_NR\n","\tCompute Spectral_Entropy_NR\n","File /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0703.mp3 deleted.\n","/content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/output_lists/output_lists/ADs_IG_2021/AD0703.xlsx\n"]},{"name":"stderr","output_type":"stream","text":["<ipython-input-46-020a70f8a660>:238: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  all_data = all_data.append(file_data, ignore_index=True)\n"]},{"name":"stdout","output_type":"stream","text":["###### CURRENT AD: /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0704.wav ######\n","###### - 16 / 53 - ######\n","Read the audio file: /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0704.wav\n","\tSuccessful read of the audio file: /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0704.wav\n","\tThe audio file contains more than one channel. Only the channel 0 will be used.\n","- Pre-processing - High-Pass Filtering: {'type': 'butterworth', 'order': 8, 'frequency': 300}\n","- Compute Indices\n","\tCompute Acoustic_Complexity_Index\n","\tCompute Acoustic_Diversity_Index\n","\tCompute Acoustic_Evenness_Index\n","\tCompute Bio_acoustic_Index\n","\tCompute Normalized_Difference_Sound_Index\n","\tCompute RMS_energy\n","\tCompute Spectral_centroid\n","\tCompute Spectral_Entropy\n","\tCompute Temporal_Entropy\n","\tCompute ZCR\n","\tCompute Wave_SNR\n","\tCompute NB_peaks\n","\tCompute Acoustic_Diversity_Index_NR\n","\tCompute Acoustic_Evenness_Index_NR\n","\tCompute Bio_acoustic_Index_NR\n","\tCompute Spectral_Entropy_NR\n","File /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0704.mp3 deleted.\n","/content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/output_lists/output_lists/ADs_IG_2021/AD0704.xlsx\n"]},{"name":"stderr","output_type":"stream","text":["<ipython-input-46-020a70f8a660>:238: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  all_data = all_data.append(file_data, ignore_index=True)\n"]},{"name":"stdout","output_type":"stream","text":["###### CURRENT AD: /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0705.wav ######\n","###### - 17 / 53 - ######\n","Read the audio file: /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0705.wav\n","\tSuccessful read of the audio file: /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0705.wav\n","\tThe audio file contains more than one channel. Only the channel 0 will be used.\n","- Pre-processing - High-Pass Filtering: {'type': 'butterworth', 'order': 8, 'frequency': 300}\n","- Compute Indices\n","\tCompute Acoustic_Complexity_Index\n","\tCompute Acoustic_Diversity_Index\n","\tCompute Acoustic_Evenness_Index\n","\tCompute Bio_acoustic_Index\n","\tCompute Normalized_Difference_Sound_Index\n","\tCompute RMS_energy\n","\tCompute Spectral_centroid\n","\tCompute Spectral_Entropy\n","\tCompute Temporal_Entropy\n","\tCompute ZCR\n","\tCompute Wave_SNR\n","\tCompute NB_peaks\n","\tCompute Acoustic_Diversity_Index_NR\n","\tCompute Acoustic_Evenness_Index_NR\n","\tCompute Bio_acoustic_Index_NR\n","\tCompute Spectral_Entropy_NR\n","File /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0705.mp3 deleted.\n","/content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/output_lists/output_lists/ADs_IG_2021/AD0705.xlsx\n"]},{"name":"stderr","output_type":"stream","text":["<ipython-input-46-020a70f8a660>:238: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  all_data = all_data.append(file_data, ignore_index=True)\n"]},{"name":"stdout","output_type":"stream","text":["###### CURRENT AD: /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0706.wav ######\n","###### - 18 / 53 - ######\n","Read the audio file: /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0706.wav\n","\tSuccessful read of the audio file: /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0706.wav\n","\tThe audio file contains more than one channel. Only the channel 0 will be used.\n","- Pre-processing - High-Pass Filtering: {'type': 'butterworth', 'order': 8, 'frequency': 300}\n","- Compute Indices\n","\tCompute Acoustic_Complexity_Index\n","\tCompute Acoustic_Diversity_Index\n","\tCompute Acoustic_Evenness_Index\n","\tCompute Bio_acoustic_Index\n","\tCompute Normalized_Difference_Sound_Index\n","\tCompute RMS_energy\n","\tCompute Spectral_centroid\n","\tCompute Spectral_Entropy\n","\tCompute Temporal_Entropy\n","\tCompute ZCR\n","\tCompute Wave_SNR\n","\tCompute NB_peaks\n","\tCompute Acoustic_Diversity_Index_NR\n","\tCompute Acoustic_Evenness_Index_NR\n","\tCompute Bio_acoustic_Index_NR\n","\tCompute Spectral_Entropy_NR\n","File /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0706.mp3 deleted.\n","/content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/output_lists/output_lists/ADs_IG_2021/AD0706.xlsx\n"]},{"name":"stderr","output_type":"stream","text":["<ipython-input-46-020a70f8a660>:238: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  all_data = all_data.append(file_data, ignore_index=True)\n"]},{"name":"stdout","output_type":"stream","text":["###### CURRENT AD: /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0707.wav ######\n","###### - 19 / 53 - ######\n","Read the audio file: /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0707.wav\n","\tSuccessful read of the audio file: /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0707.wav\n","\tThe audio file contains more than one channel. Only the channel 0 will be used.\n","- Pre-processing - High-Pass Filtering: {'type': 'butterworth', 'order': 8, 'frequency': 300}\n","- Compute Indices\n","\tCompute Acoustic_Complexity_Index\n","\tCompute Acoustic_Diversity_Index\n","\tCompute Acoustic_Evenness_Index\n","\tCompute Bio_acoustic_Index\n","\tCompute Normalized_Difference_Sound_Index\n","\tCompute RMS_energy\n","\tCompute Spectral_centroid\n","\tCompute Spectral_Entropy\n","\tCompute Temporal_Entropy\n","\tCompute ZCR\n","\tCompute Wave_SNR\n","\tCompute NB_peaks\n","\tCompute Acoustic_Diversity_Index_NR\n","\tCompute Acoustic_Evenness_Index_NR\n","\tCompute Bio_acoustic_Index_NR\n","\tCompute Spectral_Entropy_NR\n","File /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0707.mp3 deleted.\n","/content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/output_lists/output_lists/ADs_IG_2021/AD0707.xlsx\n"]},{"name":"stderr","output_type":"stream","text":["<ipython-input-46-020a70f8a660>:238: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  all_data = all_data.append(file_data, ignore_index=True)\n"]},{"name":"stdout","output_type":"stream","text":["###### CURRENT AD: /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0708.wav ######\n","###### - 20 / 53 - ######\n","Read the audio file: /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0708.wav\n","\tSuccessful read of the audio file: /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0708.wav\n","\tThe audio file contains more than one channel. Only the channel 0 will be used.\n","- Pre-processing - High-Pass Filtering: {'type': 'butterworth', 'order': 8, 'frequency': 300}\n","- Compute Indices\n","\tCompute Acoustic_Complexity_Index\n","\tCompute Acoustic_Diversity_Index\n","\tCompute Acoustic_Evenness_Index\n","\tCompute Bio_acoustic_Index\n","\tCompute Normalized_Difference_Sound_Index\n","\tCompute RMS_energy\n","\tCompute Spectral_centroid\n","\tCompute Spectral_Entropy\n","\tCompute Temporal_Entropy\n","\tCompute ZCR\n","\tCompute Wave_SNR\n","\tCompute NB_peaks\n","\tCompute Acoustic_Diversity_Index_NR\n","\tCompute Acoustic_Evenness_Index_NR\n","\tCompute Bio_acoustic_Index_NR\n","\tCompute Spectral_Entropy_NR\n","File /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0708.mp3 deleted.\n","/content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/output_lists/output_lists/ADs_IG_2021/AD0708.xlsx\n"]},{"name":"stderr","output_type":"stream","text":["<ipython-input-46-020a70f8a660>:238: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  all_data = all_data.append(file_data, ignore_index=True)\n"]},{"name":"stdout","output_type":"stream","text":["###### CURRENT AD: /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0709.wav ######\n","###### - 21 / 53 - ######\n","Read the audio file: /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0709.wav\n","\tSuccessful read of the audio file: /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0709.wav\n","\tThe audio file contains more than one channel. Only the channel 0 will be used.\n","- Pre-processing - High-Pass Filtering: {'type': 'butterworth', 'order': 8, 'frequency': 300}\n","- Compute Indices\n","\tCompute Acoustic_Complexity_Index\n","\tCompute Acoustic_Diversity_Index\n","\tCompute Acoustic_Evenness_Index\n","\tCompute Bio_acoustic_Index\n","\tCompute Normalized_Difference_Sound_Index\n","\tCompute RMS_energy\n","\tCompute Spectral_centroid\n","\tCompute Spectral_Entropy\n","\tCompute Temporal_Entropy\n","\tCompute ZCR\n","\tCompute Wave_SNR\n","\tCompute NB_peaks\n","\tCompute Acoustic_Diversity_Index_NR\n","\tCompute Acoustic_Evenness_Index_NR\n","\tCompute Bio_acoustic_Index_NR\n","\tCompute Spectral_Entropy_NR\n","File /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0709.mp3 deleted.\n","/content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/output_lists/output_lists/ADs_IG_2021/AD0709.xlsx\n"]},{"name":"stderr","output_type":"stream","text":["<ipython-input-46-020a70f8a660>:238: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  all_data = all_data.append(file_data, ignore_index=True)\n"]},{"name":"stdout","output_type":"stream","text":["###### CURRENT AD: /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0710.wav ######\n","###### - 22 / 53 - ######\n","Read the audio file: /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0710.wav\n","\tSuccessful read of the audio file: /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0710.wav\n","\tThe audio file contains more than one channel. Only the channel 0 will be used.\n","- Pre-processing - High-Pass Filtering: {'type': 'butterworth', 'order': 8, 'frequency': 300}\n","- Compute Indices\n","\tCompute Acoustic_Complexity_Index\n","\tCompute Acoustic_Diversity_Index\n","\tCompute Acoustic_Evenness_Index\n","\tCompute Bio_acoustic_Index\n","\tCompute Normalized_Difference_Sound_Index\n","\tCompute RMS_energy\n","\tCompute Spectral_centroid\n","\tCompute Spectral_Entropy\n","\tCompute Temporal_Entropy\n","\tCompute ZCR\n","\tCompute Wave_SNR\n","\tCompute NB_peaks\n","\tCompute Acoustic_Diversity_Index_NR\n","\tCompute Acoustic_Evenness_Index_NR\n","\tCompute Bio_acoustic_Index_NR\n","\tCompute Spectral_Entropy_NR\n","File /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0710.mp3 deleted.\n","/content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/output_lists/output_lists/ADs_IG_2021/AD0710.xlsx\n"]},{"name":"stderr","output_type":"stream","text":["<ipython-input-46-020a70f8a660>:238: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  all_data = all_data.append(file_data, ignore_index=True)\n"]},{"name":"stdout","output_type":"stream","text":["###### CURRENT AD: /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0711.wav ######\n","###### - 23 / 53 - ######\n","Read the audio file: /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0711.wav\n","\tSuccessful read of the audio file: /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0711.wav\n","\tThe audio file contains more than one channel. Only the channel 0 will be used.\n","- Pre-processing - High-Pass Filtering: {'type': 'butterworth', 'order': 8, 'frequency': 300}\n","- Compute Indices\n","\tCompute Acoustic_Complexity_Index\n","\tCompute Acoustic_Diversity_Index\n","\tCompute Acoustic_Evenness_Index\n","\tCompute Bio_acoustic_Index\n","\tCompute Normalized_Difference_Sound_Index\n","\tCompute RMS_energy\n","\tCompute Spectral_centroid\n","\tCompute Spectral_Entropy\n","\tCompute Temporal_Entropy\n","\tCompute ZCR\n","\tCompute Wave_SNR\n","\tCompute NB_peaks\n","\tCompute Acoustic_Diversity_Index_NR\n","\tCompute Acoustic_Evenness_Index_NR\n","\tCompute Bio_acoustic_Index_NR\n","\tCompute Spectral_Entropy_NR\n","File /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0711.mp3 deleted.\n","/content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/output_lists/output_lists/ADs_IG_2021/AD0711.xlsx\n"]},{"name":"stderr","output_type":"stream","text":["<ipython-input-46-020a70f8a660>:238: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  all_data = all_data.append(file_data, ignore_index=True)\n"]},{"name":"stdout","output_type":"stream","text":["###### CURRENT AD: /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0712.wav ######\n","###### - 24 / 53 - ######\n","Read the audio file: /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0712.wav\n","\tSuccessful read of the audio file: /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0712.wav\n","\tThe audio file contains more than one channel. Only the channel 0 will be used.\n","- Pre-processing - High-Pass Filtering: {'type': 'butterworth', 'order': 8, 'frequency': 300}\n","- Compute Indices\n","\tCompute Acoustic_Complexity_Index\n","\tCompute Acoustic_Diversity_Index\n","\tCompute Acoustic_Evenness_Index\n","\tCompute Bio_acoustic_Index\n","\tCompute Normalized_Difference_Sound_Index\n","\tCompute RMS_energy\n","\tCompute Spectral_centroid\n","\tCompute Spectral_Entropy\n","\tCompute Temporal_Entropy\n","\tCompute ZCR\n","\tCompute Wave_SNR\n","\tCompute NB_peaks\n","\tCompute Acoustic_Diversity_Index_NR\n","\tCompute Acoustic_Evenness_Index_NR\n","\tCompute Bio_acoustic_Index_NR\n","\tCompute Spectral_Entropy_NR\n","File /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0712.mp3 deleted.\n","/content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/output_lists/output_lists/ADs_IG_2021/AD0712.xlsx\n"]},{"name":"stderr","output_type":"stream","text":["<ipython-input-46-020a70f8a660>:238: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  all_data = all_data.append(file_data, ignore_index=True)\n"]},{"name":"stdout","output_type":"stream","text":["###### CURRENT AD: /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0713.wav ######\n","###### - 25 / 53 - ######\n","Read the audio file: /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0713.wav\n","\tSuccessful read of the audio file: /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0713.wav\n","\tThe audio file contains more than one channel. Only the channel 0 will be used.\n","- Pre-processing - High-Pass Filtering: {'type': 'butterworth', 'order': 8, 'frequency': 300}\n","- Compute Indices\n","\tCompute Acoustic_Complexity_Index\n","\tCompute Acoustic_Diversity_Index\n","\tCompute Acoustic_Evenness_Index\n","\tCompute Bio_acoustic_Index\n","\tCompute Normalized_Difference_Sound_Index\n","\tCompute RMS_energy\n","\tCompute Spectral_centroid\n","\tCompute Spectral_Entropy\n","\tCompute Temporal_Entropy\n","\tCompute ZCR\n","\tCompute Wave_SNR\n","\tCompute NB_peaks\n","\tCompute Acoustic_Diversity_Index_NR\n","\tCompute Acoustic_Evenness_Index_NR\n","\tCompute Bio_acoustic_Index_NR\n","\tCompute Spectral_Entropy_NR\n","File /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0713.mp3 deleted.\n","/content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/output_lists/output_lists/ADs_IG_2021/AD0713.xlsx\n"]},{"name":"stderr","output_type":"stream","text":["<ipython-input-46-020a70f8a660>:238: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  all_data = all_data.append(file_data, ignore_index=True)\n"]},{"name":"stdout","output_type":"stream","text":["###### CURRENT AD: /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0714.wav ######\n","###### - 26 / 53 - ######\n","Read the audio file: /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0714.wav\n","\tSuccessful read of the audio file: /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0714.wav\n","\tThe audio file contains more than one channel. Only the channel 0 will be used.\n","- Pre-processing - High-Pass Filtering: {'type': 'butterworth', 'order': 8, 'frequency': 300}\n","- Compute Indices\n","\tCompute Acoustic_Complexity_Index\n","\tCompute Acoustic_Diversity_Index\n","\tCompute Acoustic_Evenness_Index\n","\tCompute Bio_acoustic_Index\n","\tCompute Normalized_Difference_Sound_Index\n","\tCompute RMS_energy\n","\tCompute Spectral_centroid\n","\tCompute Spectral_Entropy\n","\tCompute Temporal_Entropy\n","\tCompute ZCR\n","\tCompute Wave_SNR\n","\tCompute NB_peaks\n","\tCompute Acoustic_Diversity_Index_NR\n","\tCompute Acoustic_Evenness_Index_NR\n","\tCompute Bio_acoustic_Index_NR\n","\tCompute Spectral_Entropy_NR\n","File /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0714.mp3 deleted.\n","/content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/output_lists/output_lists/ADs_IG_2021/AD0714.xlsx\n"]},{"name":"stderr","output_type":"stream","text":["<ipython-input-46-020a70f8a660>:238: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  all_data = all_data.append(file_data, ignore_index=True)\n"]},{"name":"stdout","output_type":"stream","text":["###### CURRENT AD: /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0715.wav ######\n","###### - 27 / 53 - ######\n","Read the audio file: /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0715.wav\n","\tSuccessful read of the audio file: /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0715.wav\n","\tThe audio file contains more than one channel. Only the channel 0 will be used.\n","- Pre-processing - High-Pass Filtering: {'type': 'butterworth', 'order': 8, 'frequency': 300}\n","- Compute Indices\n","\tCompute Acoustic_Complexity_Index\n","\tCompute Acoustic_Diversity_Index\n","\tCompute Acoustic_Evenness_Index\n","\tCompute Bio_acoustic_Index\n","\tCompute Normalized_Difference_Sound_Index\n","\tCompute RMS_energy\n","\tCompute Spectral_centroid\n","\tCompute Spectral_Entropy\n","\tCompute Temporal_Entropy\n","\tCompute ZCR\n","\tCompute Wave_SNR\n","\tCompute NB_peaks\n","\tCompute Acoustic_Diversity_Index_NR\n","\tCompute Acoustic_Evenness_Index_NR\n","\tCompute Bio_acoustic_Index_NR\n","\tCompute Spectral_Entropy_NR\n","File /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0715.mp3 deleted.\n","/content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/output_lists/output_lists/ADs_IG_2021/AD0715.xlsx\n"]},{"name":"stderr","output_type":"stream","text":["<ipython-input-46-020a70f8a660>:238: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  all_data = all_data.append(file_data, ignore_index=True)\n"]},{"name":"stdout","output_type":"stream","text":["###### CURRENT AD: /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0716.wav ######\n","###### - 28 / 53 - ######\n","Read the audio file: /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0716.wav\n","\tSuccessful read of the audio file: /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0716.wav\n","\tThe audio file contains more than one channel. Only the channel 0 will be used.\n","- Pre-processing - High-Pass Filtering: {'type': 'butterworth', 'order': 8, 'frequency': 300}\n","- Compute Indices\n","\tCompute Acoustic_Complexity_Index\n","\tCompute Acoustic_Diversity_Index\n","\tCompute Acoustic_Evenness_Index\n","\tCompute Bio_acoustic_Index\n","\tCompute Normalized_Difference_Sound_Index\n","\tCompute RMS_energy\n","\tCompute Spectral_centroid\n","\tCompute Spectral_Entropy\n","\tCompute Temporal_Entropy\n","\tCompute ZCR\n","\tCompute Wave_SNR\n","\tCompute NB_peaks\n","\tCompute Acoustic_Diversity_Index_NR\n","\tCompute Acoustic_Evenness_Index_NR\n","\tCompute Bio_acoustic_Index_NR\n","\tCompute Spectral_Entropy_NR\n","File /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0716.mp3 deleted.\n","/content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/output_lists/output_lists/ADs_IG_2021/AD0716.xlsx\n"]},{"name":"stderr","output_type":"stream","text":["<ipython-input-46-020a70f8a660>:238: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  all_data = all_data.append(file_data, ignore_index=True)\n"]},{"name":"stdout","output_type":"stream","text":["###### CURRENT AD: /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0717.wav ######\n","###### - 29 / 53 - ######\n","Read the audio file: /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0717.wav\n","\tSuccessful read of the audio file: /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0717.wav\n","\tThe audio file contains more than one channel. Only the channel 0 will be used.\n","- Pre-processing - High-Pass Filtering: {'type': 'butterworth', 'order': 8, 'frequency': 300}\n","- Compute Indices\n","\tCompute Acoustic_Complexity_Index\n","\tCompute Acoustic_Diversity_Index\n","\tCompute Acoustic_Evenness_Index\n","\tCompute Bio_acoustic_Index\n","\tCompute Normalized_Difference_Sound_Index\n","\tCompute RMS_energy\n","\tCompute Spectral_centroid\n","\tCompute Spectral_Entropy\n","\tCompute Temporal_Entropy\n","\tCompute ZCR\n","\tCompute Wave_SNR\n","\tCompute NB_peaks\n","\tCompute Acoustic_Diversity_Index_NR\n","\tCompute Acoustic_Evenness_Index_NR\n","\tCompute Bio_acoustic_Index_NR\n","\tCompute Spectral_Entropy_NR\n","File /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0717.mp3 deleted.\n","/content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/output_lists/output_lists/ADs_IG_2021/AD0717.xlsx\n"]},{"name":"stderr","output_type":"stream","text":["<ipython-input-46-020a70f8a660>:238: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  all_data = all_data.append(file_data, ignore_index=True)\n"]},{"name":"stdout","output_type":"stream","text":["###### CURRENT AD: /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0718.wav ######\n","###### - 30 / 53 - ######\n","Read the audio file: /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0718.wav\n","\tSuccessful read of the audio file: /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0718.wav\n","\tThe audio file contains more than one channel. Only the channel 0 will be used.\n","- Pre-processing - High-Pass Filtering: {'type': 'butterworth', 'order': 8, 'frequency': 300}\n","- Compute Indices\n","\tCompute Acoustic_Complexity_Index\n","\tCompute Acoustic_Diversity_Index\n","\tCompute Acoustic_Evenness_Index\n","\tCompute Bio_acoustic_Index\n","\tCompute Normalized_Difference_Sound_Index\n","\tCompute RMS_energy\n","\tCompute Spectral_centroid\n","\tCompute Spectral_Entropy\n","\tCompute Temporal_Entropy\n","\tCompute ZCR\n","\tCompute Wave_SNR\n","\tCompute NB_peaks\n","\tCompute Acoustic_Diversity_Index_NR\n","\tCompute Acoustic_Evenness_Index_NR\n","\tCompute Bio_acoustic_Index_NR\n","\tCompute Spectral_Entropy_NR\n","File /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0718.mp3 deleted.\n","/content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/output_lists/output_lists/ADs_IG_2021/AD0718.xlsx\n"]},{"name":"stderr","output_type":"stream","text":["<ipython-input-46-020a70f8a660>:238: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  all_data = all_data.append(file_data, ignore_index=True)\n"]},{"name":"stdout","output_type":"stream","text":["###### CURRENT AD: /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0719.wav ######\n","###### - 31 / 53 - ######\n","Read the audio file: /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0719.wav\n","\tSuccessful read of the audio file: /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0719.wav\n","\tThe audio file contains more than one channel. Only the channel 0 will be used.\n","- Pre-processing - High-Pass Filtering: {'type': 'butterworth', 'order': 8, 'frequency': 300}\n","- Compute Indices\n","\tCompute Acoustic_Complexity_Index\n","\tCompute Acoustic_Diversity_Index\n","\tCompute Acoustic_Evenness_Index\n","\tCompute Bio_acoustic_Index\n","\tCompute Normalized_Difference_Sound_Index\n","\tCompute RMS_energy\n","\tCompute Spectral_centroid\n","\tCompute Spectral_Entropy\n","\tCompute Temporal_Entropy\n","\tCompute ZCR\n","\tCompute Wave_SNR\n","\tCompute NB_peaks\n","\tCompute Acoustic_Diversity_Index_NR\n","\tCompute Acoustic_Evenness_Index_NR\n","\tCompute Bio_acoustic_Index_NR\n","\tCompute Spectral_Entropy_NR\n","File /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0719.mp3 deleted.\n","/content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/output_lists/output_lists/ADs_IG_2021/AD0719.xlsx\n"]},{"name":"stderr","output_type":"stream","text":["<ipython-input-46-020a70f8a660>:238: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  all_data = all_data.append(file_data, ignore_index=True)\n"]},{"name":"stdout","output_type":"stream","text":["###### CURRENT AD: /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0720.wav ######\n","###### - 32 / 53 - ######\n","Read the audio file: /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0720.wav\n","\tSuccessful read of the audio file: /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0720.wav\n","\tThe audio file contains more than one channel. Only the channel 0 will be used.\n","- Pre-processing - High-Pass Filtering: {'type': 'butterworth', 'order': 8, 'frequency': 300}\n","- Compute Indices\n","\tCompute Acoustic_Complexity_Index\n","\tCompute Acoustic_Diversity_Index\n","\tCompute Acoustic_Evenness_Index\n","\tCompute Bio_acoustic_Index\n","\tCompute Normalized_Difference_Sound_Index\n","\tCompute RMS_energy\n","\tCompute Spectral_centroid\n","\tCompute Spectral_Entropy\n","\tCompute Temporal_Entropy\n","\tCompute ZCR\n","\tCompute Wave_SNR\n","\tCompute NB_peaks\n","\tCompute Acoustic_Diversity_Index_NR\n","\tCompute Acoustic_Evenness_Index_NR\n","\tCompute Bio_acoustic_Index_NR\n","\tCompute Spectral_Entropy_NR\n","File /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0720.mp3 deleted.\n","/content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/output_lists/output_lists/ADs_IG_2021/AD0720.xlsx\n"]},{"name":"stderr","output_type":"stream","text":["<ipython-input-46-020a70f8a660>:238: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  all_data = all_data.append(file_data, ignore_index=True)\n"]},{"name":"stdout","output_type":"stream","text":["###### CURRENT AD: /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0721.wav ######\n","###### - 33 / 53 - ######\n","Read the audio file: /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0721.wav\n","\tSuccessful read of the audio file: /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0721.wav\n","\tThe audio file contains more than one channel. Only the channel 0 will be used.\n","- Pre-processing - High-Pass Filtering: {'type': 'butterworth', 'order': 8, 'frequency': 300}\n","- Compute Indices\n","\tCompute Acoustic_Complexity_Index\n","\tCompute Acoustic_Diversity_Index\n","\tCompute Acoustic_Evenness_Index\n","\tCompute Bio_acoustic_Index\n","\tCompute Normalized_Difference_Sound_Index\n","\tCompute RMS_energy\n","\tCompute Spectral_centroid\n","\tCompute Spectral_Entropy\n","\tCompute Temporal_Entropy\n","\tCompute ZCR\n","\tCompute Wave_SNR\n","\tCompute NB_peaks\n","\tCompute Acoustic_Diversity_Index_NR\n","\tCompute Acoustic_Evenness_Index_NR\n","\tCompute Bio_acoustic_Index_NR\n","\tCompute Spectral_Entropy_NR\n","File /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0721.mp3 deleted.\n","/content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/output_lists/output_lists/ADs_IG_2021/AD0721.xlsx\n"]},{"name":"stderr","output_type":"stream","text":["<ipython-input-46-020a70f8a660>:238: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  all_data = all_data.append(file_data, ignore_index=True)\n"]},{"name":"stdout","output_type":"stream","text":["###### CURRENT AD: /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0722.wav ######\n","###### - 34 / 53 - ######\n","Read the audio file: /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0722.wav\n","\tSuccessful read of the audio file: /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0722.wav\n","\tThe audio file contains more than one channel. Only the channel 0 will be used.\n","- Pre-processing - High-Pass Filtering: {'type': 'butterworth', 'order': 8, 'frequency': 300}\n","- Compute Indices\n","\tCompute Acoustic_Complexity_Index\n","\tCompute Acoustic_Diversity_Index\n","\tCompute Acoustic_Evenness_Index\n","\tCompute Bio_acoustic_Index\n","\tCompute Normalized_Difference_Sound_Index\n","\tCompute RMS_energy\n","\tCompute Spectral_centroid\n","\tCompute Spectral_Entropy\n","\tCompute Temporal_Entropy\n","\tCompute ZCR\n","\tCompute Wave_SNR\n","\tCompute NB_peaks\n","\tCompute Acoustic_Diversity_Index_NR\n","\tCompute Acoustic_Evenness_Index_NR\n","\tCompute Bio_acoustic_Index_NR\n","\tCompute Spectral_Entropy_NR\n","File /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0722.mp3 deleted.\n","/content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/output_lists/output_lists/ADs_IG_2021/AD0722.xlsx\n"]},{"name":"stderr","output_type":"stream","text":["<ipython-input-46-020a70f8a660>:238: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  all_data = all_data.append(file_data, ignore_index=True)\n"]},{"name":"stdout","output_type":"stream","text":["###### CURRENT AD: /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0723.wav ######\n","###### - 35 / 53 - ######\n","Read the audio file: /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0723.wav\n","\tSuccessful read of the audio file: /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0723.wav\n","\tThe audio file contains more than one channel. Only the channel 0 will be used.\n","- Pre-processing - High-Pass Filtering: {'type': 'butterworth', 'order': 8, 'frequency': 300}\n","- Compute Indices\n","\tCompute Acoustic_Complexity_Index\n","\tCompute Acoustic_Diversity_Index\n","\tCompute Acoustic_Evenness_Index\n","\tCompute Bio_acoustic_Index\n","\tCompute Normalized_Difference_Sound_Index\n","\tCompute RMS_energy\n","\tCompute Spectral_centroid\n","\tCompute Spectral_Entropy\n","\tCompute Temporal_Entropy\n","\tCompute ZCR\n","\tCompute Wave_SNR\n","\tCompute NB_peaks\n","\tCompute Acoustic_Diversity_Index_NR\n","\tCompute Acoustic_Evenness_Index_NR\n","\tCompute Bio_acoustic_Index_NR\n","\tCompute Spectral_Entropy_NR\n","File /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0723.mp3 deleted.\n","/content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/output_lists/output_lists/ADs_IG_2021/AD0723.xlsx\n"]},{"name":"stderr","output_type":"stream","text":["<ipython-input-46-020a70f8a660>:238: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  all_data = all_data.append(file_data, ignore_index=True)\n"]},{"name":"stdout","output_type":"stream","text":["###### CURRENT AD: /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0724.wav ######\n","###### - 36 / 53 - ######\n","Read the audio file: /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0724.wav\n","\tSuccessful read of the audio file: /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0724.wav\n","\tThe audio file contains more than one channel. Only the channel 0 will be used.\n","- Pre-processing - High-Pass Filtering: {'type': 'butterworth', 'order': 8, 'frequency': 300}\n","- Compute Indices\n","\tCompute Acoustic_Complexity_Index\n","\tCompute Acoustic_Diversity_Index\n","\tCompute Acoustic_Evenness_Index\n","\tCompute Bio_acoustic_Index\n","\tCompute Normalized_Difference_Sound_Index\n","\tCompute RMS_energy\n","\tCompute Spectral_centroid\n","\tCompute Spectral_Entropy\n","\tCompute Temporal_Entropy\n","\tCompute ZCR\n","\tCompute Wave_SNR\n","\tCompute NB_peaks\n","\tCompute Acoustic_Diversity_Index_NR\n","\tCompute Acoustic_Evenness_Index_NR\n","\tCompute Bio_acoustic_Index_NR\n","\tCompute Spectral_Entropy_NR\n","File /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0724.mp3 deleted.\n","/content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/output_lists/output_lists/ADs_IG_2021/AD0724.xlsx\n"]},{"name":"stderr","output_type":"stream","text":["<ipython-input-46-020a70f8a660>:238: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  all_data = all_data.append(file_data, ignore_index=True)\n"]},{"name":"stdout","output_type":"stream","text":["###### CURRENT AD: /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0725.wav ######\n","###### - 37 / 53 - ######\n","Read the audio file: /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0725.wav\n","\tSuccessful read of the audio file: /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0725.wav\n","\tThe audio file contains more than one channel. Only the channel 0 will be used.\n","- Pre-processing - High-Pass Filtering: {'type': 'butterworth', 'order': 8, 'frequency': 300}\n","- Compute Indices\n","\tCompute Acoustic_Complexity_Index\n","\tCompute Acoustic_Diversity_Index\n","\tCompute Acoustic_Evenness_Index\n","\tCompute Bio_acoustic_Index\n","\tCompute Normalized_Difference_Sound_Index\n","\tCompute RMS_energy\n","\tCompute Spectral_centroid\n","\tCompute Spectral_Entropy\n","\tCompute Temporal_Entropy\n","\tCompute ZCR\n","\tCompute Wave_SNR\n","\tCompute NB_peaks\n","\tCompute Acoustic_Diversity_Index_NR\n","\tCompute Acoustic_Evenness_Index_NR\n","\tCompute Bio_acoustic_Index_NR\n","\tCompute Spectral_Entropy_NR\n","File /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0725.mp3 deleted.\n","/content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/output_lists/output_lists/ADs_IG_2021/AD0725.xlsx\n"]},{"name":"stderr","output_type":"stream","text":["<ipython-input-46-020a70f8a660>:238: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  all_data = all_data.append(file_data, ignore_index=True)\n"]},{"name":"stdout","output_type":"stream","text":["###### CURRENT AD: /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0726.wav ######\n","###### - 38 / 53 - ######\n","Read the audio file: /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0726.wav\n","\tSuccessful read of the audio file: /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0726.wav\n","\tThe audio file contains more than one channel. Only the channel 0 will be used.\n","- Pre-processing - High-Pass Filtering: {'type': 'butterworth', 'order': 8, 'frequency': 300}\n","- Compute Indices\n","\tCompute Acoustic_Complexity_Index\n","\tCompute Acoustic_Diversity_Index\n","\tCompute Acoustic_Evenness_Index\n","\tCompute Bio_acoustic_Index\n","\tCompute Normalized_Difference_Sound_Index\n","\tCompute RMS_energy\n","\tCompute Spectral_centroid\n","\tCompute Spectral_Entropy\n","\tCompute Temporal_Entropy\n","\tCompute ZCR\n","\tCompute Wave_SNR\n","\tCompute NB_peaks\n","\tCompute Acoustic_Diversity_Index_NR\n","\tCompute Acoustic_Evenness_Index_NR\n","\tCompute Bio_acoustic_Index_NR\n","\tCompute Spectral_Entropy_NR\n","File /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0726.mp3 deleted.\n","/content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/output_lists/output_lists/ADs_IG_2021/AD0726.xlsx\n"]},{"name":"stderr","output_type":"stream","text":["<ipython-input-46-020a70f8a660>:238: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  all_data = all_data.append(file_data, ignore_index=True)\n"]},{"name":"stdout","output_type":"stream","text":["###### CURRENT AD: /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0727.wav ######\n","###### - 39 / 53 - ######\n","Read the audio file: /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0727.wav\n","\tSuccessful read of the audio file: /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0727.wav\n","\tThe audio file contains more than one channel. Only the channel 0 will be used.\n","- Pre-processing - High-Pass Filtering: {'type': 'butterworth', 'order': 8, 'frequency': 300}\n","- Compute Indices\n","\tCompute Acoustic_Complexity_Index\n","\tCompute Acoustic_Diversity_Index\n","\tCompute Acoustic_Evenness_Index\n","\tCompute Bio_acoustic_Index\n","\tCompute Normalized_Difference_Sound_Index\n","\tCompute RMS_energy\n","\tCompute Spectral_centroid\n","\tCompute Spectral_Entropy\n","\tCompute Temporal_Entropy\n","\tCompute ZCR\n","\tCompute Wave_SNR\n","\tCompute NB_peaks\n","\tCompute Acoustic_Diversity_Index_NR\n","\tCompute Acoustic_Evenness_Index_NR\n","\tCompute Bio_acoustic_Index_NR\n","\tCompute Spectral_Entropy_NR\n","File /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0727.mp3 deleted.\n","/content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/output_lists/output_lists/ADs_IG_2021/AD0727.xlsx\n"]},{"name":"stderr","output_type":"stream","text":["<ipython-input-46-020a70f8a660>:238: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  all_data = all_data.append(file_data, ignore_index=True)\n"]},{"name":"stdout","output_type":"stream","text":["###### CURRENT AD: /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0728.wav ######\n","###### - 40 / 53 - ######\n","Read the audio file: /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0728.wav\n","\tSuccessful read of the audio file: /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0728.wav\n","\tThe audio file contains more than one channel. Only the channel 0 will be used.\n","- Pre-processing - High-Pass Filtering: {'type': 'butterworth', 'order': 8, 'frequency': 300}\n","- Compute Indices\n","\tCompute Acoustic_Complexity_Index\n","\tCompute Acoustic_Diversity_Index\n","\tCompute Acoustic_Evenness_Index\n","\tCompute Bio_acoustic_Index\n","\tCompute Normalized_Difference_Sound_Index\n","\tCompute RMS_energy\n","\tCompute Spectral_centroid\n","\tCompute Spectral_Entropy\n","\tCompute Temporal_Entropy\n","\tCompute ZCR\n","\tCompute Wave_SNR\n","\tCompute NB_peaks\n","\tCompute Acoustic_Diversity_Index_NR\n","\tCompute Acoustic_Evenness_Index_NR\n","\tCompute Bio_acoustic_Index_NR\n","\tCompute Spectral_Entropy_NR\n","File /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0728.mp3 deleted.\n","/content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/output_lists/output_lists/ADs_IG_2021/AD0728.xlsx\n"]},{"name":"stderr","output_type":"stream","text":["<ipython-input-46-020a70f8a660>:238: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  all_data = all_data.append(file_data, ignore_index=True)\n"]},{"name":"stdout","output_type":"stream","text":["###### CURRENT AD: /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0729.wav ######\n","###### - 41 / 53 - ######\n","Read the audio file: /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0729.wav\n","\tSuccessful read of the audio file: /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0729.wav\n","\tThe audio file contains more than one channel. Only the channel 0 will be used.\n","- Pre-processing - High-Pass Filtering: {'type': 'butterworth', 'order': 8, 'frequency': 300}\n","- Compute Indices\n","\tCompute Acoustic_Complexity_Index\n","\tCompute Acoustic_Diversity_Index\n","\tCompute Acoustic_Evenness_Index\n","\tCompute Bio_acoustic_Index\n","\tCompute Normalized_Difference_Sound_Index\n","\tCompute RMS_energy\n","\tCompute Spectral_centroid\n","\tCompute Spectral_Entropy\n","\tCompute Temporal_Entropy\n","\tCompute ZCR\n","\tCompute Wave_SNR\n","\tCompute NB_peaks\n","\tCompute Acoustic_Diversity_Index_NR\n","\tCompute Acoustic_Evenness_Index_NR\n","\tCompute Bio_acoustic_Index_NR\n","\tCompute Spectral_Entropy_NR\n","File /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0729.mp3 deleted.\n","/content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/output_lists/output_lists/ADs_IG_2021/AD0729.xlsx\n"]},{"name":"stderr","output_type":"stream","text":["<ipython-input-46-020a70f8a660>:238: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  all_data = all_data.append(file_data, ignore_index=True)\n"]},{"name":"stdout","output_type":"stream","text":["###### CURRENT AD: /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0730.wav ######\n","###### - 42 / 53 - ######\n","Read the audio file: /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0730.wav\n","\tSuccessful read of the audio file: /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0730.wav\n","\tThe audio file contains more than one channel. Only the channel 0 will be used.\n","- Pre-processing - High-Pass Filtering: {'type': 'butterworth', 'order': 8, 'frequency': 300}\n","- Compute Indices\n","\tCompute Acoustic_Complexity_Index\n","\tCompute Acoustic_Diversity_Index\n","\tCompute Acoustic_Evenness_Index\n","\tCompute Bio_acoustic_Index\n","\tCompute Normalized_Difference_Sound_Index\n","\tCompute RMS_energy\n","\tCompute Spectral_centroid\n","\tCompute Spectral_Entropy\n","\tCompute Temporal_Entropy\n","\tCompute ZCR\n","\tCompute Wave_SNR\n","\tCompute NB_peaks\n","\tCompute Acoustic_Diversity_Index_NR\n","\tCompute Acoustic_Evenness_Index_NR\n","\tCompute Bio_acoustic_Index_NR\n","\tCompute Spectral_Entropy_NR\n","File /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0730.mp3 deleted.\n","/content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/output_lists/output_lists/ADs_IG_2021/AD0730.xlsx\n"]},{"name":"stderr","output_type":"stream","text":["<ipython-input-46-020a70f8a660>:238: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  all_data = all_data.append(file_data, ignore_index=True)\n"]},{"name":"stdout","output_type":"stream","text":["###### CURRENT AD: /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0732.wav ######\n","###### - 43 / 53 - ######\n","Read the audio file: /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0732.wav\n","\tSuccessful read of the audio file: /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0732.wav\n","\tThe audio file contains more than one channel. Only the channel 0 will be used.\n","- Pre-processing - High-Pass Filtering: {'type': 'butterworth', 'order': 8, 'frequency': 300}\n","- Compute Indices\n","\tCompute Acoustic_Complexity_Index\n","\tCompute Acoustic_Diversity_Index\n","\tCompute Acoustic_Evenness_Index\n","\tCompute Bio_acoustic_Index\n","\tCompute Normalized_Difference_Sound_Index\n","\tCompute RMS_energy\n","\tCompute Spectral_centroid\n","\tCompute Spectral_Entropy\n","\tCompute Temporal_Entropy\n","\tCompute ZCR\n","\tCompute Wave_SNR\n","\tCompute NB_peaks\n","\tCompute Acoustic_Diversity_Index_NR\n","\tCompute Acoustic_Evenness_Index_NR\n","\tCompute Bio_acoustic_Index_NR\n","\tCompute Spectral_Entropy_NR\n","File /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0732.mp3 deleted.\n","/content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/output_lists/output_lists/ADs_IG_2021/AD0732.xlsx\n"]},{"name":"stderr","output_type":"stream","text":["<ipython-input-46-020a70f8a660>:238: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  all_data = all_data.append(file_data, ignore_index=True)\n"]},{"name":"stdout","output_type":"stream","text":["###### CURRENT AD: /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0733.wav ######\n","###### - 44 / 53 - ######\n","Read the audio file: /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0733.wav\n","\tSuccessful read of the audio file: /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0733.wav\n","\tThe audio file contains more than one channel. Only the channel 0 will be used.\n","- Pre-processing - High-Pass Filtering: {'type': 'butterworth', 'order': 8, 'frequency': 300}\n","- Compute Indices\n","\tCompute Acoustic_Complexity_Index\n","\tCompute Acoustic_Diversity_Index\n","\tCompute Acoustic_Evenness_Index\n","\tCompute Bio_acoustic_Index\n","\tCompute Normalized_Difference_Sound_Index\n","\tCompute RMS_energy\n","\tCompute Spectral_centroid\n","\tCompute Spectral_Entropy\n","\tCompute Temporal_Entropy\n","\tCompute ZCR\n","\tCompute Wave_SNR\n","\tCompute NB_peaks\n","\tCompute Acoustic_Diversity_Index_NR\n","\tCompute Acoustic_Evenness_Index_NR\n","\tCompute Bio_acoustic_Index_NR\n","\tCompute Spectral_Entropy_NR\n","File /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0733.mp3 deleted.\n","/content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/output_lists/output_lists/ADs_IG_2021/AD0733.xlsx\n"]},{"name":"stderr","output_type":"stream","text":["<ipython-input-46-020a70f8a660>:238: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  all_data = all_data.append(file_data, ignore_index=True)\n"]},{"name":"stdout","output_type":"stream","text":["###### CURRENT AD: /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0734.wav ######\n","###### - 45 / 53 - ######\n","Read the audio file: /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0734.wav\n","\tSuccessful read of the audio file: /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0734.wav\n","\tThe audio file contains more than one channel. Only the channel 0 will be used.\n","- Pre-processing - High-Pass Filtering: {'type': 'butterworth', 'order': 8, 'frequency': 300}\n","- Compute Indices\n","\tCompute Acoustic_Complexity_Index\n","\tCompute Acoustic_Diversity_Index\n","\tCompute Acoustic_Evenness_Index\n","\tCompute Bio_acoustic_Index\n","\tCompute Normalized_Difference_Sound_Index\n","\tCompute RMS_energy\n","\tCompute Spectral_centroid\n","\tCompute Spectral_Entropy\n","\tCompute Temporal_Entropy\n","\tCompute ZCR\n","\tCompute Wave_SNR\n","\tCompute NB_peaks\n","\tCompute Acoustic_Diversity_Index_NR\n","\tCompute Acoustic_Evenness_Index_NR\n","\tCompute Bio_acoustic_Index_NR\n","\tCompute Spectral_Entropy_NR\n","File /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0734.mp3 deleted.\n","/content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/output_lists/output_lists/ADs_IG_2021/AD0734.xlsx\n"]},{"name":"stderr","output_type":"stream","text":["<ipython-input-46-020a70f8a660>:238: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  all_data = all_data.append(file_data, ignore_index=True)\n"]},{"name":"stdout","output_type":"stream","text":["###### CURRENT AD: /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0735.wav ######\n","###### - 46 / 53 - ######\n","Read the audio file: /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0735.wav\n","\tSuccessful read of the audio file: /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0735.wav\n","\tThe audio file contains more than one channel. Only the channel 0 will be used.\n","- Pre-processing - High-Pass Filtering: {'type': 'butterworth', 'order': 8, 'frequency': 300}\n","- Compute Indices\n","\tCompute Acoustic_Complexity_Index\n","\tCompute Acoustic_Diversity_Index\n","\tCompute Acoustic_Evenness_Index\n","\tCompute Bio_acoustic_Index\n","\tCompute Normalized_Difference_Sound_Index\n","\tCompute RMS_energy\n","\tCompute Spectral_centroid\n","\tCompute Spectral_Entropy\n","\tCompute Temporal_Entropy\n","\tCompute ZCR\n","\tCompute Wave_SNR\n","\tCompute NB_peaks\n","\tCompute Acoustic_Diversity_Index_NR\n","\tCompute Acoustic_Evenness_Index_NR\n","\tCompute Bio_acoustic_Index_NR\n","\tCompute Spectral_Entropy_NR\n","File /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0735.mp3 deleted.\n","/content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/output_lists/output_lists/ADs_IG_2021/AD0735.xlsx\n"]},{"name":"stderr","output_type":"stream","text":["<ipython-input-46-020a70f8a660>:238: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  all_data = all_data.append(file_data, ignore_index=True)\n"]},{"name":"stdout","output_type":"stream","text":["###### CURRENT AD: /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0736.wav ######\n","###### - 47 / 53 - ######\n","Read the audio file: /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0736.wav\n","\tSuccessful read of the audio file: /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0736.wav\n","\tThe audio file contains more than one channel. Only the channel 0 will be used.\n","- Pre-processing - High-Pass Filtering: {'type': 'butterworth', 'order': 8, 'frequency': 300}\n","- Compute Indices\n","\tCompute Acoustic_Complexity_Index\n","\tCompute Acoustic_Diversity_Index\n","\tCompute Acoustic_Evenness_Index\n","\tCompute Bio_acoustic_Index\n","\tCompute Normalized_Difference_Sound_Index\n","\tCompute RMS_energy\n","\tCompute Spectral_centroid\n","\tCompute Spectral_Entropy\n","\tCompute Temporal_Entropy\n","\tCompute ZCR\n","\tCompute Wave_SNR\n","\tCompute NB_peaks\n","\tCompute Acoustic_Diversity_Index_NR\n","\tCompute Acoustic_Evenness_Index_NR\n","\tCompute Bio_acoustic_Index_NR\n","\tCompute Spectral_Entropy_NR\n","File /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0736.mp3 deleted.\n","/content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/output_lists/output_lists/ADs_IG_2021/AD0736.xlsx\n"]},{"name":"stderr","output_type":"stream","text":["<ipython-input-46-020a70f8a660>:238: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  all_data = all_data.append(file_data, ignore_index=True)\n"]},{"name":"stdout","output_type":"stream","text":["###### CURRENT AD: /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0737.wav ######\n","###### - 48 / 53 - ######\n","Read the audio file: /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0737.wav\n","\tSuccessful read of the audio file: /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0737.wav\n","\tThe audio file contains more than one channel. Only the channel 0 will be used.\n","- Pre-processing - High-Pass Filtering: {'type': 'butterworth', 'order': 8, 'frequency': 300}\n","- Compute Indices\n","\tCompute Acoustic_Complexity_Index\n","\tCompute Acoustic_Diversity_Index\n","\tCompute Acoustic_Evenness_Index\n","\tCompute Bio_acoustic_Index\n","\tCompute Normalized_Difference_Sound_Index\n","\tCompute RMS_energy\n","\tCompute Spectral_centroid\n","\tCompute Spectral_Entropy\n","\tCompute Temporal_Entropy\n","\tCompute ZCR\n","\tCompute Wave_SNR\n","\tCompute NB_peaks\n","\tCompute Acoustic_Diversity_Index_NR\n","\tCompute Acoustic_Evenness_Index_NR\n","\tCompute Bio_acoustic_Index_NR\n","\tCompute Spectral_Entropy_NR\n","File /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0737.mp3 deleted.\n","/content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/output_lists/output_lists/ADs_IG_2021/AD0737.xlsx\n"]},{"name":"stderr","output_type":"stream","text":["<ipython-input-46-020a70f8a660>:238: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  all_data = all_data.append(file_data, ignore_index=True)\n"]},{"name":"stdout","output_type":"stream","text":["###### CURRENT AD: /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0738.wav ######\n","###### - 49 / 53 - ######\n","Read the audio file: /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0738.wav\n","\tSuccessful read of the audio file: /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0738.wav\n","\tThe audio file contains more than one channel. Only the channel 0 will be used.\n","- Pre-processing - High-Pass Filtering: {'type': 'butterworth', 'order': 8, 'frequency': 300}\n","- Compute Indices\n","\tCompute Acoustic_Complexity_Index\n","\tCompute Acoustic_Diversity_Index\n","\tCompute Acoustic_Evenness_Index\n","\tCompute Bio_acoustic_Index\n","\tCompute Normalized_Difference_Sound_Index\n","\tCompute RMS_energy\n","\tCompute Spectral_centroid\n","\tCompute Spectral_Entropy\n","\tCompute Temporal_Entropy\n","\tCompute ZCR\n","\tCompute Wave_SNR\n","\tCompute NB_peaks\n","\tCompute Acoustic_Diversity_Index_NR\n","\tCompute Acoustic_Evenness_Index_NR\n","\tCompute Bio_acoustic_Index_NR\n","\tCompute Spectral_Entropy_NR\n","File /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0738.mp3 deleted.\n","/content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/output_lists/output_lists/ADs_IG_2021/AD0738.xlsx\n"]},{"name":"stderr","output_type":"stream","text":["<ipython-input-46-020a70f8a660>:238: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  all_data = all_data.append(file_data, ignore_index=True)\n"]},{"name":"stdout","output_type":"stream","text":["###### CURRENT AD: /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0739.wav ######\n","###### - 50 / 53 - ######\n","Read the audio file: /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0739.wav\n","\tSuccessful read of the audio file: /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0739.wav\n","\tThe audio file contains more than one channel. Only the channel 0 will be used.\n","- Pre-processing - High-Pass Filtering: {'type': 'butterworth', 'order': 8, 'frequency': 300}\n","- Compute Indices\n","\tCompute Acoustic_Complexity_Index\n","\tCompute Acoustic_Diversity_Index\n","\tCompute Acoustic_Evenness_Index\n","\tCompute Bio_acoustic_Index\n","\tCompute Normalized_Difference_Sound_Index\n","\tCompute RMS_energy\n","\tCompute Spectral_centroid\n","\tCompute Spectral_Entropy\n","\tCompute Temporal_Entropy\n","\tCompute ZCR\n","\tCompute Wave_SNR\n","\tCompute NB_peaks\n","\tCompute Acoustic_Diversity_Index_NR\n","\tCompute Acoustic_Evenness_Index_NR\n","\tCompute Bio_acoustic_Index_NR\n","\tCompute Spectral_Entropy_NR\n","File /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0739.mp3 deleted.\n","/content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/output_lists/output_lists/ADs_IG_2021/AD0739.xlsx\n"]},{"name":"stderr","output_type":"stream","text":["<ipython-input-46-020a70f8a660>:238: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  all_data = all_data.append(file_data, ignore_index=True)\n"]},{"name":"stdout","output_type":"stream","text":["###### CURRENT AD: /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0740.wav ######\n","###### - 51 / 53 - ######\n","Read the audio file: /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0740.wav\n","\tSuccessful read of the audio file: /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0740.wav\n","\tThe audio file contains more than one channel. Only the channel 0 will be used.\n","- Pre-processing - High-Pass Filtering: {'type': 'butterworth', 'order': 8, 'frequency': 300}\n","- Compute Indices\n","\tCompute Acoustic_Complexity_Index\n","\tCompute Acoustic_Diversity_Index\n","\tCompute Acoustic_Evenness_Index\n","\tCompute Bio_acoustic_Index\n","\tCompute Normalized_Difference_Sound_Index\n","\tCompute RMS_energy\n","\tCompute Spectral_centroid\n","\tCompute Spectral_Entropy\n","\tCompute Temporal_Entropy\n","\tCompute ZCR\n","\tCompute Wave_SNR\n","\tCompute NB_peaks\n","\tCompute Acoustic_Diversity_Index_NR\n","\tCompute Acoustic_Evenness_Index_NR\n","\tCompute Bio_acoustic_Index_NR\n","\tCompute Spectral_Entropy_NR\n","File /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0740.mp3 deleted.\n","/content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/output_lists/output_lists/ADs_IG_2021/AD0740.xlsx\n"]},{"name":"stderr","output_type":"stream","text":["<ipython-input-46-020a70f8a660>:238: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  all_data = all_data.append(file_data, ignore_index=True)\n"]},{"name":"stdout","output_type":"stream","text":["###### CURRENT AD: /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0741.wav ######\n","###### - 52 / 53 - ######\n","Read the audio file: /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0741.wav\n","\tSuccessful read of the audio file: /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0741.wav\n","\tThe audio file contains more than one channel. Only the channel 0 will be used.\n","- Pre-processing - High-Pass Filtering: {'type': 'butterworth', 'order': 8, 'frequency': 300}\n","- Compute Indices\n","\tCompute Acoustic_Complexity_Index\n","\tCompute Acoustic_Diversity_Index\n","\tCompute Acoustic_Evenness_Index\n","\tCompute Bio_acoustic_Index\n","\tCompute Normalized_Difference_Sound_Index\n","\tCompute RMS_energy\n","\tCompute Spectral_centroid\n","\tCompute Spectral_Entropy\n","\tCompute Temporal_Entropy\n","\tCompute ZCR\n","\tCompute Wave_SNR\n","\tCompute NB_peaks\n","\tCompute Acoustic_Diversity_Index_NR\n","\tCompute Acoustic_Evenness_Index_NR\n","\tCompute Bio_acoustic_Index_NR\n","\tCompute Spectral_Entropy_NR\n","File /content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Test/ADs_IG_2021_wav/AD0741.mp3 deleted.\n","/content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/output_lists/output_lists/ADs_IG_2021/AD0741.xlsx\n"]},{"name":"stderr","output_type":"stream","text":["<ipython-input-46-020a70f8a660>:238: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  all_data = all_data.append(file_data, ignore_index=True)\n"]}],"source":["if not single_output_file:\n","\n","  print(\"audio directory: \", audio_dir)\n","  print(\"output_directory: \", output_dir)\n","\n","  for idx_file, filename in enumerate(all_audio_file_path):\n","\n","      ad_name = filename.split('/')[-1]\n","      year_folder_name = filename.split('/')[-2]\n","      #print(ad_name, year_folder_name)\n","\n","      print(f'###### CURRENT AD: {filename} ######')\n","      print(f'###### - {all_audio_file_path.index(filename)} / {len(all_audio_file_path)} - ######')\n","\n","      # Initialize an empty DataFrame for each individual ad\n","      all_data = pd.DataFrame()\n","\n","      # Read signal -------------------------------------\n","      file = AudioFile(filename, verbose=True)\n","\n","      # Pre-processing -----------------------------------------------------------------------------------\n","      if 'Filtering' in data_config:\n","          if data_config['Filtering']['type'] == 'butterworth':\n","              print('- Pre-processing - High-Pass Filtering:', data_config['Filtering'])\n","              freq_filter = data_config['Filtering']['frequency']\n","              Wn = freq_filter/float(file.niquist)\n","              order = data_config['Filtering']['order']\n","              [b,a] = signal.butter(order, Wn, btype='highpass')\n","              # to plot the frequency response\n","              #w, h = signal.freqz(b, a, worN=2000)\n","              #plt.plot((file.sr * 0.5 / np.pi) * w, abs(h))\n","              #plt.show()\n","              file.process_filtering(signal.filtfilt(b, a, file.sig_float))\n","          elif data_config['Filtering']['type'] == 'windowed_sinc':\n","              print('- Pre-processing - High-Pass Filtering:', data_config['Filtering'])\n","              freq_filter = data_config['Filtering']['frequency']\n","              fc = freq_filter / float(file.sr)\n","              roll_off = data_config['Filtering']['roll_off']\n","              b = roll_off / float(file.sr)\n","              N = int(np.ceil((4 / b)))\n","              if not N % 2: N += 1  # Make sure that N is odd.\n","              n = np.arange(N)\n","              # Compute a low-pass filter.\n","              h = np.sinc(2 * fc * (n - (N - 1) / 2.))\n","              w = np.blackman(N)\n","              h = h * w\n","              h = h / np.sum(h)\n","              # Create a high-pass filter from the low-pass filter through spectral inversion.\n","              h = -h\n","              h[(N - 1) / 2] += 1\n","              file.process_filtering(np.convolve(file.sig_float, h))\n","\n","      # Compute Indices -----------------------------------------------------------------------------------\n","      print('- Compute Indices')\n","      ci = data_config['Indices']  # use to simplify the notation\n","      for index_name in ci:  # iterate over the index names (key of dictionary in the yml file)\n","\n","          if index_name == 'Acoustic_Complexity_Index':\n","              print('\\tCompute', index_name)\n","              spectro, _ = compute_spectrogram(file, **ci[index_name]['spectro'])\n","              methodToCall = globals().get(ci[index_name]['function'])\n","              j_bin = int(ci[index_name]['arguments']['j_bin'] * file.sr / ci[index_name]['spectro']['windowHop'])  # transform j_bin in samples\n","              main_value, temporal_values = methodToCall(spectro, j_bin)\n","              file.indices[index_name] = Index(index_name, temporal_values=temporal_values, main_value=main_value)\n","\n","          elif index_name == 'Acoustic_Diversity_Index':\n","              print('\\tCompute', index_name)\n","              methodToCall = globals().get(ci[index_name]['function'])\n","              freq_band_Hz = ci[index_name]['arguments']['max_freq'] / ci[index_name]['arguments']['freq_step']\n","              windowLength = int(file.sr / freq_band_Hz)\n","              spectro, _ = compute_spectrogram(file, windowLength=windowLength, windowHop=windowLength, scale_audio=True, square=False, windowType='hamming', centered=False, normalized=False)\n","              main_value = methodToCall(spectro, freq_band_Hz, **ci[index_name]['arguments'])\n","              file.indices[index_name] = Index(index_name, main_value=main_value)\n","\n","          elif index_name == 'Acoustic_Evenness_Index':\n","              print('\\tCompute', index_name)\n","              methodToCall = globals().get(ci[index_name]['function'])\n","              freq_band_Hz = ci[index_name]['arguments']['max_freq'] / ci[index_name]['arguments']['freq_step']\n","              windowLength = int(file.sr / freq_band_Hz)\n","              spectro, _ = compute_spectrogram(file, windowLength=windowLength, windowHop=windowLength, scale_audio=True, square=False, windowType='hamming', centered=False, normalized=False)\n","              main_value = methodToCall(spectro, freq_band_Hz, **ci[index_name]['arguments'])\n","              file.indices[index_name] = Index(index_name, main_value=main_value)\n","\n","          elif index_name == 'Bio_acoustic_Index':\n","              print('\\tCompute', index_name)\n","              spectro, frequencies = compute_spectrogram(file, **ci[index_name]['spectro'])\n","              methodToCall = globals().get(ci[index_name]['function'])\n","              main_value = methodToCall(spectro, frequencies, **ci[index_name]['arguments'])\n","              file.indices[index_name] = Index(index_name, main_value=main_value)\n","\n","          elif index_name == 'Normalized_Difference_Sound_Index':\n","              print('\\tCompute', index_name)\n","              methodToCall = globals().get(ci[index_name]['function'])\n","              main_value = methodToCall(file, **ci[index_name]['arguments'])\n","              file.indices[index_name] = Index(index_name, main_value=main_value)\n","\n","          elif index_name == 'RMS_energy':\n","              print('\\tCompute', index_name)\n","              methodToCall = globals().get(ci[index_name]['function'])\n","              temporal_values = methodToCall(file, **ci[index_name]['arguments'])\n","              file.indices[index_name] = Index(index_name, temporal_values=temporal_values)\n","\n","          elif index_name == 'Spectral_centroid':\n","              print('\\tCompute', index_name)\n","              spectro, frequencies = compute_spectrogram(file, **ci[index_name]['spectro'])\n","              methodToCall = globals().get(ci[index_name]['function'])\n","              temporal_values = methodToCall(spectro, frequencies)\n","              file.indices[index_name] = Index(index_name, temporal_values=temporal_values)\n","\n","          elif index_name == 'Spectral_Entropy':\n","              print('\\tCompute', index_name)\n","              spectro, _ = compute_spectrogram(file, **ci[index_name]['spectro'])\n","              methodToCall = globals().get(ci[index_name]['function'])\n","              main_value = methodToCall(spectro)\n","              file.indices[index_name] = Index(index_name, main_value=main_value)\n","\n","          elif index_name == 'Temporal_Entropy':\n","              print('\\tCompute', index_name)\n","              methodToCall = globals().get(ci[index_name]['function'])\n","              main_value = methodToCall(file, **ci[index_name]['arguments'])\n","              file.indices[index_name] = Index(index_name, main_value=main_value)\n","\n","          elif index_name == 'ZCR':\n","              print('\\tCompute', index_name)\n","              methodToCall = globals().get(ci[index_name]['function'])\n","              temporal_values = methodToCall(file, **ci[index_name]['arguments'])\n","              file.indices[index_name] = Index(index_name, temporal_values=temporal_values)\n","\n","          elif index_name == 'Wave_SNR':\n","              print('\\tCompute', index_name)\n","              methodToCall = globals().get(ci[index_name]['function'])\n","              values = methodToCall(file, **ci[index_name]['arguments'])\n","              file.indices[index_name] = Index(index_name, values=values)\n","\n","          elif index_name == 'NB_peaks':\n","              print('\\tCompute', index_name)\n","              spectro, frequencies = compute_spectrogram(file, **ci[index_name]['spectro'])\n","              methodToCall = globals().get(ci[index_name]['function'])\n","              main_value = methodToCall(spectro, frequencies, **ci[index_name]['arguments'])\n","              file.indices[index_name] = Index(index_name, main_value=main_value)\n","\n","          elif index_name == 'Acoustic_Diversity_Index_NR': # Acoustic_Diversity_Index with Noise Removed spectrograms\n","              print('\\tCompute', index_name)\n","              methodToCall = globals().get(ci[index_name]['function'])\n","              freq_band_Hz = ci[index_name]['arguments']['max_freq'] / ci[index_name]['arguments']['freq_step']\n","              windowLength = int(file.sr / freq_band_Hz)\n","              spectro,_ = compute_spectrogram(file, windowLength=windowLength, windowHop= windowLength, scale_audio=True, square=False, windowType='hamming', centered=False, normalized= False )\n","              spectro_noise_removed = remove_noiseInSpectro(spectro, **ci[index_name]['remove_noiseInSpectro'])\n","              main_value = methodToCall(spectro_noise_removed, freq_band_Hz, **ci[index_name]['arguments'])\n","              file.indices[index_name] = Index(index_name, main_value=main_value)\n","\n","          elif index_name == 'Acoustic_Evenness_Index_NR': # Acoustic_Evenness_Index with Noise Removed spectrograms\n","              print('\\tCompute', index_name)\n","              methodToCall = globals().get(ci[index_name]['function'])\n","              freq_band_Hz = ci[index_name]['arguments']['max_freq'] / ci[index_name]['arguments']['freq_step']\n","              windowLength = int(file.sr / freq_band_Hz)\n","              spectro,_ = compute_spectrogram(file, windowLength=windowLength, windowHop= windowLength, scale_audio=True, square=False, windowType='hamming', centered=False, normalized= False )\n","              spectro_noise_removed = remove_noiseInSpectro(spectro, **ci[index_name]['remove_noiseInSpectro'])\n","              main_value = methodToCall(spectro_noise_removed, freq_band_Hz, **ci[index_name]['arguments'])\n","              file.indices[index_name] = Index(index_name, main_value=main_value)\n","\n","          elif index_name == 'Bio_acoustic_Index_NR': # Bio_acoustic_Index with Noise Removed spectrograms\n","              print('\\tCompute', index_name)\n","              spectro, frequencies = compute_spectrogram(file, **ci[index_name]['spectro'])\n","              spectro_noise_removed = remove_noiseInSpectro(spectro, **ci[index_name]['remove_noiseInSpectro'])\n","              methodToCall = globals().get(ci[index_name]['function'])\n","              main_value = methodToCall(spectro_noise_removed, frequencies, **ci[index_name]['arguments'])\n","              file.indices[index_name] = Index(index_name, main_value=main_value)\n","\n","          elif index_name == 'Spectral_Entropy_NR': # Spectral_Entropy with Noise Removed spectrograms\n","              print('\\tCompute', index_name)\n","              spectro, _ = compute_spectrogram(file, **ci[index_name]['spectro'])\n","              spectro_noise_removed = remove_noiseInSpectro(spectro, **ci[index_name]['remove_noiseInSpectro'])\n","              methodToCall = globals().get(ci[index_name]['function'])\n","              main_value = methodToCall(spectro_noise_removed)\n","              file.indices[index_name] = Index(index_name, main_value=main_value)\n","\n","      # Output Indices -----------------------------------------------------------------------------------\n","      #if idx_file == 0: # wenn ertse Datei\n","      #    with open(output_csv_file, 'w') as f_object:\n","      #        writer_object = writer(f_object)\n","      #        keys = ['filename']\n","      #        values = [file.file_name]\n","      #        for idx, current_index in file.indices.items():\n","      #            for key, value in current_index.__dict__.items():\n","      #                if key != 'name':\n","      #                    keys.append(idx + '__' + key)\n","      #                    values.append(value)\n","      #        writer_object.writerow(keys)\n","      #        writer_object.writerow(values)\n","      #        f_object.close()\n","      #else: # alles nach der ersten Datei\n","      #    with open(output_csv_file, 'a') as f_object:\n","      #        writer_object = writer(f_object)\n","      #        values = [file.file_name]\n","      #        for idx, current_index in file.indices.items():\n","      #            for key, value in current_index.__dict__.items():\n","      #                if key != 'name':\n","      #                    values.append(value)\n","      #        writer_object.writerow(values)\n","      #        f_object.close()\n","      #print(\"\\n\")\n","      # Create a dictionary to store data for the current file\n","\n","      file_data = {'filename': file.file_name}\n","\n","      for idx, current_index in file.indices.items():\n","          for key, value in current_index.__dict__.items():\n","              if key != 'name':\n","                  file_data[idx + '__' + key] = value\n","\n","      # Calculate additional values\n","      additional_values = calculate_additional_values(file.sig_float)\n","\n","      # compression rate\n","      original_size = os.path.getsize(filename)\n","      audio = AudioSegment.from_wav(filename)\n","      original_duration = audio.duration_seconds\n","      mp3_path = os.path.join(filename.replace(\".wav\", \".mp3\"))\n","      compress_wav_to_mp3(filename, mp3_path)\n","      compressed_size = os.path.getsize(mp3_path)\n","      compression_ratio = measure_compression_ratio(original_size, compressed_size, original_duration)\n","      compression_ratio_per_second = measure_compression_ratio_per_second(compression_ratio, original_duration)\n","      delete_file(mp3_path)\n","\n","      # Append additional values to the file_data dictionary\n","      duration, tempo, avg_db, min_db, max_db, max_db_value = additional_values\n","      file_data['duration'] = duration\n","      file_data['tempo'] = tempo\n","      file_data['avg_db'] = avg_db\n","      file_data['min_db'] = min_db\n","      file_data['max_db'] = max_db\n","      file_data['max_db_value'] = max_db_value\n","      file_data['compression_ratio'] = compression_ratio\n","      file_data['compression_ratio_per_second'] = compression_ratio_per_second\n","\n","      # Append the data for the current file to the DataFrame\n","      all_data = all_data.append(file_data, ignore_index=True)\n","\n","      output_xlsx_file = os.path.join(output_dir, year_folder_name[0:11] + \"/\" + ad_name[0:7] + \"xlsx\")\n","\n","      print(output_xlsx_file)\n","\n","      #To Excel\n","      #with pd.ExcelWriter(output_xlsx_file, engine='openpyxl', mode='a', if_sheet_exists='overlay') as writer:\n","      #  all_data.to_excel(writer, sheet_name='Acoustic_Indices', index=False)\n","      try:\n","          with pd.ExcelWriter(output_xlsx_file, engine='openpyxl', mode='a', if_sheet_exists='overlay') as writer:\n","              all_data.to_excel(writer, sheet_name='Acoustic_Indices', index=False)\n","\n","      except Exception as e:\n","          print(f\"Error: {e}\")\n","\n","          # Versuche, die Datei zu löschen (falls sie existiert)\n","          if os.path.exists(output_xlsx_file):\n","              os.remove(output_xlsx_file)\n","              print(f\"File '{output_xlsx_file}' deleted.\")\n","\n","          # Erstelle eine neue leere Datei\n","          with pd.ExcelWriter(output_xlsx_file, engine='openpyxl', mode='w') as writer:\n","              all_data.to_excel(writer, sheet_name='Acoustic_Indices', index=False)\n","              print(f\"New file '{output_xlsx_file}' created and written.\")"]}],"metadata":{"accelerator":"TPU","colab":{"provenance":[{"file_id":"1e6gtuF1-nQAflcMlP0kbDHvSxYIb0aG3","timestamp":1705574462824},{"file_id":"https://github.com/pyannote/pyannote-audio/blob/develop/tutorials/intro.ipynb","timestamp":1703704769456}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"},"widgets":{"application/vnd.jupyter.widget-state+json":{"2d52b4a045c14f44903b010261527ed6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"LabelModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_35ae3d30cf45414a98a606b2c9ba40be","placeholder":"​","style":"IPY_MODEL_504a095f33214bd4b36646d088b49e5e","value":"Token is valid (permission: write)."}},"34222225ba97456694d7298c879b4b19":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"LabelModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a5865557a7474b0ebeed270b592f38ac","placeholder":"​","style":"IPY_MODEL_45d6c82b2ec74893ae2b5a86eeb1daff","value":"Your token has been saved to /root/.cache/huggingface/token"}},"35ae3d30cf45414a98a606b2c9ba40be":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3ff01220d0d449f99775a49369910f63":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"LabelModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a9bf397a62944000ae68a482a8ae20c1","placeholder":"​","style":"IPY_MODEL_aa5d8610fb084fa59991f262fed05bad","value":"Your token has been saved in your configured git credential helpers (store)."}},"45d6c82b2ec74893ae2b5a86eeb1daff":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4cbb11e0ee474f2eaf7ffa7e3583e9f8":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"504a095f33214bd4b36646d088b49e5e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6c2729e3f4b84d349a112da2ccf77b25":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":"center","align_self":null,"border":null,"bottom":null,"display":"flex","flex":null,"flex_flow":"column","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"50%"}},"8a7f1963cc7848a48b2c8a5ab1ff17a3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a5865557a7474b0ebeed270b592f38ac":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a9bf397a62944000ae68a482a8ae20c1":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"aa5d8610fb084fa59991f262fed05bad":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c9ff155a579140e2bb19bfbadc4e032f":{"model_module":"@jupyter-widgets/output","model_module_version":"1.0.0","model_name":"OutputModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/output","_model_module_version":"1.0.0","_model_name":"OutputModel","_view_count":null,"_view_module":"@jupyter-widgets/output","_view_module_version":"1.0.0","_view_name":"OutputView","layout":"IPY_MODEL_4cbb11e0ee474f2eaf7ffa7e3583e9f8","msg_id":"","outputs":[{"data":{"text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">segmentation         <span style=\"color: #729c1f; text-decoration-color: #729c1f\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span> <span style=\"color: #800080; text-decoration-color: #800080\">100%</span> <span style=\"color: #808000; text-decoration-color: #808000\">0:00:00</span>\nspeaker_counting     <span style=\"color: #729c1f; text-decoration-color: #729c1f\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span> <span style=\"color: #800080; text-decoration-color: #800080\">100%</span> <span style=\"color: #808000; text-decoration-color: #808000\">0:00:00</span>\nembeddings           <span style=\"color: #729c1f; text-decoration-color: #729c1f\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span> <span style=\"color: #800080; text-decoration-color: #800080\">100%</span> <span style=\"color: #808000; text-decoration-color: #808000\">0:00:08</span>\ndiscrete_diarization <span style=\"color: #729c1f; text-decoration-color: #729c1f\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span> <span style=\"color: #800080; text-decoration-color: #800080\">100%</span> <span style=\"color: #808000; text-decoration-color: #808000\">0:00:00</span>\n</pre>\n","text/plain":"segmentation         \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m100%\u001b[0m \u001b[33m0:00:00\u001b[0m\nspeaker_counting     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m100%\u001b[0m \u001b[33m0:00:00\u001b[0m\nembeddings           \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m100%\u001b[0m \u001b[33m0:00:08\u001b[0m\ndiscrete_diarization \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m100%\u001b[0m \u001b[33m0:00:00\u001b[0m\n"},"metadata":{},"output_type":"display_data"}]}},"d0647072ab8044c7a1ff53b9e01f7189":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"VBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"VBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"VBoxView","box_style":"","children":["IPY_MODEL_2d52b4a045c14f44903b010261527ed6","IPY_MODEL_3ff01220d0d449f99775a49369910f63","IPY_MODEL_34222225ba97456694d7298c879b4b19","IPY_MODEL_d9bce56fe264426b85e729cf46a3973f"],"layout":"IPY_MODEL_6c2729e3f4b84d349a112da2ccf77b25"}},"d9bce56fe264426b85e729cf46a3973f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"LabelModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e9be5a21ad9046d181c258ea1487de5c","placeholder":"​","style":"IPY_MODEL_8a7f1963cc7848a48b2c8a5ab1ff17a3","value":"Login successful"}},"e9be5a21ad9046d181c258ea1487de5c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}}}},"nbformat":4,"nbformat_minor":0}