{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1EqcmIwFuXCZ"
   },
   "source": [
    "#**Super-Bowl Bildanalyse**\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZJtISBlJ8X7N"
   },
   "source": [
    "# 1. Set-Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "J9cFp7ESh9ng"
   },
   "outputs": [],
   "source": [
    "# import the neccessary libraries\n",
    "import os, json, cv2, random\n",
    "import numpy as np\n",
    "# from google.colab import drive\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from openpyxl.styles import Font\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 25363,
     "status": "ok",
     "timestamp": 1709720758003,
     "user": {
      "displayName": "Flavio Kuka",
      "userId": "09654499042520950340"
     },
     "user_tz": -60
    },
    "id": "z0XRxb7qnInb",
    "outputId": "6e962bf7-e365-4002-d500-e8171816a591"
   },
   "outputs": [],
   "source": [
    "# # OPTIONAL\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "uSca70_qzVTV"
   },
   "outputs": [],
   "source": [
    "# Load super-categories DataFrame\n",
    "# NOTE: We couldn't find the super-categories in the COCO dataset and therefore we wrote them by hand in an excel file\n",
    "super_categories_df = pd.read_excel(f'{os.getenv(\"BILDANALYSE_MODELS_COCO_DIR\")}/super-categories.xlsx')\n",
    "super_categories_df.set_index(\"class_name\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hpjg6z9J6lEw"
   },
   "source": [
    "# 2. Installation of the pre-trained models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VKD0QmuS6_uj"
   },
   "source": [
    "## 2.1 detectron2 (Object Detection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 25702,
     "status": "ok",
     "timestamp": 1705575649184,
     "user": {
      "displayName": "Flavio Kuka",
      "userId": "09654499042520950340"
     },
     "user_tz": -60
    },
    "id": "a_lPuyTT7H6v",
    "outputId": "c859e994-97f7-4d78-ff0a-520a044c251c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyyaml==5.1 in ./03. main_Bildanalyse-venv/lib/python3.12/site-packages (5.1)\n",
      "fatal: destination path 'detectron2' already exists and is not an empty directory.\n"
     ]
    },
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: '/home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/Final_Files/01. Bildanalyse/03. main_Script/detectron2/detectron2/model_zoo/configs'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\u001b[38;5;241m,\u001b[39m \u001b[38;5;21;01mos\u001b[39;00m\u001b[38;5;241m,\u001b[39m \u001b[38;5;21;01mdistutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\n\u001b[1;32m      4\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgit clone \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://github.com/facebookresearch/detectron2\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m dist \u001b[38;5;241m=\u001b[39m \u001b[43mdistutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_setup\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./detectron2/setup.py\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpython -m pip install \u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124m.join([f\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;132;01m{x}\u001b[39;00m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m for x in dist.install_requires])}\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      7\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39minsert(\u001b[38;5;241m0\u001b[39m, os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mabspath(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./detectron2\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "File \u001b[0;32m~/Development/Commercial-Brand-Differentiating-Message-Analysis/Final_Files/01. Bildanalyse/03. main_Script/03. main_Bildanalyse-venv/lib/python3.12/site-packages/setuptools/_distutils/core.py:265\u001b[0m, in \u001b[0;36mrun_setup\u001b[0;34m(script_name, script_args, stop_after)\u001b[0m\n\u001b[1;32m    263\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m tokenize\u001b[38;5;241m.\u001b[39mopen(script_name) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    264\u001b[0m         code \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mread()\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 265\u001b[0m         \u001b[43mexec\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    267\u001b[0m     sys\u001b[38;5;241m.\u001b[39margv \u001b[38;5;241m=\u001b[39m save_argv\n",
      "File \u001b[0;32m<string>:160\u001b[0m\n",
      "File \u001b[0;32m<string>:126\u001b[0m, in \u001b[0;36mget_model_zoo_configs\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mPermissionError\u001b[0m: [Errno 13] Permission denied: '/home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/Final_Files/01. Bildanalyse/03. main_Script/detectron2/detectron2/model_zoo/configs'"
     ]
    }
   ],
   "source": [
    "#  https://detectron2.readthedocs.io/tutorials/install.html\n",
    "!python -m pip install pyyaml==5.1\n",
    "import sys, os, distutils.core\n",
    "!git clone 'https://github.com/facebookresearch/detectron2'\n",
    "dist = distutils.core.run_setup(\"./detectron2/setup.py\")\n",
    "!python -m pip install {' '.join([f\"'{x}'\" for x in dist.install_requires])}\n",
    "sys.path.insert(0, os.path.abspath('./detectron2'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3hagrMdJ7j_b"
   },
   "outputs": [],
   "source": [
    "# import detectron2 and its utilities\n",
    "import torch\n",
    "import detectron2\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "from detectron2.data import MetadataCatalog, DatasetCatalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2868,
     "status": "ok",
     "timestamp": 1705575655522,
     "user": {
      "displayName": "Flavio Kuka",
      "userId": "09654499042520950340"
     },
     "user_tz": -60
    },
    "id": "FMDFb2tCuBz6",
    "outputId": "d1ae9fcf-47c9-484b-e911-632c036d9228"
   },
   "outputs": [],
   "source": [
    "# Get the Configurations\n",
    "cfg = get_cfg()\n",
    "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\"))\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5\n",
    "cfg.MODEL.DEVICE = \"cpu\"\n",
    "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\")\n",
    "predictor = DefaultPredictor(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7evAGtcH7f8A"
   },
   "source": [
    "## 2.2 DEX: Deep EXpectation of apparent age from a single image (Age and Gender)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "mGX43VY8g3Vi"
   },
   "outputs": [],
   "source": [
    "# !git clone 'https://github.com/serengil/tensorflow-101.git'\n",
    "# NOTE: The files imported in this part were taken from the links below\n",
    "#model structure: https://data.vision.ee.ethz.ch/cvl/rrothe/imdb-wiki/static/age.prototxt\n",
    "#pre-trained weights: https://data.vision.ee.ethz.ch/cvl/rrothe/imdb-wiki/static/dex_chalearn_iccv2015.caffemodel\n",
    "#age_model = cv2.dnn.readNetFromCaffe(\"age.prototxt\", \"dex_chalearn_iccv2015.caffemodel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "pvgl9mPtmokW"
   },
   "outputs": [],
   "source": [
    "\n",
    "age_prototxt = f'{os.getenv(\"BILDANALYSE_MODELS_DEX_DIR\")}/age.prototxt'\n",
    "dex_model = f'{os.getenv(\"BILDANALYSE_MODELS_DEX_DIR\")}/dex_chalearn_iccv2015.caffemodel'\n",
    "gender_prototxt = f'{os.getenv(\"BILDANALYSE_MODELS_DEX_DIR\")}/gender.prototxt'\n",
    "gender_model =  f'{os.getenv(\"BILDANALYSE_MODELS_DEX_DIR\")}/gender.caffemodel'\n",
    "\n",
    "age_model = cv2.dnn.readNet(age_prototxt, dex_model)\n",
    "gender_model = cv2.dnn.readNet(gender_prototxt, gender_model)\n",
    "\n",
    "output_indexes = np.array([i for i in range(0, 101)]) # Set up age output range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "eQn7TDb97FQG"
   },
   "outputs": [],
   "source": [
    "#Haar cascade for face detection\n",
    "opencv_home = cv2.__file__\n",
    "folders = opencv_home.split(os.path.sep)[0:-1]\n",
    "path = folders[0]\n",
    "for folder in folders[1:]:\n",
    "    path = path + \"/\" + folder\n",
    "face_detector_path = path+\"/data/haarcascade_frontalface_default.xml\"\n",
    "if os.path.isfile(face_detector_path) != True:\n",
    "    raise ValueError(\"Confirm that opencv is installed on your environment! Expected path \",face_detector_path,\" violated.\")\n",
    "haar_detector = cv2.CascadeClassifier(face_detector_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "00Ue5JqW7_TX"
   },
   "source": [
    "## 2.3 FER: Facial expression recognition (Emotion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11852,
     "status": "ok",
     "timestamp": 1705575701477,
     "user": {
      "displayName": "Flavio Kuka",
      "userId": "09654499042520950340"
     },
     "user_tz": -60
    },
    "id": "88r6P-B44Bbr",
    "outputId": "424bd517-4096-4362-927d-3dec682185de"
   },
   "outputs": [],
   "source": [
    "!pip install fer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qmSRqlBv5y69"
   },
   "outputs": [],
   "source": [
    "from fer import FER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mCiSl7yI7aBK"
   },
   "source": [
    "## 2.4 DeepFace (Ethnicity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fetxil3rTWPw"
   },
   "source": [
    "'Deepface is a hybrid face recognition package. It currently wraps many state-of-the-art face recognition models: VGG-Face , Google FaceNet, OpenFace, Facebook DeepFace, DeepID, ArcFace, Dlib and SFace. The default configuration uses VGG-Face model.'\n",
    "- https://github.com/serengil/deepface, accessed Nov. 16th, 2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 28843,
     "status": "ok",
     "timestamp": 1705575739364,
     "user": {
      "displayName": "Flavio Kuka",
      "userId": "09654499042520950340"
     },
     "user_tz": -60
    },
    "id": "_I63mMMHTKeT",
    "outputId": "9077d9f1-0d71-48b7-9d2e-d3315c19daa2"
   },
   "outputs": [],
   "source": [
    "!pip install deepface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 362,
     "status": "ok",
     "timestamp": 1705575739722,
     "user": {
      "displayName": "Flavio Kuka",
      "userId": "09654499042520950340"
     },
     "user_tz": -60
    },
    "id": "vMfY4OwBTNp7",
    "outputId": "20d72680-b096-4281-c39f-52f8a59bea09"
   },
   "outputs": [],
   "source": [
    "from deepface import DeepFace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UPyxir_l63S6"
   },
   "source": [
    "# 3. Model Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LIBLSDV_xbfl"
   },
   "source": [
    "## 3.1 Frame Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "4hUzF19rxiqd"
   },
   "outputs": [],
   "source": [
    "def frame_extraction():\n",
    "    # inspired by https://stackoverflow.com/questions/63666638/convert-video-to-frames-in-python-1-fps, accessed Oct. 26th, 2023\n",
    "    KPS = fpsextractor['kps'] # Target Keyframes Per Second\n",
    "    VIDEO_PATH = os.path.join(dirpath, dirname, filename) # path to current video\n",
    "    FPS_OUPUT= fpsextractor['output']\n",
    "    YEAR = dirname.replace(\"ADs_IG_\", \"\")\n",
    "    YEAR_OUTPUT_DIR = FPS_OUPUT+ \"/\" + YEAR\n",
    "    OUTPUT_PATH = YEAR_OUTPUT_DIR + \"/\" + dirname + \"/\"\n",
    "    EXTENSION = \".\" + fpsextractor['extension'] # file extension of exported images\n",
    "    fileNameOfVideoWithoutExtension = filename[:-len(\".\" + fpsextractor['extension'])];\n",
    "    # print(OUTPUT_PATH) # e.g., ./outputs/fps_extractor/ADs_IG_2018/\n",
    "    #print(OUTPUT_PATH + fileNameOfVideoWithoutExtension) # e.g., ./outputs/fps_extractor/ADs_IG_2018/AD0576\n",
    "\n",
    "    # Ordner erstellen, in welchem je Video die Frames gepseichert werden\n",
    "    # Ordner mit Jahreszahl\n",
    "    if not os.path.exists(FPS_OUPUT):\n",
    "        os.mkdir(FPS_OUPUT)\n",
    "    if not os.path.exists(YEAR_OUTPUT_DIR):\n",
    "        os.mkdir(YEAR_OUTPUT_DIR)        \n",
    "    if not os.path.exists(OUTPUT_PATH):\n",
    "        os.mkdir(OUTPUT_PATH)\n",
    "    # Unterordner mit Video-ID\n",
    "    if not os.path.exists(OUTPUT_PATH + fileNameOfVideoWithoutExtension):\n",
    "        os.mkdir(OUTPUT_PATH + fileNameOfVideoWithoutExtension)\n",
    "\n",
    "    # print(KPS, IMAGE_PATH, EXTENSION)\n",
    "\n",
    "    cap = cv2.VideoCapture(VIDEO_PATH)\n",
    "    fps = round(cap.get(cv2.CAP_PROP_FPS))\n",
    "    print(fps)\n",
    "    # exit()\n",
    "    hop = round(fps / KPS)\n",
    "    curr_frame = 0\n",
    "    while(True):\n",
    "        ret, frame = cap.read()\n",
    "        if not ret: break\n",
    "        if curr_frame % hop == 0:\n",
    "            name = OUTPUT_PATH + fileNameOfVideoWithoutExtension + \"/\" + fileNameOfVideoWithoutExtension + \"_Frame_\" + str(curr_frame) + EXTENSION\n",
    "            # print(name)\n",
    "            cv2.imwrite(name, frame)\n",
    "        curr_frame += 1\n",
    "    cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PEbX1dyj7O3L"
   },
   "source": [
    "## 3.2 Predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aQBwOdN58mB6"
   },
   "source": [
    "### 3.2.1 detectron2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "yKbFl04PzS77"
   },
   "outputs": [],
   "source": [
    "# Functions for finding the quadrant_number\n",
    "def create_nine_quadrants(image):\n",
    "    height, width = image.shape[:2]\n",
    "    quadrant_width = width // 3\n",
    "    quadrant_height = height // 3\n",
    "\n",
    "    quadrants = {}\n",
    "\n",
    "    for i in range(3):\n",
    "        for j in range(3):\n",
    "            quadrant_num = i * 3 + j + 1\n",
    "            x1 = j * quadrant_width\n",
    "            y1 = i * quadrant_height\n",
    "            x2 = x1 + quadrant_width\n",
    "            y2 = y1 + quadrant_height\n",
    "            quadrants[quadrant_num] = [(x1, y1), (x2, y2)]\n",
    "\n",
    "    def get_quadrant_for_coordinate(coord):\n",
    "        x, y = coord\n",
    "        for quadrant_num, ((x1, y1), (x2, y2)) in quadrants.items():\n",
    "            if x1 <= x < x2 and y1 <= y < y2:\n",
    "                return quadrant_num\n",
    "        return None\n",
    "\n",
    "    return get_quadrant_for_coordinate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "q18ktWF_oDEa"
   },
   "outputs": [],
   "source": [
    "# Function for creating cropped image\n",
    "def crop_human_bounding_box(image, bounding_box):\n",
    "    # Get coordinates of the bounding box\n",
    "    x_min, y_min, x_max, y_max = map(int, bounding_box)\n",
    "\n",
    "    # Ensure coordinates are within the image boundaries\n",
    "    x_min = max(0, x_min)\n",
    "    y_min = max(0, y_min)\n",
    "    x_max = min(image.shape[1], x_max)\n",
    "    y_max = min(image.shape[0], y_max)\n",
    "\n",
    "    # Crop the human region from the image based on the bounding box\n",
    "    cropped_human = image[y_min:y_max, x_min:x_max]\n",
    "\n",
    "    return cropped_human"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "gi4keqCDZm63"
   },
   "outputs": [],
   "source": [
    "# Function for visualising outputs\n",
    "def detectron2_visualisation(im, outputs):\n",
    "    v = Visualizer(im[:, :, ::-1], MetadataCatalog.get(cfg.DATASETS.TRAIN[0]), scale=1.2)\n",
    "    out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
    "\n",
    "    #Change image to RGB\n",
    "    image_bgr = out.get_image()\n",
    "    image = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Generate output path for the annotated image\n",
    "    img_filename = os.path.basename(image_path)\n",
    "    output_filename = img_filename.split('.')[0] + f'_detectron_annotated.png'\n",
    "    output_path = os.path.join(visualiser_folder_path_ad, output_filename)\n",
    "\n",
    "    cv2.imwrite(output_path, image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "XONCZ2v4gM3O"
   },
   "outputs": [],
   "source": [
    "def detectron2_analysis(im):\n",
    "    outputs = predictor(im)\n",
    "    metadata = MetadataCatalog.get(cfg.DATASETS.TEST[0])\n",
    "    class_names = metadata.get(\"thing_classes\", None)\n",
    "\n",
    "    instances = outputs[\"instances\"].to(\"cpu\")\n",
    "\n",
    "    prediction_objects = []\n",
    "\n",
    "    # Iterate over objects found\n",
    "    for i in range(len(instances)):\n",
    "        bbox = instances.pred_boxes.tensor.cpu().numpy()[i].squeeze()\n",
    "\n",
    "        # Area Calculation\n",
    "        x1, y1, x2, y2 = bbox\n",
    "        area_object = (x2 - x1) * (y2 - y1)\n",
    "        height, width = im.shape[:2]\n",
    "        area_image = height * width\n",
    "        object_propotion = area_object/ area_image\n",
    "\n",
    "        # Center-Point Calculation\n",
    "        center_point = [(x1 + x2) / 2, (y1 + y2) / 2]\n",
    "        get_quadrant_for_coordinate = create_nine_quadrants(im)\n",
    "        quadrant_number = get_quadrant_for_coordinate(center_point)\n",
    "\n",
    "        # Cropping image and predicting the other attributes\n",
    "        if instances.pred_classes[i].item() == 0:\n",
    "           human_image = crop_human_bounding_box(im, bbox)\n",
    "           age_prediction, age_group_prediction, gender_prediction = dex_analysis(human_image.copy(), visualiser, i)\n",
    "           emotion_prediction = fer_analysis(human_image.copy(), visualiser, i)\n",
    "           ethnicity_prediction = deepface_analysis(human_image.copy(), visualiser, i)\n",
    "        else:\n",
    "            age_prediction = age_group_prediction = gender_prediction = emotion_prediction = ethnicity_prediction = \"-\"\n",
    "\n",
    "        # Saving prediction data\n",
    "        data = {\n",
    "            \"video_frame\": element,\n",
    "            \"class_id\": instances.pred_classes[i].item(),\n",
    "            \"class_name\": class_names[instances.pred_classes[i].item()],\n",
    "            \"confidence\": instances.scores[i].item(),\n",
    "            \"object_propotion\": object_propotion,\n",
    "            \"quadrant_number\": quadrant_number,\n",
    "            \"age_prediction\": age_prediction,\n",
    "            \"age_group_prediction\": age_group_prediction,\n",
    "            \"gender_prediction\": gender_prediction,\n",
    "            \"emotion_prediction\": emotion_prediction,\n",
    "            \"ethnicity_prediction\": ethnicity_prediction\n",
    "        }\n",
    "        prediction_objects.append(data)\n",
    "\n",
    "    if visualiser:\n",
    "      detectron2_visualisation(im, outputs)\n",
    "\n",
    "    return prediction_objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xDvzeJS4x9_I"
   },
   "source": [
    "### 3.2.2 DEX: Deep EXpectation of apparent age from a single image (Age and Gender)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "qbwAmtC6empO"
   },
   "outputs": [],
   "source": [
    "# Detecting faces\n",
    "def detect_faces(img):\n",
    "\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    faces_unsorted = haar_detector.detectMultiScale(gray, 1.3, 5)\n",
    "\n",
    "    # Sort the faces so that only the biggest bounding box is detected\n",
    "    faces = sorted(faces_unsorted, key=lambda face: face[2] * face[3], reverse=True)\n",
    "\n",
    "    return faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "1vVeZgcc1j2A"
   },
   "outputs": [],
   "source": [
    "# Visualising output\n",
    "def dex_visualisation(img, face, age, gender, i):\n",
    "    # Bounding Box\n",
    "    x, y, w, h = face\n",
    "    # Draw rectangle around detected face\n",
    "    cv2.rectangle(img, (int(x), int(y)), (int(x + w), int(y + h)), (0, 255, 0), 2)\n",
    "\n",
    "    # Annotate with age and gender information\n",
    "    text = f'Age: {age}, Gender: {gender}'\n",
    "    cv2.putText(img, text, (int(x), int(y) - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "    # Generate output path for the annotated image\n",
    "    img_filename = os.path.basename(image_path)\n",
    "    output_filename = img_filename.split('.')[0] + f'_age_gender_{i}_annotated.png'\n",
    "    output_path = os.path.join(visualiser_folder_path_ad, output_filename)\n",
    "\n",
    "    # Save the annotated image with bounding boxes and emotions\n",
    "    cv2.imwrite(output_path, img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "dINtUDGfoLy8"
   },
   "outputs": [],
   "source": [
    "def dex_analysis(img, visualiser, i):\n",
    "    # Detect faces in the image\n",
    "    faces = detect_faces(img)\n",
    "\n",
    "    if faces is not None and len(faces) > 0:\n",
    "        face = faces[0]\n",
    "        x, y, w, h = face\n",
    "\n",
    "        detected_face = img[int(y):int(y+h), int(x):int(x+w)]\n",
    "        detected_face = cv2.resize(detected_face, (224, 224))\n",
    "        img_blob = cv2.dnn.blobFromImage(detected_face)\n",
    "\n",
    "        # Age prediction\n",
    "        age_model.setInput(img_blob)\n",
    "        age_dist = age_model.forward()[0]\n",
    "        age = round(np.sum(age_dist * output_indexes))\n",
    "\n",
    "        # Age Group (based on a similar study by Chaudhari, S. J., & Kagalkar, R. M. (2015). Methodology for gender identification, classification and recognition of human age. International Journal of Computer Applications, 975, 8887.)\n",
    "        age_group = [0, 8] if 0 < age <= 8 else \\\n",
    "                    [9, 17] if 9 <= age < 18 else \\\n",
    "                    [18, 30] if 18 <= age < 31 else \\\n",
    "                    [31, 60] if 31 <= age < 61 else \\\n",
    "                    [61, 100] if 61 <= age < 100 else \\\n",
    "                    [100, 200]\n",
    "\n",
    "        # Gender prediction\n",
    "        gender_model.setInput(img_blob)\n",
    "        gender_class = gender_model.forward()[0]\n",
    "        gender = 'Woman' if np.argmax(gender_class) == 0 else 'Man'\n",
    "\n",
    "        if visualiser:\n",
    "            dex_visualisation(img, face, age, gender, i)\n",
    "    else:\n",
    "        age = age_group = gender = \"-\"\n",
    "\n",
    "    return age, age_group, gender\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gBpXPQbc57fz"
   },
   "source": [
    "### 3.2.3 FER: Facial expression recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "5wqo6HIc6Nw3"
   },
   "outputs": [],
   "source": [
    "def fer_visualisation(img, result, emotions, i):\n",
    "    # Get face bounding box coordinates\n",
    "    x, y, w, h = result['box']\n",
    "\n",
    "    # Draw a bounding box around the detected face\n",
    "    cv2.rectangle(img, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "\n",
    "    # Get the dominant emotion and its probability for the face\n",
    "    dominant_emotion = max(emotions, key=emotions.get)\n",
    "    emotion_probability = emotions[dominant_emotion]\n",
    "\n",
    "    # Annotate the image with the dominant emotion for each face\n",
    "    cv2.putText(img, f'{dominant_emotion} ({emotion_probability:.2f})',(x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 1.5, (0, 255, 0), 2)\n",
    "\n",
    "    # Generate output path for the annotated image\n",
    "    img_filename = os.path.basename(image_path)\n",
    "    output_filename = img_filename.split('.')[0] + f'_emotion_{i}_annotated.png'\n",
    "    output_path = os.path.join(visualiser_folder_path_ad, output_filename)\n",
    "\n",
    "    # Save the annotated image with bounding boxes and emotions\n",
    "    cv2.imwrite(output_path, img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "29AayRos6DyO"
   },
   "outputs": [],
   "source": [
    "def fer_analysis(img, visualiser, i):\n",
    "    # Initialize the FER model\n",
    "    detector = FER(mtcnn=True)\n",
    "\n",
    "    try:\n",
    "      # Detect faces and their emotions in the image\n",
    "      results = detector.detect_emotions(img)\n",
    "\n",
    "      # Sort the results so that only the biggest bounding box is detected\n",
    "      sorted_results = sorted(results, key=lambda x: (x['box'][2] * x['box'][3]), reverse=True)\n",
    "      result= sorted_results[0]\n",
    "\n",
    "      # Get detected emotions for the face\n",
    "      emotions = result['emotions']\n",
    "\n",
    "      # Get the dominant emotion and its probability for the face\n",
    "      dominant_emotion = max(emotions, key=emotions.get)\n",
    "\n",
    "      if visualiser:\n",
    "          fer_visualisation(img, result, emotions, i)\n",
    "\n",
    "    except:\n",
    "        dominant_emotion = \"-\"\n",
    "\n",
    "    return dominant_emotion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kku-Bcn4ThSK"
   },
   "source": [
    "### 3.2.4 DeepFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "NbFTZp8hT1RL"
   },
   "outputs": [],
   "source": [
    "def deepface_visualiser(img, prediction, i):\n",
    "\n",
    "    x, y, w, h = prediction['region']['x'], prediction['region']['y'], prediction['region']['w'], prediction['region']['h']\n",
    "\n",
    "    #draw the rectangle\n",
    "    cv2.rectangle(img, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "    #plot text on image\n",
    "    cv2.putText(img, str(prediction['dominant_race']), (int(x), int(y) - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "    # Generate output path for the annotated image\n",
    "    img_filename = os.path.basename(image_path)\n",
    "    output_filename = img_filename.split('.')[0] + f'_ethnicity_{i}_annotated.png'\n",
    "    output_path = os.path.join(visualiser_folder_path_ad, output_filename)\n",
    "\n",
    "    # Save the annotated image with bounding boxes and emotions\n",
    "    cv2.imwrite(output_path, img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "P5PGK_Z3TgWf"
   },
   "outputs": [],
   "source": [
    "def deepface_analysis(img, visualiser, i):\n",
    "\n",
    "    try:\n",
    "      predictions = DeepFace.analyze(img, actions = ['race'])\n",
    "\n",
    "      sorted_predictions = sorted(predictions, key=lambda x: x['region']['w'] * x['region']['h'], reverse=True)\n",
    "      prediction = sorted_predictions[0]\n",
    "      dominant_ethnicity = prediction['dominant_race']\n",
    "\n",
    "      if visualiser:\n",
    "          deepface_visualiser(img, prediction, i)\n",
    "\n",
    "    except:\n",
    "      dominant_ethnicity = \"-\"\n",
    "\n",
    "    return dominant_ethnicity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UniRLoRE7mFp"
   },
   "source": [
    "## 3.2 Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tNdx4mCz91Ps"
   },
   "source": [
    "### 3.2.1 Summary (1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "Fbm2qPDl_frz"
   },
   "outputs": [],
   "source": [
    "def generate_summary(input_file_path):\n",
    "    # Read the Excel file\n",
    "    data = pd.read_excel(input_file_path, sheet_name='Predictions', header=[0])\n",
    "    # Filter out the instances where objects are smaller than the schwellenwert_proportion_detectron_2\n",
    "    filtered_predictions_classes = data[data['object_propotion'] >= schwellenwert_proportion_detectron_2]\n",
    "\n",
    "    # Calculate the summary for classes\n",
    "    summary_classes_sheet = filtered_predictions_classes.groupby('class_name').size().reset_index(name='count')\n",
    "    summary_classes_sheet['avg_confidence'] = filtered_predictions_classes.groupby('class_name')['confidence'].mean().values.round(4)\n",
    "    summary_classes_sheet['avg_object_propotion'] = filtered_predictions_classes.groupby('class_name')['object_propotion'].mean().values.round(4)\n",
    "    summary_classes_sheet['avg_quadrant_number'] = filtered_predictions_classes.groupby('class_name')['quadrant_number'].mean().values.round()\n",
    "    summary_classes_sheet['frame_nr'] = filtered_predictions_classes.groupby('class_name')['video_frame'].nunique().reset_index()['video_frame']\n",
    "\n",
    "    # Calculate the frame_nr_ratio\n",
    "    total_frame_number = filtered_predictions_classes.iloc[0, 13]\n",
    "    summary_classes_sheet['frame_ratio'] = round(summary_classes_sheet['frame_nr'] / total_frame_number, 4)\n",
    "\n",
    "    # Filter rows based on the frame number\n",
    "    summary_classes_sheet = summary_classes_sheet[summary_classes_sheet['frame_nr'] >= schwellenwert_frame_nr_detectron_2]\n",
    "\n",
    "    # Sort by count\n",
    "    summary_classes_sheet = summary_classes_sheet.sort_values(by='count', ascending=False)\n",
    "\n",
    "    # Define columns to compute counts for in the original data\n",
    "    columns_to_count = ['super-category', 'gender_prediction', 'ethnicity_prediction', 'age_group_prediction', 'emotion_prediction']\n",
    "\n",
    "    data.columns = [re.sub(r'Unnamed:\\s*\\d*', '', col) if 'Unnamed' in str(col) else col for col in data.columns]\n",
    "\n",
    "    # Write to Excel\n",
    "    with pd.ExcelWriter(input_file_path, engine='openpyxl') as writer:\n",
    "        # Saving the excel files\n",
    "        data.to_excel(writer, sheet_name='Predictions', index=False, index_label=None)\n",
    "        summary_classes_sheet.to_excel(writer, sheet_name='Summary_Objects', index=False)\n",
    "\n",
    "        # Update the summary attributes sheet\n",
    "        for column in columns_to_count:\n",
    "\n",
    "            # Filter out the empty instances\n",
    "            non_empty_predictions = data[data[column] != \"-\"]\n",
    "\n",
    "            # Filter out the instances where objects are smaller than the schwellenwert_proportion_detectron_2 or schwellenwert_proportion_human_attributes\n",
    "            if column == 'super-category':\n",
    "              filtered_predictions_human = non_empty_predictions[non_empty_predictions['object_propotion'] >= schwellenwert_proportion_detectron_2]\n",
    "            else:\n",
    "              filtered_predictions_human = non_empty_predictions[non_empty_predictions['object_propotion'] >= schwellenwert_proportion_human_attributes]\n",
    "\n",
    "            # Calculate the summary for human attributes\n",
    "            summary_attributes_sheet = filtered_predictions_human.groupby(column).size().reset_index(name='count_' + column)\n",
    "            summary_attributes_sheet[f'avg_object_propotion_{column}'] = filtered_predictions_human.groupby(column)['object_propotion'].mean().values.round(4)\n",
    "            summary_attributes_sheet[f'avg_quadrant_number_{column}'] = filtered_predictions_human.groupby(column)['quadrant_number'].mean().values.round()\n",
    "            summary_attributes_sheet[f'frame_nr_{column}'] = filtered_predictions_human.groupby(column)['video_frame'].nunique().reset_index()['video_frame']\n",
    "            summary_attributes_sheet[f'frame_ratio_{column}'] = round(summary_attributes_sheet[f'frame_nr_{column}'] / total_frame_number, 4)\n",
    "\n",
    "            # Filter out the instances where the number of unique frames are smaller than the schwellenwert_frame_nr_detectron_2 or schwellenwert_frame_nr_human_attributes\n",
    "            if column == \"super-category\":\n",
    "              summary_attributes_sheet = summary_attributes_sheet[summary_attributes_sheet[f'frame_nr_{column}'] >= schwellenwert_frame_nr_detectron_2]\n",
    "            else:\n",
    "              summary_attributes_sheet = summary_attributes_sheet[summary_attributes_sheet[f'frame_nr_{column}'] >= schwellenwert_frame_nr_human_attributes]\n",
    "\n",
    "            summary_attributes_sheet.sort_values(by='count_' + column, ascending=False, inplace=True)\n",
    "            summary_attributes_sheet.to_excel(writer, sheet_name='Summary_Objects', startrow=0, startcol=(columns_to_count.index(column) * 7) + 8, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2X2gvOv01tX6"
   },
   "source": [
    "### 3.2.2 Summary Gender & Ethnicity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "ofR3T7MY178B"
   },
   "outputs": [],
   "source": [
    "def generate_summary_gender_ethnicity(input_file_path):\n",
    "\n",
    "    data = pd.read_excel(input_file_path, sheet_name = 'Predictions')\n",
    "    summary_1 = pd.read_excel(input_file_path, sheet_name='Summary_Objects')\n",
    "\n",
    "    ########################################################################### Gender ##############################################################################################\n",
    "    data_gender = data[data['gender_prediction'] != '-']\n",
    "\n",
    "    # Filter out the instances where objects are smaller than the schwellenwert_proportion_human_attributes\n",
    "    data_gender = data_gender[data_gender['object_propotion'] >= schwellenwert_proportion_human_attributes]\n",
    "\n",
    "    # Filter out the instances where the number of unique frames are smaller than the schwellenwert_frame_nr_human_attributes\n",
    "    unique_genders = data_gender['gender_prediction'].unique()\n",
    "    genders_to_remove = []\n",
    "\n",
    "    for gender in unique_genders:\n",
    "        filtered_df = data_gender[data_gender['gender_prediction'] == gender]\n",
    "        unique_video_frames = filtered_df['video_frame'].unique().tolist()\n",
    "\n",
    "        if len(unique_video_frames) < schwellenwert_frame_nr_human_attributes:\n",
    "            genders_to_remove.append(gender)\n",
    "\n",
    "    data_gender = data_gender[~data_gender['gender_prediction'].isin(genders_to_remove)]\n",
    "\n",
    "    #New sheet\n",
    "    Gender_sheet = pd.DataFrame(columns=['case', 'count', 'frame_ratio',\n",
    "                                        'avg_object_propotion_women', 'avg_quadrant_number_women',\n",
    "                                        'quadrant_numbers_women',\n",
    "                                        'avg_object_propotion_men', 'avg_quadrant_number_men',\n",
    "                                        'quadrant_numbers_men'\n",
    "                                        ])\n",
    "    case_data_gender = {}\n",
    "\n",
    "    # Iterate through the data to determine cases and calculate averages\n",
    "    for frame_key, frame_data in data_gender.groupby('video_frame'):\n",
    "        # Count the occurrences of each gender in the frame\n",
    "        gender_counts = frame_data['gender_prediction'].value_counts()\n",
    "\n",
    "        # Create the case string dynamically based on the gender counts\n",
    "        case = ' & '.join([f'{count} {gender}' for gender, count in sorted(gender_counts.items())])\n",
    "\n",
    "        # Check if the case already exists in the case_data dictionary\n",
    "        if case not in case_data_gender:\n",
    "            case_data_gender[case] = {'count': 0,\n",
    "                              'total_object_propotion_women': 0, 'total_quadrant_number_women': 0,\n",
    "                              'quadrant_numbers_women': [],\n",
    "                              'total_object_propotion_men': 0, 'total_quadrant_number_men': 0,\n",
    "                              'quadrant_numbers_men': []\n",
    "                              }\n",
    "\n",
    "        # Update cumulative values for the case\n",
    "        case_data_gender[case]['count'] += 1\n",
    "        case_data_gender[case]['total_object_propotion_women'] += frame_data[frame_data['gender_prediction'] == 'Woman']['object_propotion'].mean()\n",
    "        case_data_gender[case]['total_quadrant_number_women'] += frame_data[frame_data['gender_prediction'] == 'Woman']['quadrant_number'].mean()\n",
    "        case_data_gender[case]['quadrant_numbers_women'].extend(frame_data[frame_data['gender_prediction'] == 'Woman']['quadrant_number'].tolist())\n",
    "\n",
    "        case_data_gender[case]['total_object_propotion_men'] += frame_data[frame_data['gender_prediction'] == 'Man']['object_propotion'].mean()\n",
    "        case_data_gender[case]['total_quadrant_number_men'] += frame_data[frame_data['gender_prediction'] == 'Man']['quadrant_number'].mean()\n",
    "        case_data_gender[case]['quadrant_numbers_men'].extend(frame_data[frame_data['gender_prediction'] == 'Man']['quadrant_number'].tolist())\n",
    "\n",
    "    # Calculate averages for each case\n",
    "    for case, values in case_data_gender.items():\n",
    "        count = values['count']\n",
    "\n",
    "        # Calculate averages for women\n",
    "        avg_object_propotion_women = round(values['total_object_propotion_women'] / count, 4) if count > 0 else 0\n",
    "        avg_quadrant_number_women = round(values['total_quadrant_number_women'] / count) if count > 0 and not np.isnan(values['total_quadrant_number_women']) else 0\n",
    "\n",
    "        # Calculate averages for men\n",
    "        avg_object_propotion_men = round(values['total_object_propotion_men'] / count, 4) if count > 0 else 0\n",
    "        avg_quadrant_number_men = round(values['total_quadrant_number_men'] / count) if count > 0 and not np.isnan(values['total_quadrant_number_men']) else 0\n",
    "\n",
    "        # Calculate frame ratio\n",
    "        total_frame_number = data.iloc[0, 13]\n",
    "        frame_ratio = round(count/ total_frame_number, 4)\n",
    "\n",
    "        # Append the values to the Gender_sheet\n",
    "    new_row = pd.DataFrame([{\n",
    "        'case': case,\n",
    "        'count': count,\n",
    "        'frame_ratio': frame_ratio,\n",
    "        'avg_object_propotion_women': avg_object_propotion_women,\n",
    "        'avg_quadrant_number_women': avg_quadrant_number_women,\n",
    "        'quadrant_numbers_women': values['quadrant_numbers_women'],\n",
    "        'avg_object_propotion_men': avg_object_propotion_men,\n",
    "        'avg_quadrant_number_men': avg_quadrant_number_men,\n",
    "        'quadrant_numbers_men': values['quadrant_numbers_men']\n",
    "    }])\n",
    "\n",
    "    # Concatenate the new row to the existing DataFrame\n",
    "    Gender_sheet = pd.concat([Gender_sheet, new_row], ignore_index=True)\n",
    "\n",
    "    Gender_sheet = Gender_sheet.sort_values(by='count', ascending=False)\n",
    "\n",
    "    # Replace empty spaces with 0 in the entire dataframe\n",
    "    Gender_sheet[['avg_object_propotion_women', 'avg_object_propotion_men']] = Gender_sheet[['avg_object_propotion_women', 'avg_object_propotion_men']].fillna(0)\n",
    "\n",
    "\n",
    "    ########################################################################### Ethnicity #########################################################################################\n",
    "    data_ethnicity = data[data['ethnicity_prediction'] != '-']\n",
    "\n",
    "    # Filter out the instances where objects are smaller than the schwellenwert_proportion_human_attributes\n",
    "    data_ethnicity = data_ethnicity[data_ethnicity['object_propotion'] >= schwellenwert_proportion_human_attributes]\n",
    "\n",
    "    # Filter out the instances where the number of unique frames are smaller than the schwellenwert_frame_nr_human_attributes\n",
    "    unique_ethnicities = data_ethnicity['ethnicity_prediction'].unique()\n",
    "    ethnicity_to_remove = []\n",
    "\n",
    "    for ethnicity in unique_ethnicities:\n",
    "        filtered_df = data_ethnicity[data_ethnicity['ethnicity_prediction'] == ethnicity]\n",
    "        unique_video_frames = filtered_df['video_frame'].unique().tolist()\n",
    "\n",
    "        if len(unique_video_frames) < schwellenwert_frame_nr_human_attributes:\n",
    "            ethnicity_to_remove.append(ethnicity)\n",
    "\n",
    "    data_ethnicity = data_ethnicity[~data_ethnicity['ethnicity_prediction'].isin(ethnicity_to_remove)]\n",
    "\n",
    "    # New sheet\n",
    "    Ethnicity_sheet = pd.DataFrame(columns=['case', 'count', 'frame_ratio',\n",
    "                                            'avg_object_propotion_asian', 'avg_quadrant_number_asian',\n",
    "                                            'quadrant_numbers_asian',\n",
    "                                            'avg_object_propotion_black', 'avg_quadrant_number_black',\n",
    "                                            'quadrant_numbers_black',\n",
    "                                            'avg_object_propotion_indian', 'avg_quadrant_number_indian',\n",
    "                                            'quadrant_numbers_indian',\n",
    "                                            'avg_object_propotion_latino_hispanic', 'avg_quadrant_number_latino_hispanic',\n",
    "                                            'quadrant_numbers_latino_hispanic',\n",
    "                                            'avg_object_propotion_middle_eastern', 'avg_quadrant_number_middle_eastern',\n",
    "                                            'quadrant_numbers_middle_eastern',\n",
    "                                            'avg_object_propotion_white', 'avg_quadrant_number_white',\n",
    "                                            'quadrant_numbers_white'\n",
    "                                            ])\n",
    "\n",
    "    case_data_ethnicity = {}\n",
    "\n",
    "    # Iterate through the data to determine cases and calculate averages\n",
    "    for frame_key, frame_data in data_ethnicity.groupby('video_frame'):\n",
    "        # Count the occurrences of each ethnicity in the frame\n",
    "        ethnicity_counts = frame_data['ethnicity_prediction'].value_counts()\n",
    "\n",
    "        # Create the case string dynamically based on the ethnicity counts\n",
    "        case = ' & '.join([f'{count} {ethnicity}' for ethnicity, count in sorted(ethnicity_counts.items())])\n",
    "\n",
    "        # Check if the case already exists in the case_data dictionary\n",
    "        if case not in case_data_ethnicity:\n",
    "            case_data_ethnicity[case] = {'count': 0,\n",
    "                              'total_object_propotion_asian': 0, 'total_quadrant_number_asian': 0,\n",
    "                              'quadrant_numbers_asian': [],\n",
    "                              'total_object_propotion_black': 0, 'total_quadrant_number_black': 0,\n",
    "                              'quadrant_numbers_black': [],\n",
    "                              'total_object_propotion_indian': 0, 'total_quadrant_number_indian': 0,\n",
    "                              'quadrant_numbers_indian': [],\n",
    "                              'total_object_propotion_latino_hispanic': 0, 'total_quadrant_number_latino_hispanic': 0,\n",
    "                              'quadrant_numbers_latino_hispanic': [],\n",
    "                              'total_object_propotion_middle_eastern': 0, 'total_quadrant_number_middle_eastern': 0,\n",
    "                              'quadrant_numbers_middle_eastern': [],\n",
    "                              'total_object_propotion_white': 0, 'total_quadrant_number_white': 0,\n",
    "                              'quadrant_numbers_white': []\n",
    "                              }\n",
    "\n",
    "        # Update cumulative values for the case\n",
    "        case_data_ethnicity[case]['count'] += 1\n",
    "        case_data_ethnicity[case]['total_object_propotion_asian'] += frame_data[frame_data['ethnicity_prediction'] == 'asian']['object_propotion'].mean()\n",
    "        case_data_ethnicity[case]['total_quadrant_number_asian'] += frame_data[frame_data['ethnicity_prediction'] == 'asian']['quadrant_number'].mean()\n",
    "        case_data_ethnicity[case]['quadrant_numbers_asian'].extend(frame_data[frame_data['ethnicity_prediction'] == 'asian']['quadrant_number'].tolist())\n",
    "\n",
    "        case_data_ethnicity[case]['total_object_propotion_black'] += frame_data[frame_data['ethnicity_prediction'] == 'black']['object_propotion'].mean()\n",
    "        case_data_ethnicity[case]['total_quadrant_number_black'] += frame_data[frame_data['ethnicity_prediction'] == 'black']['quadrant_number'].mean()\n",
    "        case_data_ethnicity[case]['quadrant_numbers_black'].extend(frame_data[frame_data['ethnicity_prediction'] == 'black']['quadrant_number'].tolist())\n",
    "\n",
    "        case_data_ethnicity[case]['total_object_propotion_indian'] += frame_data[frame_data['ethnicity_prediction'] == 'indian']['object_propotion'].mean()\n",
    "        case_data_ethnicity[case]['total_quadrant_number_indian'] += frame_data[frame_data['ethnicity_prediction'] == 'indian']['quadrant_number'].mean()\n",
    "        case_data_ethnicity[case]['quadrant_numbers_indian'].extend(frame_data[frame_data['ethnicity_prediction'] == 'indian']['quadrant_number'].tolist())\n",
    "\n",
    "        case_data_ethnicity[case]['total_object_propotion_latino_hispanic'] += frame_data[frame_data['ethnicity_prediction'] == 'latino hispanic']['object_propotion'].mean()\n",
    "        case_data_ethnicity[case]['total_quadrant_number_latino_hispanic'] += frame_data[frame_data['ethnicity_prediction'] == 'latino hispanic']['quadrant_number'].mean()\n",
    "        case_data_ethnicity[case]['quadrant_numbers_latino_hispanic'].extend(frame_data[frame_data['ethnicity_prediction'] == 'latino hispanic']['quadrant_number'].tolist())\n",
    "\n",
    "        case_data_ethnicity[case]['total_object_propotion_middle_eastern'] += frame_data[frame_data['ethnicity_prediction'] == 'middle eastern']['object_propotion'].mean()\n",
    "        case_data_ethnicity[case]['total_quadrant_number_middle_eastern'] += frame_data[frame_data['ethnicity_prediction'] == 'middle eastern']['quadrant_number'].mean()\n",
    "        case_data_ethnicity[case]['quadrant_numbers_middle_eastern'].extend(frame_data[frame_data['ethnicity_prediction'] == 'middle eastern']['quadrant_number'].tolist())\n",
    "\n",
    "        case_data_ethnicity[case]['total_object_propotion_white'] += frame_data[frame_data['ethnicity_prediction'] == 'white']['object_propotion'].mean()\n",
    "        case_data_ethnicity[case]['total_quadrant_number_white'] += frame_data[frame_data['ethnicity_prediction'] == 'white']['quadrant_number'].mean()\n",
    "        case_data_ethnicity[case]['quadrant_numbers_white'].extend(frame_data[frame_data['ethnicity_prediction'] == 'white']['quadrant_number'].tolist())\n",
    "\n",
    "    # Calculate averages for each case\n",
    "    for case, values in case_data_ethnicity.items():\n",
    "        count = values['count']\n",
    "\n",
    "        # Calculate averages for asian\n",
    "        avg_object_propotion_asian = round(values['total_object_propotion_asian'] / count, 4) if count > 0 else 0\n",
    "        avg_quadrant_number_asian = round(values['total_quadrant_number_asian'] / count) if count > 0 and not np.isnan(values['total_quadrant_number_asian']) else 0\n",
    "\n",
    "        # Calculate averages for black\n",
    "        avg_object_propotion_black = round(values['total_object_propotion_black'] / count, 4) if count > 0 else 0\n",
    "        avg_quadrant_number_black = round(values['total_quadrant_number_black'] / count) if count > 0 and not np.isnan(values['total_quadrant_number_black']) else 0\n",
    "\n",
    "        # Calculate averages for indian\n",
    "        avg_object_propotion_indian = round(values['total_object_propotion_indian'] / count, 4) if count > 0 else 0\n",
    "        avg_quadrant_number_indian = round(values['total_quadrant_number_indian'] / count) if count > 0 and not np.isnan(values['total_quadrant_number_indian']) else 0\n",
    "\n",
    "        # Calculate averages for latino_hispanic\n",
    "        avg_object_propotion_latino_hispanic = round(values['total_object_propotion_latino_hispanic'] / count, 4) if count > 0 else 0\n",
    "        avg_quadrant_number_latino_hispanic = round(values['total_quadrant_number_latino_hispanic'] / count) if count > 0 and not np.isnan(values['total_quadrant_number_latino_hispanic']) else 0\n",
    "\n",
    "\n",
    "        # Calculate averages for middle_eastern\n",
    "        avg_object_propotion_middle_eastern = round(values['total_object_propotion_middle_eastern'] / count, 4) if count > 0 else 0\n",
    "        avg_quadrant_number_middle_eastern = round(values['total_quadrant_number_middle_eastern'] / count) if count > 0 and not np.isnan(values['total_quadrant_number_middle_eastern']) else 0\n",
    "\n",
    "        # Calculate averages for white\n",
    "        avg_object_propotion_white = round(values['total_object_propotion_white'] / count, 4) if count > 0 else 0\n",
    "        avg_quadrant_number_white = round(values['total_quadrant_number_white'] / count) if count > 0 and not np.isnan(values['total_quadrant_number_white']) else 0\n",
    "\n",
    "        # Calculate frame ratio\n",
    "        total_frame_number = data.iloc[0, 13]\n",
    "        frame_ratio = round(count/ total_frame_number, 4)\n",
    "        # Append the values to the Ethnicity_sheet\n",
    "        new_row = pd.DataFrame([{\n",
    "        'case': [case],\n",
    "        'count': [count],\n",
    "        'frame_ratio': [frame_ratio],\n",
    "        'avg_object_propotion_asian': [avg_object_propotion_asian],\n",
    "        'avg_quadrant_number_asian': [avg_quadrant_number_asian],\n",
    "        'quadrant_numbers_asian': [values['quadrant_numbers_asian']],\n",
    "        'avg_object_propotion_black': [avg_object_propotion_black],\n",
    "        'avg_quadrant_number_black': [avg_quadrant_number_black],\n",
    "        'quadrant_numbers_black': [values['quadrant_numbers_black']],\n",
    "        'avg_object_propotion_indian': [avg_object_propotion_indian],\n",
    "        'avg_quadrant_number_indian': [avg_quadrant_number_indian],\n",
    "        'quadrant_numbers_indian': [values['quadrant_numbers_indian']],\n",
    "        'avg_object_propotion_latino_hispanic': [avg_object_propotion_latino_hispanic],\n",
    "        'avg_quadrant_number_latino_hispanic': [avg_quadrant_number_latino_hispanic],\n",
    "        'quadrant_numbers_latino_hispanic': [values['quadrant_numbers_latino_hispanic']],\n",
    "        'avg_object_propotion_middle_eastern': [avg_object_propotion_middle_eastern],\n",
    "        'avg_quadrant_number_middle_eastern': [avg_quadrant_number_middle_eastern],\n",
    "        'quadrant_numbers_middle_eastern': [values['quadrant_numbers_middle_eastern']],\n",
    "        'avg_object_propotion_white': [avg_object_propotion_white],\n",
    "        'avg_quadrant_number_white': [avg_quadrant_number_white],\n",
    "        'quadrant_numbers_white': [values['quadrant_numbers_white']],\n",
    "        }])\n",
    "\n",
    "    # Concatenate the new row to the existing DataFrame\n",
    "    Gender_sheet = pd.concat([Ethnicity_sheet, new_row], ignore_index=True)\n",
    "\n",
    "    # Replace empty spaces with 0 in the entire dataframe\n",
    "    Ethnicity_sheet[['avg_object_propotion_asian', 'avg_object_propotion_black', 'avg_object_propotion_indian',\n",
    "                    'avg_object_propotion_latino_hispanic', 'avg_object_propotion_middle_eastern', 'avg_object_propotion_white']] = \\\n",
    "        Ethnicity_sheet[['avg_object_propotion_asian', 'avg_object_propotion_black', 'avg_object_propotion_indian',\n",
    "                        'avg_object_propotion_latino_hispanic', 'avg_object_propotion_middle_eastern', 'avg_object_propotion_white']].fillna(0)\n",
    "\n",
    "    Ethnicity_sheet = Ethnicity_sheet.sort_values(by='count', ascending=False)\n",
    "\n",
    "    data.columns = [re.sub(r'Unnamed:\\s*\\d*', '', col) if 'Unnamed' in str(col) else col for col in data.columns]\n",
    "    summary_1.columns = [re.sub(r'Unnamed:\\s*\\d*', '', col) if 'Unnamed' in str(col) else col for col in summary_1.columns]\n",
    "\n",
    "    with pd.ExcelWriter(input_file_path, engine='openpyxl') as writer:\n",
    "        data.to_excel(writer, sheet_name='Predictions', index=False, index_label=None)\n",
    "        summary_1.to_excel(writer, sheet_name='Summary_Objects', index= False, index_label=None)\n",
    "        Gender_sheet.to_excel(writer, sheet_name='Summary_Gender_Ethnicity', index=False)\n",
    "        Ethnicity_sheet.to_excel(writer, sheet_name='Summary_Gender_Ethnicity', startrow=0, startcol=10, index=False)\n",
    "    print(\"debug\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rQ4WyELY0LGb"
   },
   "source": [
    "# 4. Main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zRBXng24xobY"
   },
   "source": [
    "## 4.1 Extracting the Frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "zViBOO78O28Y"
   },
   "outputs": [],
   "source": [
    "# settings\n",
    "fpsextractor = {\n",
    "    \"format\": r\"mp4\", # Video Format of ads within sources.ads folder.\n",
    "    \"yearsToEvaluate\": [2013,2014,2015,2016,2017,2018,2019,2020,2021,2022], # A list of the years which will be analysed.\n",
    "\n",
    "    # fot OpenCV\n",
    "    \"kps\": 3, # Target Keyframes Per Second\n",
    "    \"input\": os.getenv(\"ADS_DIR\"), # Path to the downloaded Ads. The folder this path belongs to must contain subfolgers like \"ADs_IG_2013\" to divide the videos.\n",
    "    \"output\": os.getenv(\"INPUT_FRAMES_ALL\"), # Output path where to store exported images\n",
    "    \"extension\": r\"png\", # File Extension for the exported images\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LjiFUQKXxsy2"
   },
   "outputs": [],
   "source": [
    "# Liste der auszuwertenden Jahre aus conf laden und zu Strings umwanzeln.\n",
    "yearsAsString = map(str, fpsextractor['yearsToEvaluate'])\n",
    "yearsAsString = list(yearsAsString)\n",
    "print(\"yearsasstring\",yearsAsString)\n",
    "print(\"fpsextractorinput\", fpsextractor['input'])\n",
    "# Durch Werbevideos iterieren und nur die Jahre auswerten, die in conf hinterlegt sind.\n",
    "for dirpath, dirnames, filenames in os.walk(fpsextractor['input']):\n",
    "    for dirname in dirnames:\n",
    "        print (\"dirname\", dirname)\n",
    "        # wenn aktuell betrachteter Ordner (Jahr) in Config hinterlegt ist, dann auswerten\n",
    "        if (dirname[-4:] in yearsAsString):\n",
    "            print('# ', dirname[-4:], dirname) # print year and folder name\n",
    "\n",
    "            for filename in os.listdir(os.path.join(dirpath, dirname)):\n",
    "                # if filename.endswith('.' + cfg.fpsextractor['format']):\n",
    "                    # print(os.path.join(dirpath, dirname, filename))\n",
    "                    print(filename)\n",
    "                    frame_extraction()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ttUh9YPU-jXC"
   },
   "source": [
    "## 4.2 Creating the output files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "qJbfYC4lHslq"
   },
   "outputs": [],
   "source": [
    "# Path to the folder containing the frames and the excel lists\n",
    "input_folder_path = os.getenv(\"INPUT_FRAMES_ALL\")\n",
    "# output_folder_path = '/content/drive/MyDrive/SuperBowl_Project_FUB/output_lists'\n",
    "output_folder_path = os.getenv(\"OUTPUT_BILD_PLUS_TON_LISTS_DIR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "xS-hNn3wLVPx"
   },
   "outputs": [],
   "source": [
    "# If you want to save the output images then set visualiser to 1\n",
    "visualiser = 0\n",
    "if visualiser:\n",
    "    visualiser_folder_path = os.path.join(output_folder_path, \"visualiser\")\n",
    "    os.makedirs(visualiser_folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_egy94b8EwJs"
   },
   "outputs": [],
   "source": [
    "# OPTIONAL: So that the loops starts with the years in an alphabetical order\n",
    "years = []\n",
    "for year in os.listdir(input_folder_path):\n",
    "  print(\"year detected\", year)\n",
    "  years.append(year)\n",
    "years.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fng7BioEqZjX"
   },
   "outputs": [],
   "source": [
    "# Creating the output folders\n",
    "for year in years:\n",
    "    output_year_folder_path = os.path.join(output_folder_path, \"ADs_IG_\"+year)\n",
    "    print(\"output_year_folder_path created\", output_year_folder_path)\n",
    "    os.makedirs(output_year_folder_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iDMlEiWX_0uA"
   },
   "outputs": [],
   "source": [
    "# Creating the output excel files\n",
    "for year in years:\n",
    "    # Save the ad folders in a list\n",
    "    ad_folder_path = os.path.join(input_folder_path, year, \"ADs_IG_\" + year)\n",
    "    print(\"ad_folder_path \", ad_folder_path)\n",
    "    all_ADs = os.listdir(ad_folder_path)\n",
    "    AD_names = [item for item in all_ADs if os.path.isdir(os.path.join(ad_folder_path, item))]\n",
    "    sorted_AD_names = sorted(AD_names)\n",
    "\n",
    "    # Run the code for each ad\n",
    "    while len(sorted_AD_names) > 0:\n",
    "        all_predictions = []\n",
    "        all_frames = []\n",
    "        current_AD = sorted_AD_names.pop(0)\n",
    "        active_ad_folder_path = os.path.join(ad_folder_path, current_AD)\n",
    "\n",
    "        if visualiser:\n",
    "            visualiser_folder_path_ad = os.path.join(visualiser_folder_path, current_AD)\n",
    "            os.makedirs(visualiser_folder_path_ad)\n",
    "\n",
    "        for frame in os.listdir(active_ad_folder_path):\n",
    "            if frame.endswith(\".png\"):\n",
    "                all_frames.append(frame)\n",
    "\n",
    "        for element in tqdm(all_frames, desc=f\"Processing {current_AD}\", unit=\"element\"):\n",
    "            image_path = os.path.join(active_ad_folder_path, element)\n",
    "            im = cv2.imread(image_path)\n",
    "\n",
    "            prediction_objects = detectron2_analysis(im)\n",
    "            all_predictions.extend(prediction_objects)\n",
    "\n",
    "        # Extending all_predictions with super-categories used for the setting\n",
    "        # NOTE: The reason why we have used \"try and except\" is because for AD0347 and AD0754 Detectron2 wasn't able to detect any object since only text and a QR-Code was shown in the ad.\n",
    "        all_predictions = pd.DataFrame(all_predictions)\n",
    "        try:\n",
    "          all_predictions_super_categories = all_predictions.join(super_categories_df, on=\"class_name\")\n",
    "        except:\n",
    "          all_predictions_super_categories = all_predictions\n",
    "\n",
    "        # Calculate total frame number\n",
    "        total_frame_number = len([file for file in os.listdir(active_ad_folder_path) if file.lower().endswith('.png')])\n",
    "\n",
    "        # To Excel\n",
    "        output_year_folder_path = os.path.join(output_folder_path,\"ADs_IG_\"+year)\n",
    "        AD_name = current_AD + \".xlsx\"\n",
    "        output_file = os.path.join(output_year_folder_path, AD_name)\n",
    "        with pd.ExcelWriter(output_file, engine='openpyxl') as writer:\n",
    "            all_predictions_super_categories.to_excel(writer, sheet_name='Predictions', index=False)\n",
    "\n",
    "            # Add total frame number to the Excel sheet\n",
    "            sheet = writer.sheets['Predictions']\n",
    "            sheet[f'N1'] = 'Total Frame Number'\n",
    "            sheet[f'N2'] = total_frame_number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZxW9ZWIG-oVB"
   },
   "source": [
    "## 4.3 Creating the summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 923371,
     "status": "ok",
     "timestamp": 1706472490607,
     "user": {
      "displayName": "Flavio Kuka",
      "userId": "09654499042520950340"
     },
     "user_tz": -60
    },
    "id": "_2oBeL_A-wlg",
    "outputId": "93833656-d0d1-47cc-caf4-3825e7fa8dd0"
   },
   "outputs": [],
   "source": [
    "# NOTE: The Threshold values were determined manually by checking the predictions for the frame_count = [3, 6, 9, 12, 15] and proportion = [5%, 10%, 15%]\n",
    "\n",
    "schwellenwert_frame_nr_human_attributes = 9\n",
    "schwellenwert_frame_nr_detectron_2 = 6\n",
    "schwellenwert_proportion_human_attributes = 0.05\n",
    "schwellenwert_proportion_detectron_2 = 0\n",
    "\n",
    "for year in os.listdir(output_folder_path):\n",
    "  print(\"Output folder path\", output_folder_path)\n",
    "  print(f\"Processing year {year}\")\n",
    "  ad_folder_path = os.path.join(output_folder_path, year)\n",
    "  print(\"AD folder path\", ad_folder_path)\n",
    "  for ad in os.listdir(ad_folder_path):\n",
    "      if ad.endswith(\".xlsx\"):\n",
    "          input_file_path = os.path.join(ad_folder_path, ad)\n",
    "          try:\n",
    "            generate_summary(input_file_path)\n",
    "          except:\n",
    "            # print(f\"Summary for ad {ad} couldn't get created\")\n",
    "            continue\n",
    "          try:\n",
    "              generate_summary_gender_ethnicity(input_file_path)\n",
    "          except Exception as e:\n",
    "              print(f\"Gender and Ethnicity Summary for ad {ad} couldn't get created: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "1EqcmIwFuXCZ",
    "ZJtISBlJ8X7N",
    "hpjg6z9J6lEw",
    "LIBLSDV_xbfl",
    "UniRLoRE7mFp",
    "zRBXng24xobY",
    "ttUh9YPU-jXC"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
