{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1EqcmIwFuXCZ"
   },
   "source": [
    "#**Super-Bowl Bildanalyse**\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZJtISBlJ8X7N"
   },
   "source": [
    "# 1. Set-Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "J9cFp7ESh9ng"
   },
   "outputs": [],
   "source": [
    "# import the neccessary libraries\n",
    "import os, json, cv2, random\n",
    "import numpy as np\n",
    "# from google.colab import drive\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from openpyxl.styles import Font\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 25363,
     "status": "ok",
     "timestamp": 1709720758003,
     "user": {
      "displayName": "Flavio Kuka",
      "userId": "09654499042520950340"
     },
     "user_tz": -60
    },
    "id": "z0XRxb7qnInb",
    "outputId": "6e962bf7-e365-4002-d500-e8171816a591"
   },
   "outputs": [],
   "source": [
    "# # OPTIONAL\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "uSca70_qzVTV"
   },
   "outputs": [],
   "source": [
    "# Load super-categories DataFrame\n",
    "# NOTE: We couldn't find the super-categories in the COCO dataset and therefore we wrote them by hand in an excel file\n",
    "super_categories_df = pd.read_excel(f'{os.getenv(\"BILDANALYSE_MODELS_COCO_DIR\")}/super-categories.xlsx')\n",
    "super_categories_df.set_index(\"class_name\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hpjg6z9J6lEw"
   },
   "source": [
    "# 2. Installation of the pre-trained models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VKD0QmuS6_uj"
   },
   "source": [
    "## 2.1 detectron2 (Object Detection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 25702,
     "status": "ok",
     "timestamp": 1705575649184,
     "user": {
      "displayName": "Flavio Kuka",
      "userId": "09654499042520950340"
     },
     "user_tz": -60
    },
    "id": "a_lPuyTT7H6v",
    "outputId": "c859e994-97f7-4d78-ff0a-520a044c251c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyyaml==5.1 in /home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/venv/lib/python3.12/site-packages (5.1)\n",
      "fatal: destination path 'detectron2' already exists and is not an empty directory.\n",
      "Ignoring dataclasses: markers 'python_version < \"3.7\"' don't match your environment\n",
      "Requirement already satisfied: Pillow>=7.1 in /home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/venv/lib/python3.12/site-packages (10.2.0)\n",
      "Requirement already satisfied: matplotlib in /home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/venv/lib/python3.12/site-packages (3.9.2)\n",
      "Requirement already satisfied: pycocotools>=2.0.2 in /home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/venv/lib/python3.12/site-packages (2.0.8)\n",
      "Requirement already satisfied: termcolor>=1.1 in /home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/venv/lib/python3.12/site-packages (2.5.0)\n",
      "Requirement already satisfied: yacs>=0.1.8 in /home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/venv/lib/python3.12/site-packages (0.1.8)\n",
      "Requirement already satisfied: tabulate in /home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/venv/lib/python3.12/site-packages (0.9.0)\n",
      "Requirement already satisfied: cloudpickle in /home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/venv/lib/python3.12/site-packages (3.1.0)\n",
      "Requirement already satisfied: tqdm>4.29.0 in /home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/venv/lib/python3.12/site-packages (4.67.0)\n",
      "Requirement already satisfied: tensorboard in /home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/venv/lib/python3.12/site-packages (2.18.0)\n",
      "Requirement already satisfied: fvcore<0.1.6,>=0.1.5 in /home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/venv/lib/python3.12/site-packages (0.1.5.post20221221)\n",
      "Requirement already satisfied: iopath<0.1.10,>=0.1.7 in /home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/venv/lib/python3.12/site-packages (0.1.9)\n",
      "Requirement already satisfied: omegaconf<2.4,>=2.1 in /home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/venv/lib/python3.12/site-packages (2.3.0)\n",
      "Requirement already satisfied: hydra-core>=1.1 in /home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/venv/lib/python3.12/site-packages (1.3.2)\n",
      "Requirement already satisfied: black in /home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/venv/lib/python3.12/site-packages (24.10.0)\n",
      "Requirement already satisfied: packaging in /home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/venv/lib/python3.12/site-packages (24.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/venv/lib/python3.12/site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/venv/lib/python3.12/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/venv/lib/python3.12/site-packages (from matplotlib) (4.54.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/venv/lib/python3.12/site-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: numpy>=1.23 in /home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/venv/lib/python3.12/site-packages (from matplotlib) (1.26.4)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/venv/lib/python3.12/site-packages (from matplotlib) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/venv/lib/python3.12/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: PyYAML in /home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/venv/lib/python3.12/site-packages (from yacs>=0.1.8) (5.1)\n",
      "Requirement already satisfied: absl-py>=0.4 in /home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/venv/lib/python3.12/site-packages (from tensorboard) (2.1.0)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in /home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/venv/lib/python3.12/site-packages (from tensorboard) (1.67.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/venv/lib/python3.12/site-packages (from tensorboard) (3.7)\n",
      "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/venv/lib/python3.12/site-packages (from tensorboard) (5.28.3)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/venv/lib/python3.12/site-packages (from tensorboard) (75.4.0)\n",
      "Requirement already satisfied: six>1.9 in /home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/venv/lib/python3.12/site-packages (from tensorboard) (1.16.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/venv/lib/python3.12/site-packages (from tensorboard) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/venv/lib/python3.12/site-packages (from tensorboard) (3.1.3)\n",
      "Requirement already satisfied: portalocker in /home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/venv/lib/python3.12/site-packages (from iopath<0.1.10,>=0.1.7) (2.10.1)\n",
      "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/venv/lib/python3.12/site-packages (from omegaconf<2.4,>=2.1) (4.9.3)\n",
      "Requirement already satisfied: click>=8.0.0 in /home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/venv/lib/python3.12/site-packages (from black) (8.1.7)\n",
      "Requirement already satisfied: mypy-extensions>=0.4.3 in /home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/venv/lib/python3.12/site-packages (from black) (1.0.0)\n",
      "Requirement already satisfied: pathspec>=0.9.0 in /home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/venv/lib/python3.12/site-packages (from black) (0.12.1)\n",
      "Requirement already satisfied: platformdirs>=2 in /home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/venv/lib/python3.12/site-packages (from black) (4.3.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/venv/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "#  https://detectron2.readthedocs.io/tutorials/install.html\n",
    "!python -m pip install pyyaml==5.1\n",
    "import sys, os, distutils.core\n",
    "!git clone 'https://github.com/facebookresearch/detectron2'\n",
    "dist = distutils.core.run_setup(\"./detectron2/setup.py\")\n",
    "!python -m pip install {' '.join([f\"'{x}'\" for x in dist.install_requires])}\n",
    "sys.path.insert(0, os.path.abspath('./detectron2'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "3hagrMdJ7j_b"
   },
   "outputs": [],
   "source": [
    "# import detectron2 and its utilities\n",
    "import torch\n",
    "import detectron2\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "from detectron2.data import MetadataCatalog, DatasetCatalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2868,
     "status": "ok",
     "timestamp": 1705575655522,
     "user": {
      "displayName": "Flavio Kuka",
      "userId": "09654499042520950340"
     },
     "user_tz": -60
    },
    "id": "FMDFb2tCuBz6",
    "outputId": "d1ae9fcf-47c9-484b-e911-632c036d9228"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DetectionCheckpointer] Loading from https://dl.fbaipublicfiles.com/detectron2/COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x/137849600/model_final_f10217.pkl ...\n",
      "[Checkpointer] Loading from /home/arkastor/.torch/iopath_cache/detectron2/COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x/137849600/model_final_f10217.pkl ...\n",
      "Reading a file from 'Detectron2 Model Zoo'\n"
     ]
    }
   ],
   "source": [
    "# Get the Configurations\n",
    "cfg = get_cfg()\n",
    "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\"))\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5\n",
    "cfg.MODEL.DEVICE = \"cpu\"\n",
    "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\")\n",
    "predictor = DefaultPredictor(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7evAGtcH7f8A"
   },
   "source": [
    "## 2.2 DEX: Deep EXpectation of apparent age from a single image (Age and Gender)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "mGX43VY8g3Vi"
   },
   "outputs": [],
   "source": [
    "# !git clone 'https://github.com/serengil/tensorflow-101.git'\n",
    "# NOTE: The files imported in this part were taken from the links below\n",
    "#model structure: https://data.vision.ee.ethz.ch/cvl/rrothe/imdb-wiki/static/age.prototxt\n",
    "#pre-trained weights: https://data.vision.ee.ethz.ch/cvl/rrothe/imdb-wiki/static/dex_chalearn_iccv2015.caffemodel\n",
    "#age_model = cv2.dnn.readNetFromCaffe(\"age.prototxt\", \"dex_chalearn_iccv2015.caffemodel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "pvgl9mPtmokW"
   },
   "outputs": [],
   "source": [
    "\n",
    "age_prototxt = f'{os.getenv(\"BILDANALYSE_MODELS_DEX_DIR\")}/age.prototxt'\n",
    "dex_model = f'{os.getenv(\"BILDANALYSE_MODELS_DEX_DIR\")}/dex_chalearn_iccv2015.caffemodel'\n",
    "gender_prototxt = f'{os.getenv(\"BILDANALYSE_MODELS_DEX_DIR\")}/gender.prototxt'\n",
    "gender_model =  f'{os.getenv(\"BILDANALYSE_MODELS_DEX_DIR\")}/gender.caffemodel'\n",
    "\n",
    "age_model = cv2.dnn.readNet(age_prototxt, dex_model)\n",
    "gender_model = cv2.dnn.readNet(gender_prototxt, gender_model)\n",
    "\n",
    "output_indexes = np.array([i for i in range(0, 101)]) # Set up age output range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "eQn7TDb97FQG"
   },
   "outputs": [],
   "source": [
    "#Haar cascade for face detection\n",
    "opencv_home = cv2.__file__\n",
    "folders = opencv_home.split(os.path.sep)[0:-1]\n",
    "path = folders[0]\n",
    "for folder in folders[1:]:\n",
    "    path = path + \"/\" + folder\n",
    "face_detector_path = path+\"/data/haarcascade_frontalface_default.xml\"\n",
    "if os.path.isfile(face_detector_path) != True:\n",
    "    raise ValueError(\"Confirm that opencv is installed on your environment! Expected path \",face_detector_path,\" violated.\")\n",
    "haar_detector = cv2.CascadeClassifier(face_detector_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "00Ue5JqW7_TX"
   },
   "source": [
    "## 2.3 FER: Facial expression recognition (Emotion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11852,
     "status": "ok",
     "timestamp": 1705575701477,
     "user": {
      "displayName": "Flavio Kuka",
      "userId": "09654499042520950340"
     },
     "user_tz": -60
    },
    "id": "88r6P-B44Bbr",
    "outputId": "424bd517-4096-4362-927d-3dec682185de"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fer in /home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/venv/lib/python3.12/site-packages (22.5.1)\n",
      "Requirement already satisfied: matplotlib in /home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/venv/lib/python3.12/site-packages (from fer) (3.9.2)\n",
      "Requirement already satisfied: opencv-contrib-python in /home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/venv/lib/python3.12/site-packages (from fer) (4.10.0.84)\n",
      "Requirement already satisfied: keras>=2.0.0 in /home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/venv/lib/python3.12/site-packages (from fer) (3.6.0)\n",
      "Requirement already satisfied: pandas in /home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/venv/lib/python3.12/site-packages (from fer) (2.2.3)\n",
      "Requirement already satisfied: requests in /home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/venv/lib/python3.12/site-packages (from fer) (2.32.3)\n",
      "Requirement already satisfied: facenet-pytorch in /home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/venv/lib/python3.12/site-packages (from fer) (2.6.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/venv/lib/python3.12/site-packages (from fer) (4.67.0)\n",
      "Requirement already satisfied: moviepy in /home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/venv/lib/python3.12/site-packages (from fer) (1.0.3)\n",
      "Requirement already satisfied: ffmpeg==1.4 in /home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/venv/lib/python3.12/site-packages (from fer) (1.4)\n",
      "Requirement already satisfied: Pillow in /home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/venv/lib/python3.12/site-packages (from fer) (10.2.0)\n",
      "Requirement already satisfied: absl-py in /home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/venv/lib/python3.12/site-packages (from keras>=2.0.0->fer) (2.1.0)\n",
      "Requirement already satisfied: numpy in /home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/venv/lib/python3.12/site-packages (from keras>=2.0.0->fer) (1.26.4)\n",
      "Requirement already satisfied: rich in /home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/venv/lib/python3.12/site-packages (from keras>=2.0.0->fer) (13.9.4)\n",
      "Requirement already satisfied: namex in /home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/venv/lib/python3.12/site-packages (from keras>=2.0.0->fer) (0.0.8)\n",
      "Requirement already satisfied: h5py in /home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/venv/lib/python3.12/site-packages (from keras>=2.0.0->fer) (3.12.1)\n",
      "Requirement already satisfied: optree in /home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/venv/lib/python3.12/site-packages (from keras>=2.0.0->fer) (0.13.1)\n",
      "Requirement already satisfied: ml-dtypes in /home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/venv/lib/python3.12/site-packages (from keras>=2.0.0->fer) (0.4.1)\n",
      "Requirement already satisfied: packaging in /home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/venv/lib/python3.12/site-packages (from keras>=2.0.0->fer) (24.2)\n",
      "Requirement already satisfied: torch<2.3.0,>=2.2.0 in /home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/venv/lib/python3.12/site-packages (from facenet-pytorch->fer) (2.2.2)\n",
      "Requirement already satisfied: torchvision<0.18.0,>=0.17.0 in /home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/venv/lib/python3.12/site-packages (from facenet-pytorch->fer) (0.17.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/venv/lib/python3.12/site-packages (from requests->fer) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/venv/lib/python3.12/site-packages (from requests->fer) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/venv/lib/python3.12/site-packages (from requests->fer) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/venv/lib/python3.12/site-packages (from requests->fer) (2024.8.30)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/venv/lib/python3.12/site-packages (from matplotlib->fer) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/venv/lib/python3.12/site-packages (from matplotlib->fer) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/venv/lib/python3.12/site-packages (from matplotlib->fer) (4.54.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/venv/lib/python3.12/site-packages (from matplotlib->fer) (1.4.7)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/venv/lib/python3.12/site-packages (from matplotlib->fer) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/venv/lib/python3.12/site-packages (from matplotlib->fer) (2.9.0.post0)\n",
      "Requirement already satisfied: decorator<5.0,>=4.0.2 in /home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/venv/lib/python3.12/site-packages (from moviepy->fer) (4.4.2)\n",
      "Requirement already satisfied: proglog<=1.0.0 in /home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/venv/lib/python3.12/site-packages (from moviepy->fer) (0.1.10)\n",
      "Requirement already satisfied: imageio<3.0,>=2.5 in /home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/venv/lib/python3.12/site-packages (from moviepy->fer) (2.36.0)\n",
      "Requirement already satisfied: imageio-ffmpeg>=0.2.0 in /home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/venv/lib/python3.12/site-packages (from moviepy->fer) (0.5.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/venv/lib/python3.12/site-packages (from pandas->fer) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/venv/lib/python3.12/site-packages (from pandas->fer) (2024.2)\n",
      "Requirement already satisfied: setuptools in /home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/venv/lib/python3.12/site-packages (from imageio-ffmpeg>=0.2.0->moviepy->fer) (75.4.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/venv/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib->fer) (1.16.0)\n",
      "Requirement already satisfied: filelock in /home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/venv/lib/python3.12/site-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch->fer) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/venv/lib/python3.12/site-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch->fer) (4.12.2)\n",
      "Requirement already satisfied: sympy in /home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/venv/lib/python3.12/site-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch->fer) (1.13.1)\n",
      "Requirement already satisfied: networkx in /home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/venv/lib/python3.12/site-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch->fer) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/venv/lib/python3.12/site-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch->fer) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/venv/lib/python3.12/site-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch->fer) (2024.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/venv/lib/python3.12/site-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch->fer) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/venv/lib/python3.12/site-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch->fer) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/venv/lib/python3.12/site-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch->fer) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/venv/lib/python3.12/site-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch->fer) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/venv/lib/python3.12/site-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch->fer) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/venv/lib/python3.12/site-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch->fer) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/venv/lib/python3.12/site-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch->fer) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/venv/lib/python3.12/site-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch->fer) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/venv/lib/python3.12/site-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch->fer) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/venv/lib/python3.12/site-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch->fer) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/venv/lib/python3.12/site-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch->fer) (12.1.105)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/venv/lib/python3.12/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch<2.3.0,>=2.2.0->facenet-pytorch->fer) (12.5.82)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/venv/lib/python3.12/site-packages (from rich->keras>=2.0.0->fer) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/venv/lib/python3.12/site-packages (from rich->keras>=2.0.0->fer) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/venv/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich->keras>=2.0.0->fer) (0.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/venv/lib/python3.12/site-packages (from jinja2->torch<2.3.0,>=2.2.0->facenet-pytorch->fer) (3.0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/venv/lib/python3.12/site-packages (from sympy->torch<2.3.0,>=2.2.0->facenet-pytorch->fer) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install fer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "qmSRqlBv5y69"
   },
   "outputs": [],
   "source": [
    "from fer import FER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mCiSl7yI7aBK"
   },
   "source": [
    "## 2.4 DeepFace (Ethnicity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fetxil3rTWPw"
   },
   "source": [
    "'Deepface is a hybrid face recognition package. It currently wraps many state-of-the-art face recognition models: VGG-Face , Google FaceNet, OpenFace, Facebook DeepFace, DeepID, ArcFace, Dlib and SFace. The default configuration uses VGG-Face model.'\n",
    "- https://github.com/serengil/deepface, accessed Nov. 16th, 2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 28843,
     "status": "ok",
     "timestamp": 1705575739364,
     "user": {
      "displayName": "Flavio Kuka",
      "userId": "09654499042520950340"
     },
     "user_tz": -60
    },
    "id": "_I63mMMHTKeT",
    "outputId": "9077d9f1-0d71-48b7-9d2e-d3315c19daa2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: deepface in /home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/venv/lib/python3.12/site-packages (0.0.93)\n",
      "Requirement already satisfied: requests>=2.27.1 in /home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/venv/lib/python3.12/site-packages (from deepface) (2.32.3)\n",
      "Requirement already satisfied: numpy>=1.14.0 in /home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/venv/lib/python3.12/site-packages (from deepface) (1.26.4)\n",
      "Requirement already satisfied: pandas>=0.23.4 in /home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/venv/lib/python3.12/site-packages (from deepface) (2.2.3)\n",
      "Requirement already satisfied: gdown>=3.10.1 in /home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/venv/lib/python3.12/site-packages (from deepface) (5.2.0)\n",
      "Requirement already satisfied: tqdm>=4.30.0 in /home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/venv/lib/python3.12/site-packages (from deepface) (4.67.0)\n",
      "Requirement already satisfied: Pillow>=5.2.0 in /home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/venv/lib/python3.12/site-packages (from deepface) (10.2.0)\n",
      "Requirement already satisfied: opencv-python>=4.5.5.64 in /home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/venv/lib/python3.12/site-packages (from deepface) (4.10.0.84)\n",
      "Requirement already satisfied: tensorflow>=1.9.0 in /home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/venv/lib/python3.12/site-packages (from deepface) (2.18.0)\n",
      "Requirement already satisfied: keras>=2.2.0 in /home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/venv/lib/python3.12/site-packages (from deepface) (3.6.0)\n",
      "Requirement already satisfied: Flask>=1.1.2 in /home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/venv/lib/python3.12/site-packages (from deepface) (3.0.3)\n",
      "Requirement already satisfied: flask-cors>=4.0.1 in /home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/venv/lib/python3.12/site-packages (from deepface) (5.0.0)\n",
      "Requirement already satisfied: mtcnn>=0.1.0 in /home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/venv/lib/python3.12/site-packages (from deepface) (1.0.0)\n",
      "Requirement already satisfied: retina-face>=0.0.1 in /home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/venv/lib/python3.12/site-packages (from deepface) (0.0.17)\n",
      "Requirement already satisfied: fire>=0.4.0 in /home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/venv/lib/python3.12/site-packages (from deepface) (0.7.0)\n",
      "Requirement already satisfied: gunicorn>=20.1.0 in /home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/venv/lib/python3.12/site-packages (from deepface) (23.0.0)\n",
      "Requirement already satisfied: termcolor in /home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/venv/lib/python3.12/site-packages (from fire>=0.4.0->deepface) (2.5.0)\n",
      "Requirement already satisfied: Werkzeug>=3.0.0 in /home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/venv/lib/python3.12/site-packages (from Flask>=1.1.2->deepface) (3.1.3)\n",
      "Requirement already satisfied: Jinja2>=3.1.2 in /home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/venv/lib/python3.12/site-packages (from Flask>=1.1.2->deepface) (3.1.4)\n",
      "Requirement already satisfied: itsdangerous>=2.1.2 in /home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/venv/lib/python3.12/site-packages (from Flask>=1.1.2->deepface) (2.2.0)\n",
      "Requirement already satisfied: click>=8.1.3 in /home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/venv/lib/python3.12/site-packages (from Flask>=1.1.2->deepface) (8.1.7)\n",
      "Requirement already satisfied: blinker>=1.6.2 in /home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/venv/lib/python3.12/site-packages (from Flask>=1.1.2->deepface) (1.9.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/venv/lib/python3.12/site-packages (from gdown>=3.10.1->deepface) (4.12.3)\n",
      "Requirement already satisfied: filelock in /home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/venv/lib/python3.12/site-packages (from gdown>=3.10.1->deepface) (3.16.1)\n",
      "Requirement already satisfied: packaging in /home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/venv/lib/python3.12/site-packages (from gunicorn>=20.1.0->deepface) (24.2)\n",
      "Requirement already satisfied: absl-py in /home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/venv/lib/python3.12/site-packages (from keras>=2.2.0->deepface) (2.1.0)\n",
      "Requirement already satisfied: rich in /home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/venv/lib/python3.12/site-packages (from keras>=2.2.0->deepface) (13.9.4)\n",
      "Requirement already satisfied: namex in /home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/venv/lib/python3.12/site-packages (from keras>=2.2.0->deepface) (0.0.8)\n",
      "Requirement already satisfied: h5py in /home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/venv/lib/python3.12/site-packages (from keras>=2.2.0->deepface) (3.12.1)\n",
      "Requirement already satisfied: optree in /home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/venv/lib/python3.12/site-packages (from keras>=2.2.0->deepface) (0.13.1)\n",
      "Requirement already satisfied: ml-dtypes in /home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/venv/lib/python3.12/site-packages (from keras>=2.2.0->deepface) (0.4.1)\n",
      "Requirement already satisfied: joblib>=1.4.2 in /home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/venv/lib/python3.12/site-packages (from mtcnn>=0.1.0->deepface) (1.4.2)\n",
      "Requirement already satisfied: lz4>=4.3.3 in /home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/venv/lib/python3.12/site-packages (from mtcnn>=0.1.0->deepface) (4.3.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/venv/lib/python3.12/site-packages (from pandas>=0.23.4->deepface) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/venv/lib/python3.12/site-packages (from pandas>=0.23.4->deepface) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/venv/lib/python3.12/site-packages (from pandas>=0.23.4->deepface) (2024.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/venv/lib/python3.12/site-packages (from requests>=2.27.1->deepface) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/venv/lib/python3.12/site-packages (from requests>=2.27.1->deepface) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/venv/lib/python3.12/site-packages (from requests>=2.27.1->deepface) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/venv/lib/python3.12/site-packages (from requests>=2.27.1->deepface) (2024.8.30)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/venv/lib/python3.12/site-packages (from tensorflow>=1.9.0->deepface) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/venv/lib/python3.12/site-packages (from tensorflow>=1.9.0->deepface) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/venv/lib/python3.12/site-packages (from tensorflow>=1.9.0->deepface) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/venv/lib/python3.12/site-packages (from tensorflow>=1.9.0->deepface) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/venv/lib/python3.12/site-packages (from tensorflow>=1.9.0->deepface) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/venv/lib/python3.12/site-packages (from tensorflow>=1.9.0->deepface) (3.4.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/venv/lib/python3.12/site-packages (from tensorflow>=1.9.0->deepface) (5.28.3)\n",
      "Requirement already satisfied: setuptools in /home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/venv/lib/python3.12/site-packages (from tensorflow>=1.9.0->deepface) (75.4.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/venv/lib/python3.12/site-packages (from tensorflow>=1.9.0->deepface) (1.16.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/venv/lib/python3.12/site-packages (from tensorflow>=1.9.0->deepface) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/venv/lib/python3.12/site-packages (from tensorflow>=1.9.0->deepface) (1.16.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/venv/lib/python3.12/site-packages (from tensorflow>=1.9.0->deepface) (1.67.1)\n",
      "Requirement already satisfied: tensorboard<2.19,>=2.18 in /home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/venv/lib/python3.12/site-packages (from tensorflow>=1.9.0->deepface) (2.18.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/venv/lib/python3.12/site-packages (from astunparse>=1.6.0->tensorflow>=1.9.0->deepface) (0.45.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/venv/lib/python3.12/site-packages (from Jinja2>=3.1.2->Flask>=1.1.2->deepface) (3.0.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/venv/lib/python3.12/site-packages (from tensorboard<2.19,>=2.18->tensorflow>=1.9.0->deepface) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/venv/lib/python3.12/site-packages (from tensorboard<2.19,>=2.18->tensorflow>=1.9.0->deepface) (0.7.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/venv/lib/python3.12/site-packages (from beautifulsoup4->gdown>=3.10.1->deepface) (2.6)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/venv/lib/python3.12/site-packages (from requests[socks]->gdown>=3.10.1->deepface) (1.7.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/venv/lib/python3.12/site-packages (from rich->keras>=2.2.0->deepface) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/venv/lib/python3.12/site-packages (from rich->keras>=2.2.0->deepface) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/venv/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich->keras>=2.2.0->deepface) (0.1.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install deepface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 362,
     "status": "ok",
     "timestamp": 1705575739722,
     "user": {
      "displayName": "Flavio Kuka",
      "userId": "09654499042520950340"
     },
     "user_tz": -60
    },
    "id": "vMfY4OwBTNp7",
    "outputId": "20d72680-b096-4281-c39f-52f8a59bea09"
   },
   "outputs": [],
   "source": [
    "from deepface import DeepFace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UPyxir_l63S6"
   },
   "source": [
    "# 3. Model Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LIBLSDV_xbfl"
   },
   "source": [
    "## 3.1 Frame Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "4hUzF19rxiqd"
   },
   "outputs": [],
   "source": [
    "def frame_extraction():\n",
    "    # inspired by https://stackoverflow.com/questions/63666638/convert-video-to-frames-in-python-1-fps, accessed Oct. 26th, 2023\n",
    "    KPS = fpsextractor['kps'] # Target Keyframes Per Second\n",
    "    VIDEO_PATH = os.path.join(dirpath, dirname, filename) # path to current video\n",
    "    FPS_OUPUT= fpsextractor['output']\n",
    "    YEAR = dirname.replace(\"ADs_IG_\", \"\")\n",
    "    YEAR_OUTPUT_DIR = FPS_OUPUT+ \"/\" + YEAR\n",
    "    OUTPUT_PATH = YEAR_OUTPUT_DIR + \"/\" + dirname + \"/\"\n",
    "    EXTENSION = \".\" + fpsextractor['extension'] # file extension of exported images\n",
    "    fileNameOfVideoWithoutExtension = filename[:-len(\".\" + fpsextractor['extension'])];\n",
    "    # print(OUTPUT_PATH) # e.g., ./outputs/fps_extractor/ADs_IG_2018/\n",
    "    #print(OUTPUT_PATH + fileNameOfVideoWithoutExtension) # e.g., ./outputs/fps_extractor/ADs_IG_2018/AD0576\n",
    "\n",
    "    # Ordner erstellen, in welchem je Video die Frames gepseichert werden\n",
    "    # Ordner mit Jahreszahl\n",
    "    if not os.path.exists(FPS_OUPUT):\n",
    "        os.mkdir(FPS_OUPUT)\n",
    "    if not os.path.exists(YEAR_OUTPUT_DIR):\n",
    "        os.mkdir(YEAR_OUTPUT_DIR)        \n",
    "    if not os.path.exists(OUTPUT_PATH):\n",
    "        os.mkdir(OUTPUT_PATH)\n",
    "    # Unterordner mit Video-ID\n",
    "    if not os.path.exists(OUTPUT_PATH + fileNameOfVideoWithoutExtension):\n",
    "        os.mkdir(OUTPUT_PATH + fileNameOfVideoWithoutExtension)\n",
    "\n",
    "    # print(KPS, IMAGE_PATH, EXTENSION)\n",
    "\n",
    "    cap = cv2.VideoCapture(VIDEO_PATH)\n",
    "    fps = round(cap.get(cv2.CAP_PROP_FPS))\n",
    "    print(fps)\n",
    "    # exit()\n",
    "    hop = round(fps / KPS)\n",
    "    curr_frame = 0\n",
    "    while(True):\n",
    "        ret, frame = cap.read()\n",
    "        if not ret: break\n",
    "        if curr_frame % hop == 0:\n",
    "            name = OUTPUT_PATH + fileNameOfVideoWithoutExtension + \"/\" + fileNameOfVideoWithoutExtension + \"_Frame_\" + str(curr_frame) + EXTENSION\n",
    "            # print(name)\n",
    "            cv2.imwrite(name, frame)\n",
    "        curr_frame += 1\n",
    "    cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PEbX1dyj7O3L"
   },
   "source": [
    "## 3.2 Predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aQBwOdN58mB6"
   },
   "source": [
    "### 3.2.1 detectron2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "yKbFl04PzS77"
   },
   "outputs": [],
   "source": [
    "# Functions for finding the quadrant_number\n",
    "def create_nine_quadrants(image):\n",
    "    height, width = image.shape[:2]\n",
    "    quadrant_width = width // 3\n",
    "    quadrant_height = height // 3\n",
    "\n",
    "    quadrants = {}\n",
    "\n",
    "    for i in range(3):\n",
    "        for j in range(3):\n",
    "            quadrant_num = i * 3 + j + 1\n",
    "            x1 = j * quadrant_width\n",
    "            y1 = i * quadrant_height\n",
    "            x2 = x1 + quadrant_width\n",
    "            y2 = y1 + quadrant_height\n",
    "            quadrants[quadrant_num] = [(x1, y1), (x2, y2)]\n",
    "\n",
    "    def get_quadrant_for_coordinate(coord):\n",
    "        x, y = coord\n",
    "        for quadrant_num, ((x1, y1), (x2, y2)) in quadrants.items():\n",
    "            if x1 <= x < x2 and y1 <= y < y2:\n",
    "                return quadrant_num\n",
    "        return None\n",
    "\n",
    "    return get_quadrant_for_coordinate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "q18ktWF_oDEa"
   },
   "outputs": [],
   "source": [
    "# Function for creating cropped image\n",
    "def crop_human_bounding_box(image, bounding_box):\n",
    "    # Get coordinates of the bounding box\n",
    "    x_min, y_min, x_max, y_max = map(int, bounding_box)\n",
    "\n",
    "    # Ensure coordinates are within the image boundaries\n",
    "    x_min = max(0, x_min)\n",
    "    y_min = max(0, y_min)\n",
    "    x_max = min(image.shape[1], x_max)\n",
    "    y_max = min(image.shape[0], y_max)\n",
    "\n",
    "    # Crop the human region from the image based on the bounding box\n",
    "    cropped_human = image[y_min:y_max, x_min:x_max]\n",
    "\n",
    "    return cropped_human"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "gi4keqCDZm63"
   },
   "outputs": [],
   "source": [
    "# Function for visualising outputs\n",
    "def detectron2_visualisation(im, outputs):\n",
    "    v = Visualizer(im[:, :, ::-1], MetadataCatalog.get(cfg.DATASETS.TRAIN[0]), scale=1.2)\n",
    "    out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
    "\n",
    "    #Change image to RGB\n",
    "    image_bgr = out.get_image()\n",
    "    image = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Generate output path for the annotated image\n",
    "    img_filename = os.path.basename(image_path)\n",
    "    output_filename = img_filename.split('.')[0] + f'_detectron_annotated.png'\n",
    "    output_path = os.path.join(visualiser_folder_path_ad, output_filename)\n",
    "\n",
    "    cv2.imwrite(output_path, image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "XONCZ2v4gM3O"
   },
   "outputs": [],
   "source": [
    "def detectron2_analysis(im):\n",
    "    outputs = predictor(im)\n",
    "    metadata = MetadataCatalog.get(cfg.DATASETS.TEST[0])\n",
    "    class_names = metadata.get(\"thing_classes\", None)\n",
    "\n",
    "    instances = outputs[\"instances\"].to(\"cpu\")\n",
    "\n",
    "    prediction_objects = []\n",
    "\n",
    "    # Iterate over objects found\n",
    "    for i in range(len(instances)):\n",
    "        bbox = instances.pred_boxes.tensor.cpu().numpy()[i].squeeze()\n",
    "\n",
    "        # Area Calculation\n",
    "        x1, y1, x2, y2 = bbox\n",
    "        area_object = (x2 - x1) * (y2 - y1)\n",
    "        height, width = im.shape[:2]\n",
    "        area_image = height * width\n",
    "        object_propotion = area_object/ area_image\n",
    "\n",
    "        # Center-Point Calculation\n",
    "        center_point = [(x1 + x2) / 2, (y1 + y2) / 2]\n",
    "        get_quadrant_for_coordinate = create_nine_quadrants(im)\n",
    "        quadrant_number = get_quadrant_for_coordinate(center_point)\n",
    "\n",
    "        # Cropping image and predicting the other attributes\n",
    "        if instances.pred_classes[i].item() == 0:\n",
    "           human_image = crop_human_bounding_box(im, bbox)\n",
    "           age_prediction, age_group_prediction, gender_prediction = dex_analysis(human_image.copy(), visualiser, i)\n",
    "           emotion_prediction = fer_analysis(human_image.copy(), visualiser, i)\n",
    "           ethnicity_prediction = deepface_analysis(human_image.copy(), visualiser, i)\n",
    "        else:\n",
    "            age_prediction = age_group_prediction = gender_prediction = emotion_prediction = ethnicity_prediction = \"-\"\n",
    "\n",
    "        # Saving prediction data\n",
    "        data = {\n",
    "            \"video_frame\": element,\n",
    "            \"class_id\": instances.pred_classes[i].item(),\n",
    "            \"class_name\": class_names[instances.pred_classes[i].item()],\n",
    "            \"confidence\": instances.scores[i].item(),\n",
    "            \"object_propotion\": object_propotion,\n",
    "            \"quadrant_number\": quadrant_number,\n",
    "            \"age_prediction\": age_prediction,\n",
    "            \"age_group_prediction\": age_group_prediction,\n",
    "            \"gender_prediction\": gender_prediction,\n",
    "            \"emotion_prediction\": emotion_prediction,\n",
    "            \"ethnicity_prediction\": ethnicity_prediction\n",
    "        }\n",
    "        prediction_objects.append(data)\n",
    "\n",
    "    if visualiser:\n",
    "      detectron2_visualisation(im, outputs)\n",
    "\n",
    "    return prediction_objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xDvzeJS4x9_I"
   },
   "source": [
    "### 3.2.2 DEX: Deep EXpectation of apparent age from a single image (Age and Gender)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "qbwAmtC6empO"
   },
   "outputs": [],
   "source": [
    "# Detecting faces\n",
    "def detect_faces(img):\n",
    "\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    faces_unsorted = haar_detector.detectMultiScale(gray, 1.3, 5)\n",
    "\n",
    "    # Sort the faces so that only the biggest bounding box is detected\n",
    "    faces = sorted(faces_unsorted, key=lambda face: face[2] * face[3], reverse=True)\n",
    "\n",
    "    return faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "1vVeZgcc1j2A"
   },
   "outputs": [],
   "source": [
    "# Visualising output\n",
    "def dex_visualisation(img, face, age, gender, i):\n",
    "    # Bounding Box\n",
    "    x, y, w, h = face\n",
    "    # Draw rectangle around detected face\n",
    "    cv2.rectangle(img, (int(x), int(y)), (int(x + w), int(y + h)), (0, 255, 0), 2)\n",
    "\n",
    "    # Annotate with age and gender information\n",
    "    text = f'Age: {age}, Gender: {gender}'\n",
    "    cv2.putText(img, text, (int(x), int(y) - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "    # Generate output path for the annotated image\n",
    "    img_filename = os.path.basename(image_path)\n",
    "    output_filename = img_filename.split('.')[0] + f'_age_gender_{i}_annotated.png'\n",
    "    output_path = os.path.join(visualiser_folder_path_ad, output_filename)\n",
    "\n",
    "    # Save the annotated image with bounding boxes and emotions\n",
    "    cv2.imwrite(output_path, img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "dINtUDGfoLy8"
   },
   "outputs": [],
   "source": [
    "def dex_analysis(img, visualiser, i):\n",
    "    # Detect faces in the image\n",
    "    faces = detect_faces(img)\n",
    "\n",
    "    if faces is not None and len(faces) > 0:\n",
    "        face = faces[0]\n",
    "        x, y, w, h = face\n",
    "\n",
    "        detected_face = img[int(y):int(y+h), int(x):int(x+w)]\n",
    "        detected_face = cv2.resize(detected_face, (224, 224))\n",
    "        img_blob = cv2.dnn.blobFromImage(detected_face)\n",
    "\n",
    "        # Age prediction\n",
    "        age_model.setInput(img_blob)\n",
    "        age_dist = age_model.forward()[0]\n",
    "        age = round(np.sum(age_dist * output_indexes))\n",
    "\n",
    "        # Age Group (based on a similar study by Chaudhari, S. J., & Kagalkar, R. M. (2015). Methodology for gender identification, classification and recognition of human age. International Journal of Computer Applications, 975, 8887.)\n",
    "        age_group = [0, 8] if 0 < age <= 8 else \\\n",
    "                    [9, 17] if 9 <= age < 18 else \\\n",
    "                    [18, 30] if 18 <= age < 31 else \\\n",
    "                    [31, 60] if 31 <= age < 61 else \\\n",
    "                    [61, 100] if 61 <= age < 100 else \\\n",
    "                    [100, 200]\n",
    "\n",
    "        # Gender prediction\n",
    "        gender_model.setInput(img_blob)\n",
    "        gender_class = gender_model.forward()[0]\n",
    "        gender = 'Woman' if np.argmax(gender_class) == 0 else 'Man'\n",
    "\n",
    "        if visualiser:\n",
    "            dex_visualisation(img, face, age, gender, i)\n",
    "    else:\n",
    "        age = age_group = gender = \"-\"\n",
    "\n",
    "    return age, age_group, gender\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gBpXPQbc57fz"
   },
   "source": [
    "### 3.2.3 FER: Facial expression recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "5wqo6HIc6Nw3"
   },
   "outputs": [],
   "source": [
    "def fer_visualisation(img, result, emotions, i):\n",
    "    # Get face bounding box coordinates\n",
    "    x, y, w, h = result['box']\n",
    "\n",
    "    # Draw a bounding box around the detected face\n",
    "    cv2.rectangle(img, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "\n",
    "    # Get the dominant emotion and its probability for the face\n",
    "    dominant_emotion = max(emotions, key=emotions.get)\n",
    "    emotion_probability = emotions[dominant_emotion]\n",
    "\n",
    "    # Annotate the image with the dominant emotion for each face\n",
    "    cv2.putText(img, f'{dominant_emotion} ({emotion_probability:.2f})',(x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 1.5, (0, 255, 0), 2)\n",
    "\n",
    "    # Generate output path for the annotated image\n",
    "    img_filename = os.path.basename(image_path)\n",
    "    output_filename = img_filename.split('.')[0] + f'_emotion_{i}_annotated.png'\n",
    "    output_path = os.path.join(visualiser_folder_path_ad, output_filename)\n",
    "\n",
    "    # Save the annotated image with bounding boxes and emotions\n",
    "    cv2.imwrite(output_path, img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "29AayRos6DyO"
   },
   "outputs": [],
   "source": [
    "def fer_analysis(img, visualiser, i):\n",
    "    # Initialize the FER model\n",
    "    detector = FER(mtcnn=True)\n",
    "\n",
    "    try:\n",
    "      # Detect faces and their emotions in the image\n",
    "      results = detector.detect_emotions(img)\n",
    "\n",
    "      # Sort the results so that only the biggest bounding box is detected\n",
    "      sorted_results = sorted(results, key=lambda x: (x['box'][2] * x['box'][3]), reverse=True)\n",
    "      result= sorted_results[0]\n",
    "\n",
    "      # Get detected emotions for the face\n",
    "      emotions = result['emotions']\n",
    "\n",
    "      # Get the dominant emotion and its probability for the face\n",
    "      dominant_emotion = max(emotions, key=emotions.get)\n",
    "\n",
    "      if visualiser:\n",
    "          fer_visualisation(img, result, emotions, i)\n",
    "\n",
    "    except:\n",
    "        dominant_emotion = \"-\"\n",
    "\n",
    "    return dominant_emotion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kku-Bcn4ThSK"
   },
   "source": [
    "### 3.2.4 DeepFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "NbFTZp8hT1RL"
   },
   "outputs": [],
   "source": [
    "def deepface_visualiser(img, prediction, i):\n",
    "\n",
    "    x, y, w, h = prediction['region']['x'], prediction['region']['y'], prediction['region']['w'], prediction['region']['h']\n",
    "\n",
    "    #draw the rectangle\n",
    "    cv2.rectangle(img, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "    #plot text on image\n",
    "    cv2.putText(img, str(prediction['dominant_race']), (int(x), int(y) - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "    # Generate output path for the annotated image\n",
    "    img_filename = os.path.basename(image_path)\n",
    "    output_filename = img_filename.split('.')[0] + f'_ethnicity_{i}_annotated.png'\n",
    "    output_path = os.path.join(visualiser_folder_path_ad, output_filename)\n",
    "\n",
    "    # Save the annotated image with bounding boxes and emotions\n",
    "    cv2.imwrite(output_path, img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "P5PGK_Z3TgWf"
   },
   "outputs": [],
   "source": [
    "def deepface_analysis(img, visualiser, i):\n",
    "\n",
    "    try:\n",
    "      predictions = DeepFace.analyze(img, actions = ['race'])\n",
    "\n",
    "      sorted_predictions = sorted(predictions, key=lambda x: x['region']['w'] * x['region']['h'], reverse=True)\n",
    "      prediction = sorted_predictions[0]\n",
    "      dominant_ethnicity = prediction['dominant_race']\n",
    "\n",
    "      if visualiser:\n",
    "          deepface_visualiser(img, prediction, i)\n",
    "\n",
    "    except:\n",
    "      dominant_ethnicity = \"-\"\n",
    "\n",
    "    return dominant_ethnicity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UniRLoRE7mFp"
   },
   "source": [
    "## 3.2 Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tNdx4mCz91Ps"
   },
   "source": [
    "### 3.2.1 Summary (1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "Fbm2qPDl_frz"
   },
   "outputs": [],
   "source": [
    "def generate_summary(input_file_path):\n",
    "    # Read the Excel file\n",
    "    data = pd.read_excel(input_file_path, sheet_name='Predictions', header=[0])\n",
    "\n",
    "    # Filter out the instances where objects are smaller than the schwellenwert_proportion_detectron_2\n",
    "    filtered_predictions_classes = data[data['object_propotion'] >= schwellenwert_proportion_detectron_2]\n",
    "\n",
    "    # Calculate the summary for classes\n",
    "    summary_classes_sheet = filtered_predictions_classes.groupby('class_name').size().reset_index(name='count')\n",
    "    summary_classes_sheet['avg_confidence'] = filtered_predictions_classes.groupby('class_name')['confidence'].mean().values.round(4)\n",
    "    summary_classes_sheet['avg_object_propotion'] = filtered_predictions_classes.groupby('class_name')['object_propotion'].mean().values.round(4)\n",
    "    summary_classes_sheet['avg_quadrant_number'] = filtered_predictions_classes.groupby('class_name')['quadrant_number'].mean().values.round()\n",
    "    summary_classes_sheet['frame_nr'] = filtered_predictions_classes.groupby('class_name')['video_frame'].nunique().reset_index()['video_frame']\n",
    "\n",
    "    # Calculate the frame_nr_ratio\n",
    "    total_frame_number = filtered_predictions_classes.iloc[0, 13]\n",
    "    summary_classes_sheet['frame_ratio'] = round(summary_classes_sheet['frame_nr'] / total_frame_number, 4)\n",
    "\n",
    "    # Filter rows based on the frame number\n",
    "    summary_classes_sheet = summary_classes_sheet[summary_classes_sheet['frame_nr'] >= schwellenwert_frame_nr_detectron_2]\n",
    "\n",
    "    # Sort by count\n",
    "    summary_classes_sheet = summary_classes_sheet.sort_values(by='count', ascending=False)\n",
    "\n",
    "    # Define columns to compute counts for in the original data\n",
    "    columns_to_count = ['super-category', 'gender_prediction', 'ethnicity_prediction', 'age_group_prediction', 'emotion_prediction']\n",
    "\n",
    "    data.columns = [re.sub(r'Unnamed:\\s*\\d*', '', col) if 'Unnamed' in str(col) else col for col in data.columns]\n",
    "\n",
    "    # Write to Excel\n",
    "    with pd.ExcelWriter(input_file_path, engine='openpyxl') as writer:\n",
    "        # Saving the excel files\n",
    "        data.to_excel(writer, sheet_name='Predictions', index=False, index_label=None)\n",
    "        summary_classes_sheet.to_excel(writer, sheet_name='Summary_Objects', index=False)\n",
    "\n",
    "        # Update the summary attributes sheet\n",
    "        for column in columns_to_count:\n",
    "\n",
    "            # Filter out the empty instances\n",
    "            non_empty_predictions = data[data[column] != \"-\"]\n",
    "\n",
    "            # Filter out the instances where objects are smaller than the schwellenwert_proportion_detectron_2 or schwellenwert_proportion_human_attributes\n",
    "            if column == 'super-category':\n",
    "              filtered_predictions_human = non_empty_predictions[non_empty_predictions['object_propotion'] >= schwellenwert_proportion_detectron_2]\n",
    "            else:\n",
    "              filtered_predictions_human = non_empty_predictions[non_empty_predictions['object_propotion'] >= schwellenwert_proportion_human_attributes]\n",
    "\n",
    "            # Calculate the summary for human attributes\n",
    "            summary_attributes_sheet = filtered_predictions_human.groupby(column).size().reset_index(name='count_' + column)\n",
    "            summary_attributes_sheet[f'avg_object_propotion_{column}'] = filtered_predictions_human.groupby(column)['object_propotion'].mean().values.round(4)\n",
    "            summary_attributes_sheet[f'avg_quadrant_number_{column}'] = filtered_predictions_human.groupby(column)['quadrant_number'].mean().values.round()\n",
    "            summary_attributes_sheet[f'frame_nr_{column}'] = filtered_predictions_human.groupby(column)['video_frame'].nunique().reset_index()['video_frame']\n",
    "            summary_attributes_sheet[f'frame_ratio_{column}'] = round(summary_attributes_sheet[f'frame_nr_{column}'] / total_frame_number, 4)\n",
    "\n",
    "            # Filter out the instances where the number of unique frames are smaller than the schwellenwert_frame_nr_detectron_2 or schwellenwert_frame_nr_human_attributes\n",
    "            if column == \"super-category\":\n",
    "              summary_attributes_sheet = summary_attributes_sheet[summary_attributes_sheet[f'frame_nr_{column}'] >= schwellenwert_frame_nr_detectron_2]\n",
    "            else:\n",
    "              summary_attributes_sheet = summary_attributes_sheet[summary_attributes_sheet[f'frame_nr_{column}'] >= schwellenwert_frame_nr_human_attributes]\n",
    "\n",
    "            summary_attributes_sheet.sort_values(by='count_' + column, ascending=False, inplace=True)\n",
    "            summary_attributes_sheet.to_excel(writer, sheet_name='Summary_Objects', startrow=0, startcol=(columns_to_count.index(column) * 7) + 8, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2X2gvOv01tX6"
   },
   "source": [
    "### 3.2.2 Summary Gender & Ethnicity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "ofR3T7MY178B"
   },
   "outputs": [],
   "source": [
    "def generate_summary_gender_ethnicity(input_file_path):\n",
    "\n",
    "    data = pd.read_excel(input_file_path, sheet_name = 'Predictions')\n",
    "    summary_1 = pd.read_excel(input_file_path, sheet_name='Summary')\n",
    "\n",
    "    ########################################################################### Gender ##############################################################################################\n",
    "    data_gender = data[data['gender_prediction'] != '-']\n",
    "\n",
    "    # Filter out the instances where objects are smaller than the schwellenwert_proportion_human_attributes\n",
    "    data_gender = data_gender[data_gender['object_propotion'] >= schwellenwert_proportion_human_attributes]\n",
    "\n",
    "    # Filter out the instances where the number of unique frames are smaller than the schwellenwert_frame_nr_human_attributes\n",
    "    unique_genders = data_gender['gender_prediction'].unique()\n",
    "    genders_to_remove = []\n",
    "\n",
    "    for gender in unique_genders:\n",
    "        filtered_df = data_gender[data_gender['gender_prediction'] == gender]\n",
    "        unique_video_frames = filtered_df['video_frame'].unique().tolist()\n",
    "\n",
    "        if len(unique_video_frames) < schwellenwert_frame_nr_human_attributes:\n",
    "            genders_to_remove.append(gender)\n",
    "\n",
    "    data_gender = data_gender[~data_gender['gender_prediction'].isin(genders_to_remove)]\n",
    "\n",
    "    #New sheet\n",
    "    Gender_sheet = pd.DataFrame(columns=['case', 'count', 'frame_ratio',\n",
    "                                        'avg_object_propotion_women', 'avg_quadrant_number_women',\n",
    "                                        'quadrant_numbers_women',\n",
    "                                        'avg_object_propotion_men', 'avg_quadrant_number_men',\n",
    "                                        'quadrant_numbers_men'\n",
    "                                        ])\n",
    "    case_data_gender = {}\n",
    "\n",
    "    # Iterate through the data to determine cases and calculate averages\n",
    "    for frame_key, frame_data in data_gender.groupby('video_frame'):\n",
    "        # Count the occurrences of each gender in the frame\n",
    "        gender_counts = frame_data['gender_prediction'].value_counts()\n",
    "\n",
    "        # Create the case string dynamically based on the gender counts\n",
    "        case = ' & '.join([f'{count} {gender}' for gender, count in sorted(gender_counts.items())])\n",
    "\n",
    "        # Check if the case already exists in the case_data dictionary\n",
    "        if case not in case_data_gender:\n",
    "            case_data_gender[case] = {'count': 0,\n",
    "                              'total_object_propotion_women': 0, 'total_quadrant_number_women': 0,\n",
    "                              'quadrant_numbers_women': [],\n",
    "                              'total_object_propotion_men': 0, 'total_quadrant_number_men': 0,\n",
    "                              'quadrant_numbers_men': []\n",
    "                              }\n",
    "\n",
    "        # Update cumulative values for the case\n",
    "        case_data_gender[case]['count'] += 1\n",
    "        case_data_gender[case]['total_object_propotion_women'] += frame_data[frame_data['gender_prediction'] == 'Woman']['object_propotion'].mean()\n",
    "        case_data_gender[case]['total_quadrant_number_women'] += frame_data[frame_data['gender_prediction'] == 'Woman']['quadrant_number'].mean()\n",
    "        case_data_gender[case]['quadrant_numbers_women'].extend(frame_data[frame_data['gender_prediction'] == 'Woman']['quadrant_number'].tolist())\n",
    "\n",
    "        case_data_gender[case]['total_object_propotion_men'] += frame_data[frame_data['gender_prediction'] == 'Man']['object_propotion'].mean()\n",
    "        case_data_gender[case]['total_quadrant_number_men'] += frame_data[frame_data['gender_prediction'] == 'Man']['quadrant_number'].mean()\n",
    "        case_data_gender[case]['quadrant_numbers_men'].extend(frame_data[frame_data['gender_prediction'] == 'Man']['quadrant_number'].tolist())\n",
    "\n",
    "    # Calculate averages for each case\n",
    "    for case, values in case_data_gender.items():\n",
    "        count = values['count']\n",
    "\n",
    "        # Calculate averages for women\n",
    "        avg_object_propotion_women = round(values['total_object_propotion_women'] / count, 4) if count > 0 else 0\n",
    "        avg_quadrant_number_women = round(values['total_quadrant_number_women'] / count) if count > 0 and not np.isnan(values['total_quadrant_number_women']) else 0\n",
    "\n",
    "        # Calculate averages for men\n",
    "        avg_object_propotion_men = round(values['total_object_propotion_men'] / count, 4) if count > 0 else 0\n",
    "        avg_quadrant_number_men = round(values['total_quadrant_number_men'] / count) if count > 0 and not np.isnan(values['total_quadrant_number_men']) else 0\n",
    "\n",
    "        # Calculate frame ratio\n",
    "        total_frame_number = data.iloc[0, 13]\n",
    "        frame_ratio = round(count/ total_frame_number, 4)\n",
    "\n",
    "        # Append the values to the Gender_sheet\n",
    "        Gender_sheet = Gender_sheet.append({\n",
    "            'case': case,\n",
    "            'count': count,\n",
    "            'frame_ratio': frame_ratio,\n",
    "            'avg_object_propotion_women': avg_object_propotion_women,\n",
    "            'avg_quadrant_number_women': avg_quadrant_number_women,\n",
    "            'quadrant_numbers_women': values['quadrant_numbers_women'],\n",
    "            'avg_object_propotion_men': avg_object_propotion_men,\n",
    "            'avg_quadrant_number_men': avg_quadrant_number_men,\n",
    "            'quadrant_numbers_men': values['quadrant_numbers_men']\n",
    "        }, ignore_index=True)\n",
    "\n",
    "    Gender_sheet = Gender_sheet.sort_values(by='count', ascending=False)\n",
    "\n",
    "    # Replace empty spaces with 0 in the entire dataframe\n",
    "    Gender_sheet[['avg_object_propotion_women', 'avg_object_propotion_men']] = Gender_sheet[['avg_object_propotion_women', 'avg_object_propotion_men']].fillna(0)\n",
    "\n",
    "\n",
    "    ########################################################################### Ethnicity #########################################################################################\n",
    "    data_ethnicity = data[data['ethnicity_prediction'] != '-']\n",
    "\n",
    "    # Filter out the instances where objects are smaller than the schwellenwert_proportion_human_attributes\n",
    "    data_ethnicity = data_ethnicity[data_ethnicity['object_propotion'] >= schwellenwert_proportion_human_attributes]\n",
    "\n",
    "    # Filter out the instances where the number of unique frames are smaller than the schwellenwert_frame_nr_human_attributes\n",
    "    unique_ethnicities = data_ethnicity['ethnicity_prediction'].unique()\n",
    "    ethnicity_to_remove = []\n",
    "\n",
    "    for ethnicity in unique_ethnicities:\n",
    "        filtered_df = data_ethnicity[data_ethnicity['ethnicity_prediction'] == ethnicity]\n",
    "        unique_video_frames = filtered_df['video_frame'].unique().tolist()\n",
    "\n",
    "        if len(unique_video_frames) < schwellenwert_frame_nr_human_attributes:\n",
    "            ethnicity_to_remove.append(ethnicity)\n",
    "\n",
    "    data_ethnicity = data_ethnicity[~data_ethnicity['ethnicity_prediction'].isin(ethnicity_to_remove)]\n",
    "\n",
    "    # New sheet\n",
    "    Ethnicity_sheet = pd.DataFrame(columns=['case', 'count', 'frame_ratio',\n",
    "                                            'avg_object_propotion_asian', 'avg_quadrant_number_asian',\n",
    "                                            'quadrant_numbers_asian',\n",
    "                                            'avg_object_propotion_black', 'avg_quadrant_number_black',\n",
    "                                            'quadrant_numbers_black',\n",
    "                                            'avg_object_propotion_indian', 'avg_quadrant_number_indian',\n",
    "                                            'quadrant_numbers_indian',\n",
    "                                            'avg_object_propotion_latino_hispanic', 'avg_quadrant_number_latino_hispanic',\n",
    "                                            'quadrant_numbers_latino_hispanic',\n",
    "                                            'avg_object_propotion_middle_eastern', 'avg_quadrant_number_middle_eastern',\n",
    "                                            'quadrant_numbers_middle_eastern',\n",
    "                                            'avg_object_propotion_white', 'avg_quadrant_number_white',\n",
    "                                            'quadrant_numbers_white'\n",
    "                                            ])\n",
    "\n",
    "    case_data_ethnicity = {}\n",
    "\n",
    "    # Iterate through the data to determine cases and calculate averages\n",
    "    for frame_key, frame_data in data_ethnicity.groupby('video_frame'):\n",
    "        # Count the occurrences of each ethnicity in the frame\n",
    "        ethnicity_counts = frame_data['ethnicity_prediction'].value_counts()\n",
    "\n",
    "        # Create the case string dynamically based on the ethnicity counts\n",
    "        case = ' & '.join([f'{count} {ethnicity}' for ethnicity, count in sorted(ethnicity_counts.items())])\n",
    "\n",
    "        # Check if the case already exists in the case_data dictionary\n",
    "        if case not in case_data_ethnicity:\n",
    "            case_data_ethnicity[case] = {'count': 0,\n",
    "                              'total_object_propotion_asian': 0, 'total_quadrant_number_asian': 0,\n",
    "                              'quadrant_numbers_asian': [],\n",
    "                              'total_object_propotion_black': 0, 'total_quadrant_number_black': 0,\n",
    "                              'quadrant_numbers_black': [],\n",
    "                              'total_object_propotion_indian': 0, 'total_quadrant_number_indian': 0,\n",
    "                              'quadrant_numbers_indian': [],\n",
    "                              'total_object_propotion_latino_hispanic': 0, 'total_quadrant_number_latino_hispanic': 0,\n",
    "                              'quadrant_numbers_latino_hispanic': [],\n",
    "                              'total_object_propotion_middle_eastern': 0, 'total_quadrant_number_middle_eastern': 0,\n",
    "                              'quadrant_numbers_middle_eastern': [],\n",
    "                              'total_object_propotion_white': 0, 'total_quadrant_number_white': 0,\n",
    "                              'quadrant_numbers_white': []\n",
    "                              }\n",
    "\n",
    "        # Update cumulative values for the case\n",
    "        case_data_ethnicity[case]['count'] += 1\n",
    "        case_data_ethnicity[case]['total_object_propotion_asian'] += frame_data[frame_data['ethnicity_prediction'] == 'asian']['object_propotion'].mean()\n",
    "        case_data_ethnicity[case]['total_quadrant_number_asian'] += frame_data[frame_data['ethnicity_prediction'] == 'asian']['quadrant_number'].mean()\n",
    "        case_data_ethnicity[case]['quadrant_numbers_asian'].extend(frame_data[frame_data['ethnicity_prediction'] == 'asian']['quadrant_number'].tolist())\n",
    "\n",
    "        case_data_ethnicity[case]['total_object_propotion_black'] += frame_data[frame_data['ethnicity_prediction'] == 'black']['object_propotion'].mean()\n",
    "        case_data_ethnicity[case]['total_quadrant_number_black'] += frame_data[frame_data['ethnicity_prediction'] == 'black']['quadrant_number'].mean()\n",
    "        case_data_ethnicity[case]['quadrant_numbers_black'].extend(frame_data[frame_data['ethnicity_prediction'] == 'black']['quadrant_number'].tolist())\n",
    "\n",
    "        case_data_ethnicity[case]['total_object_propotion_indian'] += frame_data[frame_data['ethnicity_prediction'] == 'indian']['object_propotion'].mean()\n",
    "        case_data_ethnicity[case]['total_quadrant_number_indian'] += frame_data[frame_data['ethnicity_prediction'] == 'indian']['quadrant_number'].mean()\n",
    "        case_data_ethnicity[case]['quadrant_numbers_indian'].extend(frame_data[frame_data['ethnicity_prediction'] == 'indian']['quadrant_number'].tolist())\n",
    "\n",
    "        case_data_ethnicity[case]['total_object_propotion_latino_hispanic'] += frame_data[frame_data['ethnicity_prediction'] == 'latino hispanic']['object_propotion'].mean()\n",
    "        case_data_ethnicity[case]['total_quadrant_number_latino_hispanic'] += frame_data[frame_data['ethnicity_prediction'] == 'latino hispanic']['quadrant_number'].mean()\n",
    "        case_data_ethnicity[case]['quadrant_numbers_latino_hispanic'].extend(frame_data[frame_data['ethnicity_prediction'] == 'latino hispanic']['quadrant_number'].tolist())\n",
    "\n",
    "        case_data_ethnicity[case]['total_object_propotion_middle_eastern'] += frame_data[frame_data['ethnicity_prediction'] == 'middle eastern']['object_propotion'].mean()\n",
    "        case_data_ethnicity[case]['total_quadrant_number_middle_eastern'] += frame_data[frame_data['ethnicity_prediction'] == 'middle eastern']['quadrant_number'].mean()\n",
    "        case_data_ethnicity[case]['quadrant_numbers_middle_eastern'].extend(frame_data[frame_data['ethnicity_prediction'] == 'middle eastern']['quadrant_number'].tolist())\n",
    "\n",
    "        case_data_ethnicity[case]['total_object_propotion_white'] += frame_data[frame_data['ethnicity_prediction'] == 'white']['object_propotion'].mean()\n",
    "        case_data_ethnicity[case]['total_quadrant_number_white'] += frame_data[frame_data['ethnicity_prediction'] == 'white']['quadrant_number'].mean()\n",
    "        case_data_ethnicity[case]['quadrant_numbers_white'].extend(frame_data[frame_data['ethnicity_prediction'] == 'white']['quadrant_number'].tolist())\n",
    "\n",
    "    # Calculate averages for each case\n",
    "    for case, values in case_data_ethnicity.items():\n",
    "        count = values['count']\n",
    "\n",
    "        # Calculate averages for asian\n",
    "        avg_object_propotion_asian = round(values['total_object_propotion_asian'] / count, 4) if count > 0 else 0\n",
    "        avg_quadrant_number_asian = round(values['total_quadrant_number_asian'] / count) if count > 0 and not np.isnan(values['total_quadrant_number_asian']) else 0\n",
    "\n",
    "        # Calculate averages for black\n",
    "        avg_object_propotion_black = round(values['total_object_propotion_black'] / count, 4) if count > 0 else 0\n",
    "        avg_quadrant_number_black = round(values['total_quadrant_number_black'] / count) if count > 0 and not np.isnan(values['total_quadrant_number_black']) else 0\n",
    "\n",
    "        # Calculate averages for indian\n",
    "        avg_object_propotion_indian = round(values['total_object_propotion_indian'] / count, 4) if count > 0 else 0\n",
    "        avg_quadrant_number_indian = round(values['total_quadrant_number_indian'] / count) if count > 0 and not np.isnan(values['total_quadrant_number_indian']) else 0\n",
    "\n",
    "        # Calculate averages for latino_hispanic\n",
    "        avg_object_propotion_latino_hispanic = round(values['total_object_propotion_latino_hispanic'] / count, 4) if count > 0 else 0\n",
    "        avg_quadrant_number_latino_hispanic = round(values['total_quadrant_number_latino_hispanic'] / count) if count > 0 and not np.isnan(values['total_quadrant_number_latino_hispanic']) else 0\n",
    "\n",
    "\n",
    "        # Calculate averages for middle_eastern\n",
    "        avg_object_propotion_middle_eastern = round(values['total_object_propotion_middle_eastern'] / count, 4) if count > 0 else 0\n",
    "        avg_quadrant_number_middle_eastern = round(values['total_quadrant_number_middle_eastern'] / count) if count > 0 and not np.isnan(values['total_quadrant_number_middle_eastern']) else 0\n",
    "\n",
    "        # Calculate averages for white\n",
    "        avg_object_propotion_white = round(values['total_object_propotion_white'] / count, 4) if count > 0 else 0\n",
    "        avg_quadrant_number_white = round(values['total_quadrant_number_white'] / count) if count > 0 and not np.isnan(values['total_quadrant_number_white']) else 0\n",
    "\n",
    "        # Calculate frame ratio\n",
    "        total_frame_number = data.iloc[0, 13]\n",
    "        frame_ratio = round(count/ total_frame_number, 4)\n",
    "\n",
    "        # Append the values to the Ethnicity_sheet\n",
    "        Ethnicity_sheet = Ethnicity_sheet.append({\n",
    "            'case': case,\n",
    "            'count': count,\n",
    "            'frame_ratio': frame_ratio,\n",
    "            'avg_object_propotion_asian': avg_object_propotion_asian,\n",
    "            'avg_quadrant_number_asian': avg_quadrant_number_asian,\n",
    "            'quadrant_numbers_asian': values['quadrant_numbers_asian'],\n",
    "            'avg_object_propotion_black': avg_object_propotion_black,\n",
    "            'avg_quadrant_number_black': avg_quadrant_number_black,\n",
    "            'quadrant_numbers_black': values['quadrant_numbers_black'],\n",
    "            'avg_object_propotion_indian': avg_object_propotion_indian,\n",
    "            'avg_quadrant_number_indian': avg_quadrant_number_indian,\n",
    "            'quadrant_numbers_indian': values['quadrant_numbers_indian'],\n",
    "            'avg_object_propotion_latino_hispanic': avg_object_propotion_latino_hispanic,\n",
    "            'avg_quadrant_number_latino_hispanic': avg_quadrant_number_latino_hispanic,\n",
    "            'quadrant_numbers_latino_hispanic': values['quadrant_numbers_latino_hispanic'],\n",
    "            'avg_object_propotion_middle_eastern': avg_object_propotion_middle_eastern,\n",
    "            'avg_quadrant_number_middle_eastern': avg_quadrant_number_middle_eastern,\n",
    "            'quadrant_numbers_middle_eastern': values['quadrant_numbers_middle_eastern'],\n",
    "            'avg_object_propotion_white': avg_object_propotion_white,\n",
    "            'avg_quadrant_number_white': avg_quadrant_number_white,\n",
    "            'quadrant_numbers_white': values['quadrant_numbers_white'],\n",
    "        }, ignore_index=True)\n",
    "\n",
    "    # Replace empty spaces with 0 in the entire dataframe\n",
    "    Ethnicity_sheet[['avg_object_propotion_asian', 'avg_object_propotion_black', 'avg_object_propotion_indian',\n",
    "                    'avg_object_propotion_latino_hispanic', 'avg_object_propotion_middle_eastern', 'avg_object_propotion_white']] = \\\n",
    "        Ethnicity_sheet[['avg_object_propotion_asian', 'avg_object_propotion_black', 'avg_object_propotion_indian',\n",
    "                        'avg_object_propotion_latino_hispanic', 'avg_object_propotion_middle_eastern', 'avg_object_propotion_white']].fillna(0)\n",
    "\n",
    "    Ethnicity_sheet = Ethnicity_sheet.sort_values(by='count', ascending=False)\n",
    "\n",
    "    data.columns = [re.sub(r'Unnamed:\\s*\\d*', '', col) if 'Unnamed' in str(col) else col for col in data.columns]\n",
    "    summary_1.columns = [re.sub(r'Unnamed:\\s*\\d*', '', col) if 'Unnamed' in str(col) else col for col in summary_1.columns]\n",
    "\n",
    "    with pd.ExcelWriter(input_file_path, engine='openpyxl') as writer:\n",
    "        data.to_excel(writer, sheet_name='Predictions', index=False, index_label=None)\n",
    "        summary_1.to_excel(writer, sheet_name='Summary_Objects', index= False, index_label=None)\n",
    "        Gender_sheet.to_excel(writer, sheet_name='Summary_Gender_Ethnicity', index=False)\n",
    "        Ethnicity_sheet.to_excel(writer, sheet_name='Summary_Gender_Ethnicity', startrow=0, startcol=10, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rQ4WyELY0LGb"
   },
   "source": [
    "# 4. Main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zRBXng24xobY"
   },
   "source": [
    "## 4.1 Extracting the Frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "zViBOO78O28Y"
   },
   "outputs": [],
   "source": [
    "# settings\n",
    "fpsextractor = {\n",
    "    \"format\": r\"mp4\", # Video Format of ads within sources.ads folder.\n",
    "    \"yearsToEvaluate\": [2013,2014,2015,2016,2017,2018,2019,2020,2021,2022], # A list of the years which will be analysed.\n",
    "\n",
    "    # fot OpenCV\n",
    "    \"kps\": 3, # Target Keyframes Per Second\n",
    "    \"input\": os.getenv(\"ADS_DIR\"), # Path to the downloaded Ads. The folder this path belongs to must contain subfolgers like \"ADs_IG_2013\" to divide the videos.\n",
    "    \"output\": os.getenv(\"INPUT_FRAMES_ALL\"), # Output path where to store exported images\n",
    "    \"extension\": r\"png\", # File Extension for the exported images\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "LjiFUQKXxsy2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yearsasstring 2013\n",
      "fpsextractorinput /home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/ADs\n",
      "dirname ADs_IG_2015\n",
      "dirname ADs_IG_2017\n",
      "dirname ADs_IG_2020\n",
      "dirname ADs_IG_2019\n",
      "dirname ADs_IG_2018\n",
      "dirname ADs_IG_2022\n",
      "dirname ADs_IG_2013\n",
      "#  2013 ADs_IG_2013\n",
      "AD0254.mp4\n",
      "30\n",
      "AD0277.mp4\n",
      "30\n",
      "AD0264.mp4\n",
      "30\n",
      "AD0298.mp4\n",
      "30\n",
      "AD0269.mp4\n",
      "30\n",
      "AD0273.mp4\n",
      "30\n",
      "AD0266.mp4\n",
      "30\n",
      "AD0292.mp4\n",
      "30\n",
      "AD0290.mp4\n",
      "30\n",
      "AD0282.mp4\n",
      "30\n",
      "AD0291.mp4\n",
      "30\n",
      "AD0284.mp4\n",
      "30\n",
      "AD0293.mp4\n",
      "30\n",
      "AD0275.mp4\n",
      "30\n",
      "AD0262.mp4\n",
      "30\n",
      "AD0286.mp4\n",
      "30\n",
      "AD0278.mp4\n",
      "30\n",
      "AD0300.mp4\n",
      "30\n",
      "AD0297.mp4\n",
      "30\n",
      "AD0276.mp4\n",
      "30\n",
      "AD0267.mp4\n",
      "30\n",
      "AD0263.mp4\n",
      "30\n",
      "AD0280.mp4\n",
      "30\n",
      "AD0256.mp4\n",
      "30\n",
      "AD0272.mp4\n",
      "30\n",
      "AD0295.mp4\n",
      "30\n",
      "AD0253.mp4\n",
      "30\n",
      "AD0299.mp4\n",
      "30\n",
      "AD0270.mp4\n",
      "30\n",
      "AD0288.mp4\n",
      "30\n",
      "AD0252.mp4\n",
      "30\n",
      "AD0265.mp4\n",
      "30\n",
      "AD0296.mp4\n",
      "30\n",
      "AD0271.mp4\n",
      "30\n",
      "AD0287.mp4\n",
      "30\n",
      "AD0268.mp4\n",
      "30\n",
      "AD0285.mp4\n",
      "30\n",
      "AD0294.mp4\n",
      "30\n",
      "AD0279.mp4\n",
      "30\n",
      "AD0260.mp4\n",
      "30\n",
      "AD0289.mp4\n",
      "30\n",
      "AD0258.mp4\n",
      "30\n",
      "AD0261.mp4\n",
      "30\n",
      "AD0259.mp4\n",
      "30\n",
      "AD0255.mp4\n",
      "30\n",
      "AD0281.mp4\n",
      "30\n",
      "AD0257.mp4\n",
      "30\n",
      "dirname ADs_IG_2016\n",
      "dirname ADs_IG_2014\n",
      "dirname ADs_IG_2021\n"
     ]
    }
   ],
   "source": [
    "# Liste der auszuwertenden Jahre aus conf laden und zu Strings umwanzeln.\n",
    "yearsAsString = map(str, fpsextractor['yearsToEvaluate'])\n",
    "yearsAsString = list(yearsAsString)\n",
    "yearsAsString = yearsAsString[0] # TODO Delete nce script works\n",
    "print(\"yearsasstring\",yearsAsString)\n",
    "print(\"fpsextractorinput\", fpsextractor['input'])\n",
    "# Durch Werbevideos iterieren und nur die Jahre auswerten, die in conf hinterlegt sind.\n",
    "for dirpath, dirnames, filenames in os.walk(fpsextractor['input']):\n",
    "    for dirname in dirnames:\n",
    "        print (\"dirname\", dirname)\n",
    "        # wenn aktuell betrachteter Ordner (Jahr) in Config hinterlegt ist, dann auswerten\n",
    "        if (dirname[-4:] in yearsAsString):\n",
    "            print('# ', dirname[-4:], dirname) # print year and folder name\n",
    "\n",
    "            for filename in os.listdir(os.path.join(dirpath, dirname)):\n",
    "                # if filename.endswith('.' + cfg.fpsextractor['format']):\n",
    "                    # print(os.path.join(dirpath, dirname, filename))\n",
    "                    print(filename)\n",
    "                    frame_extraction()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ttUh9YPU-jXC"
   },
   "source": [
    "## 4.2 Creating the output files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "qJbfYC4lHslq"
   },
   "outputs": [],
   "source": [
    "# Path to the folder containing the frames and the excel lists\n",
    "input_folder_path = os.getenv(\"INPUT_FRAMES_ALL\")\n",
    "# output_folder_path = '/content/drive/MyDrive/SuperBowl_Project_FUB/output_lists'\n",
    "output_folder_path = os.getenv(\"OUTPUT_BILD_PLUS_TON_LISTS_DIR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "xS-hNn3wLVPx"
   },
   "outputs": [],
   "source": [
    "# If you want to save the output images then set visualiser to 1\n",
    "visualiser = 0\n",
    "if visualiser:\n",
    "    visualiser_folder_path = os.path.join(output_folder_path, \"visualiser\")\n",
    "    os.makedirs(visualiser_folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "_egy94b8EwJs"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "year detected 2013\n"
     ]
    }
   ],
   "source": [
    "# OPTIONAL: So that the loops starts with the years in an alphabetical order\n",
    "years = []\n",
    "for year in os.listdir(input_folder_path):\n",
    "  print(\"year detected\", year)\n",
    "  years.append(year)\n",
    "years.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "id": "fng7BioEqZjX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output_year_folder_path created /home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/Final_Files/03. Output Bild + Ton/01. output_lists/ADs_IG_2013\n"
     ]
    }
   ],
   "source": [
    "# Creating the output folders\n",
    "for year in years:\n",
    "    output_year_folder_path = os.path.join(output_folder_path, \"ADs_IG_\"+year)\n",
    "    print(\"output_year_folder_path created\", output_year_folder_path)\n",
    "    os.makedirs(output_year_folder_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "id": "iDMlEiWX_0uA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ad_folder_path  /home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/01_input_frames_all/2013/ADs_IG_2013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing AD0252:   0%|          | 0/184 [00:00<?, ?element/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating converter from 3 to 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 17:18:23.051450: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "I0000 00:00:1731428303.051513  322717 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 10289 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4070, pci bus id: 0000:01:00.0, compute capability: 8.9\n",
      "Processing AD0252:   2%|▏         | 4/184 [00:07<04:25,  1.47s/element]I0000 00:00:1731428310.111885  322717 cuda_dnn.cc:529] Loaded cuDNN version 90501\n",
      "Processing AD0252: 100%|██████████| 184/184 [04:20<00:00,  1.42s/element]\n",
      "Processing AD0253: 100%|██████████| 86/86 [02:03<00:00,  1.44s/element]\n",
      "Processing AD0254:  81%|████████▏ | 70/86 [02:02<00:27,  1.74s/element]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[75], line 29\u001b[0m\n\u001b[1;32m     26\u001b[0m     image_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(active_ad_folder_path, element)\n\u001b[1;32m     27\u001b[0m     im \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mimread(image_path)\n\u001b[0;32m---> 29\u001b[0m     prediction_objects \u001b[38;5;241m=\u001b[39m \u001b[43mdetectron2_analysis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m     all_predictions\u001b[38;5;241m.\u001b[39mextend(prediction_objects)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Extending all_predictions with super-categories used for the setting\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# NOTE: The reason why we have used \"try and except\" is because for AD0347 and AD0754 Detectron2 wasn't able to detect any object since only text and a QR-Code was shown in the ad.\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[59], line 2\u001b[0m, in \u001b[0;36mdetectron2_analysis\u001b[0;34m(im)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdetectron2_analysis\u001b[39m(im):\n\u001b[0;32m----> 2\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mpredictor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m     metadata \u001b[38;5;241m=\u001b[39m MetadataCatalog\u001b[38;5;241m.\u001b[39mget(cfg\u001b[38;5;241m.\u001b[39mDATASETS\u001b[38;5;241m.\u001b[39mTEST[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m      4\u001b[0m     class_names \u001b[38;5;241m=\u001b[39m metadata\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthing_classes\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/Development/Commercial-Brand-Differentiating-Message-Analysis/Final_Files/01. Bildanalyse/03. main_Script/detectron2/detectron2/engine/defaults.py:351\u001b[0m, in \u001b[0;36mDefaultPredictor.__call__\u001b[0;34m(self, original_image)\u001b[0m\n\u001b[1;32m    347\u001b[0m image\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mMODEL\u001b[38;5;241m.\u001b[39mDEVICE)\n\u001b[1;32m    349\u001b[0m inputs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m\"\u001b[39m: image, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheight\u001b[39m\u001b[38;5;124m\"\u001b[39m: height, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwidth\u001b[39m\u001b[38;5;124m\"\u001b[39m: width}\n\u001b[0;32m--> 351\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    352\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m predictions\n",
      "File \u001b[0;32m~/Development/Commercial-Brand-Differentiating-Message-Analysis/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1735\u001b[0m     buffers \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_buffers\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffers \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m buffers:\n\u001b[1;32m   1737\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m   1738\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot assign \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtorch\u001b[38;5;241m.\u001b[39mtypename(value)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m as buffer \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1739\u001b[0m                             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(torch.Tensor or None expected)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1740\u001b[0m                             )\n",
      "File \u001b[0;32m~/Development/Commercial-Brand-Differentiating-Message-Analysis/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m     buffers[name] \u001b[38;5;241m=\u001b[39m value\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__setattr__\u001b[39m(name, value)\n",
      "File \u001b[0;32m~/Development/Commercial-Brand-Differentiating-Message-Analysis/Final_Files/01. Bildanalyse/03. main_Script/detectron2/detectron2/modeling/meta_arch/rcnn.py:150\u001b[0m, in \u001b[0;36mGeneralizedRCNN.forward\u001b[0;34m(self, batched_inputs)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;124;03m    batched_inputs: a list, batched outputs of :class:`DatasetMapper` .\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;124;03m        \"pred_boxes\", \"pred_classes\", \"scores\", \"pred_masks\", \"pred_keypoints\"\u001b[39;00m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:\n\u001b[0;32m--> 150\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatched_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    152\u001b[0m images \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess_image(batched_inputs)\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minstances\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m batched_inputs[\u001b[38;5;241m0\u001b[39m]:\n",
      "File \u001b[0;32m~/Development/Commercial-Brand-Differentiating-Message-Analysis/Final_Files/01. Bildanalyse/03. main_Script/detectron2/detectron2/modeling/meta_arch/rcnn.py:208\u001b[0m, in \u001b[0;36mGeneralizedRCNN.inference\u001b[0;34m(self, batched_inputs, detected_instances, do_postprocess)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m detected_instances \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproposal_generator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 208\u001b[0m         proposals, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproposal_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mproposals\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m batched_inputs[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/Development/Commercial-Brand-Differentiating-Message-Analysis/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1735\u001b[0m     buffers \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_buffers\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffers \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m buffers:\n\u001b[1;32m   1737\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m   1738\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot assign \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtorch\u001b[38;5;241m.\u001b[39mtypename(value)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m as buffer \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1739\u001b[0m                             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(torch.Tensor or None expected)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1740\u001b[0m                             )\n",
      "File \u001b[0;32m~/Development/Commercial-Brand-Differentiating-Message-Analysis/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m     buffers[name] \u001b[38;5;241m=\u001b[39m value\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__setattr__\u001b[39m(name, value)\n",
      "File \u001b[0;32m~/Development/Commercial-Brand-Differentiating-Message-Analysis/Final_Files/01. Bildanalyse/03. main_Script/detectron2/detectron2/modeling/proposal_generator/rpn.py:454\u001b[0m, in \u001b[0;36mRPN.forward\u001b[0;34m(self, images, features, gt_instances)\u001b[0m\n\u001b[1;32m    451\u001b[0m features \u001b[38;5;241m=\u001b[39m [features[f] \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_features]\n\u001b[1;32m    452\u001b[0m anchors \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39manchor_generator(features)\n\u001b[0;32m--> 454\u001b[0m pred_objectness_logits, pred_anchor_deltas \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrpn_head\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[38;5;66;03m# Transpose the Hi*Wi*A dimension to the middle:\u001b[39;00m\n\u001b[1;32m    456\u001b[0m pred_objectness_logits \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    457\u001b[0m     \u001b[38;5;66;03m# (N, A, Hi, Wi) -> (N, Hi, Wi, A) -> (N, Hi*Wi*A)\u001b[39;00m\n\u001b[1;32m    458\u001b[0m     score\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mflatten(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    459\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m score \u001b[38;5;129;01min\u001b[39;00m pred_objectness_logits\n\u001b[1;32m    460\u001b[0m ]\n",
      "File \u001b[0;32m~/Development/Commercial-Brand-Differentiating-Message-Analysis/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1735\u001b[0m     buffers \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_buffers\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffers \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m buffers:\n\u001b[1;32m   1737\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m   1738\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot assign \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtorch\u001b[38;5;241m.\u001b[39mtypename(value)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m as buffer \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1739\u001b[0m                             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(torch.Tensor or None expected)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1740\u001b[0m                             )\n",
      "File \u001b[0;32m~/Development/Commercial-Brand-Differentiating-Message-Analysis/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m     buffers[name] \u001b[38;5;241m=\u001b[39m value\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__setattr__\u001b[39m(name, value)\n",
      "File \u001b[0;32m~/Development/Commercial-Brand-Differentiating-Message-Analysis/Final_Files/01. Bildanalyse/03. main_Script/detectron2/detectron2/modeling/proposal_generator/rpn.py:174\u001b[0m, in \u001b[0;36mStandardRPNHead.forward\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m    172\u001b[0m pred_anchor_deltas \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m features:\n\u001b[0;32m--> 174\u001b[0m     t \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    175\u001b[0m     pred_objectness_logits\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjectness_logits(t))\n\u001b[1;32m    176\u001b[0m     pred_anchor_deltas\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39manchor_deltas(t))\n",
      "File \u001b[0;32m~/Development/Commercial-Brand-Differentiating-Message-Analysis/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1735\u001b[0m     buffers \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_buffers\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffers \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m buffers:\n\u001b[1;32m   1737\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m   1738\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot assign \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtorch\u001b[38;5;241m.\u001b[39mtypename(value)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m as buffer \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1739\u001b[0m                             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(torch.Tensor or None expected)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1740\u001b[0m                             )\n",
      "File \u001b[0;32m~/Development/Commercial-Brand-Differentiating-Message-Analysis/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m     buffers[name] \u001b[38;5;241m=\u001b[39m value\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__setattr__\u001b[39m(name, value)\n",
      "File \u001b[0;32m~/Development/Commercial-Brand-Differentiating-Message-Analysis/Final_Files/01. Bildanalyse/03. main_Script/detectron2/detectron2/layers/wrappers.py:142\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    136\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:\n\u001b[1;32m    137\u001b[0m                 \u001b[38;5;66;03m# https://github.com/pytorch/pytorch/issues/12013\u001b[39;00m\n\u001b[1;32m    138\u001b[0m                 \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[1;32m    139\u001b[0m                     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm, torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mSyncBatchNorm\n\u001b[1;32m    140\u001b[0m                 ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSyncBatchNorm does not support empty inputs!\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 142\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    146\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(x)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Creating the output excel files\n",
    "for year in years:\n",
    "    # Save the ad folders in a list\n",
    "    ad_folder_path = os.path.join(input_folder_path, year, \"ADs_IG_\" + year)\n",
    "    print(\"ad_folder_path \", ad_folder_path)\n",
    "    all_ADs = os.listdir(ad_folder_path)\n",
    "    AD_names = [item for item in all_ADs if os.path.isdir(os.path.join(ad_folder_path, item))]\n",
    "    sorted_AD_names = sorted(AD_names)\n",
    "\n",
    "    # Run the code for each ad\n",
    "    while len(sorted_AD_names) > 0:\n",
    "        all_predictions = []\n",
    "        all_frames = []\n",
    "        current_AD = sorted_AD_names.pop(0)\n",
    "        active_ad_folder_path = os.path.join(ad_folder_path, current_AD)\n",
    "\n",
    "        if visualiser:\n",
    "            visualiser_folder_path_ad = os.path.join(visualiser_folder_path, current_AD)\n",
    "            os.makedirs(visualiser_folder_path_ad)\n",
    "\n",
    "        for frame in os.listdir(active_ad_folder_path):\n",
    "            if frame.endswith(\".png\"):\n",
    "                all_frames.append(frame)\n",
    "\n",
    "        for element in tqdm(all_frames, desc=f\"Processing {current_AD}\", unit=\"element\"):\n",
    "            image_path = os.path.join(active_ad_folder_path, element)\n",
    "            im = cv2.imread(image_path)\n",
    "\n",
    "            prediction_objects = detectron2_analysis(im)\n",
    "            all_predictions.extend(prediction_objects)\n",
    "\n",
    "        # Extending all_predictions with super-categories used for the setting\n",
    "        # NOTE: The reason why we have used \"try and except\" is because for AD0347 and AD0754 Detectron2 wasn't able to detect any object since only text and a QR-Code was shown in the ad.\n",
    "        all_predictions = pd.DataFrame(all_predictions)\n",
    "        try:\n",
    "          all_predictions_super_categories = all_predictions.join(super_categories_df, on=\"class_name\")\n",
    "        except:\n",
    "          all_predictions_super_categories = all_predictions\n",
    "\n",
    "        # Calculate total frame number\n",
    "        total_frame_number = len([file for file in os.listdir(active_ad_folder_path) if file.lower().endswith('.png')])\n",
    "\n",
    "        # To Excel\n",
    "        output_year_folder_path = os.path.join(output_folder_path,\"ADs_IG_\"+year)\n",
    "        AD_name = current_AD + \".xlsx\"\n",
    "        output_file = os.path.join(output_year_folder_path, AD_name)\n",
    "        with pd.ExcelWriter(output_file, engine='openpyxl') as writer:\n",
    "            all_predictions_super_categories.to_excel(writer, sheet_name='Predictions', index=False)\n",
    "\n",
    "            # Add total frame number to the Excel sheet\n",
    "            sheet = writer.sheets['Predictions']\n",
    "            sheet[f'N1'] = 'Total Frame Number'\n",
    "            sheet[f'N2'] = total_frame_number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZxW9ZWIG-oVB"
   },
   "source": [
    "## 4.3 Creating the summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 923371,
     "status": "ok",
     "timestamp": 1706472490607,
     "user": {
      "displayName": "Flavio Kuka",
      "userId": "09654499042520950340"
     },
     "user_tz": -60
    },
    "id": "_2oBeL_A-wlg",
    "outputId": "93833656-d0d1-47cc-caf4-3825e7fa8dd0"
   },
   "outputs": [],
   "source": [
    "# NOTE: The Threshold values were determined manually by checking the predictions for the frame_count = [3, 6, 9, 12, 15] and proportion = [5%, 10%, 15%]\n",
    "\n",
    "schwellenwert_frame_nr_human_attributes = 9\n",
    "schwellenwert_frame_nr_detectron_2 = 6\n",
    "schwellenwert_proportion_human_attributes = 0.05\n",
    "schwellenwert_proportion_detectron_2 = 0\n",
    "\n",
    "for year in os.listdir(output_folder_path):\n",
    "  print(\"Output folder path\", output_folder_path)\n",
    "  print(f\"Processing year {year}\")\n",
    "  ad_folder_path = os.path.join(output_folder_path, year)\n",
    "  print(\"AD folder path\", ad_folder_path)\n",
    "  for ad in os.listdir(ad_folder_path):\n",
    "      if ad.endswith(\".xlsx\"):\n",
    "          input_file_path = os.path.join(ad_folder_path, ad)\n",
    "          try:\n",
    "            generate_summary(input_file_path)\n",
    "            generate_summary_gender_ethnicity(input_file_path)\n",
    "          except:\n",
    "            print(f\"Summary for ad {ad} couldn't get created\")\n",
    "            continue"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "1EqcmIwFuXCZ",
    "ZJtISBlJ8X7N",
    "hpjg6z9J6lEw",
    "LIBLSDV_xbfl",
    "UniRLoRE7mFp",
    "zRBXng24xobY",
    "ttUh9YPU-jXC"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
