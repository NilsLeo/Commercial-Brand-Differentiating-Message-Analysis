{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dE96Mw7hUjlE"
   },
   "source": [
    "#**Automatic Audio Recognition**\n",
    "This script includes 3 different models which analyze different parts of audio in super bowl ads.\n",
    "1. *Gender specific speaking time* (and durations of speaking parts)\n",
    "2. *Emotion recognition from Transcription* (uses only transcription from WhisperAI for analysis)\n",
    "3. *Acoustic Indizes* (many different indicators like min/max_energy, db and tempo)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "54LsLpyqT-lM"
   },
   "source": [
    "# *1. Gender specific speaking time*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jLzGh7XjtR9V"
   },
   "source": [
    "Initially, the audio file will be segmented before gender recognition can take place\n",
    "\n",
    "Audio segmentation: https://github.com/pyannote/pyannote-audio\n",
    "\n",
    "Gender recognition: https://github.com/x4nth055/gender-recognition-by-voice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tckHJKZnYnp7"
   },
   "source": [
    "### Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 115024,
     "status": "ok",
     "timestamp": 1708638592563,
     "user": {
      "displayName": "Thorben",
      "userId": "17213772383075988098"
     },
     "user_tz": -60
    },
    "id": "IL25VsVSgzOw",
    "outputId": "39d1663c-2a19-4910-d1cb-72952e68ea61"
   },
   "outputs": [],
   "source": [
    "from pydub import AudioSegment\n",
    "from tqdm import tqdm\n",
    "from pyannote.audio import Pipeline\n",
    "import torch\n",
    "import pyaudio\n",
    "import os\n",
    "import wave\n",
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "import locale\n",
    "import tensorflow as tf\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"  # Force CPU usage\n",
    "\n",
    "\n",
    "# sed -i 's/np\\.NaN/np\\.nan/g' ~/Development/Commercial-Brand-Differentiating-Message-Analysis/venv/lib/python3.12/site-packages/pyannote/audio/core/inference.py\n",
    "def getpreferredencoding(do_setlocale = True):\n",
    "    return \"UTF-8\"\n",
    "locale.getpreferredencoding = getpreferredencoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22883,
     "status": "ok",
     "timestamp": 1708638620870,
     "user": {
      "displayName": "Thorben",
      "userId": "17213772383075988098"
     },
     "user_tz": -60
    },
    "id": "YPbio2axS-_m",
    "outputId": "4a7aa1b1-dbdc-4e6b-8c6b-b01c6a8174e3"
   },
   "outputs": [],
   "source": [
    "# #Optional\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5MclWK2GYnp_"
   },
   "source": [
    "### Mandatory Login\n",
    "\n",
    "To load the speaker diarization pipeline,\n",
    "\n",
    "* accept the user conditions on [hf.co/pyannote/speaker-diarization-3.1](https://hf.co/pyannote/speaker-diarization-3.1)\n",
    "* accept the user conditions on [hf.co/pyannote/segmentation-3.0](https://hf.co/pyannote/segmentation-3.0)\n",
    "* login using `notebook_login` below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "id": "jh3EV9Y2wFXC"
   },
   "outputs": [],
   "source": [
    "# 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"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 145,
     "referenced_widgets": [
      "d0647072ab8044c7a1ff53b9e01f7189",
      "2d52b4a045c14f44903b010261527ed6",
      "3ff01220d0d449f99775a49369910f63",
      "34222225ba97456694d7298c879b4b19",
      "d9bce56fe264426b85e729cf46a3973f",
      "6c2729e3f4b84d349a112da2ccf77b25",
      "35ae3d30cf45414a98a606b2c9ba40be",
      "504a095f33214bd4b36646d088b49e5e",
      "a9bf397a62944000ae68a482a8ae20c1",
      "aa5d8610fb084fa59991f262fed05bad",
      "a5865557a7474b0ebeed270b592f38ac",
      "45d6c82b2ec74893ae2b5a86eeb1daff",
      "e9be5a21ad9046d181c258ea1487de5c",
      "8a7f1963cc7848a48b2c8a5ab1ff17a3"
     ]
    },
    "executionInfo": {
     "elapsed": 284,
     "status": "ok",
     "timestamp": 1708638630457,
     "user": {
      "displayName": "Thorben",
      "userId": "17213772383075988098"
     },
     "user_tz": -60
    },
    "id": "r5u7VMb-YnqB",
    "outputId": "1ddafd98-b25a-4ac0-b544-0b2a0419522e"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from huggingface_hub import login\n",
    "\n",
    "# Load environment variables from the .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Retrieve the Hugging Face API token from the environment\n",
    "hf_token = os.getenv(\"HF_API_KEY\")\n",
    "# Log in to Hugging Face\n",
    "login(token=hf_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tjNgvcnOWRIF"
   },
   "source": [
    "## Audio Splitter Method Definition\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4eqTRtgxgc0A"
   },
   "source": [
    "### Utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "id": "tRqtCGZegfjr"
   },
   "outputs": [],
   "source": [
    "from sys import byteorder\n",
    "from array import array\n",
    "from struct import pack\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "label2int = {\n",
    "    \"male\": 1,\n",
    "    \"female\": 0\n",
    "}\n",
    "\n",
    "\n",
    "def load_data(vector_length=128):\n",
    "    \"\"\"A function to load gender recognition dataset from `data` folder\n",
    "    After the second run, this will load from results/features.npy and results/labels.npy files\n",
    "    as it is much faster!\"\"\"\n",
    "    # make sure results folder exists\n",
    "    if not os.path.isdir(\"results\"):\n",
    "        os.mkdir(\"results\")\n",
    "    # if features & labels already loaded individually and bundled, load them from there instead\n",
    "    if os.path.isfile(os.path.join(os.getenv(\"TONANALYSE_AUDIO_GENDER_NOTEBOOKS_DIR\"), \"results/features.npy\")) and os.path.isfile(\"results/labels.npy\"):\n",
    "        X = np.load(os.path.join(os.getenv(\"TONANALYSE_AUDIO_GENDER_NOTEBOOKS_DIR\"), \"results/features.npy\"))\n",
    "        y = np.load(os.path.join(os.getenv(\"TONANALYSE_AUDIO_GENDER_NOTEBOOKS_DIR\"), \"results/labels.npy\"))\n",
    "        return X, y\n",
    "    # read dataframe\n",
    "    df = pd.read_csv(os.path.join(os.getenv(\"TONANALYSE_AUDIO_GENDER_NOTEBOOKS_DIR\"), \"balanced-all.csv\"))\n",
    "    # get total samples\n",
    "    n_samples = len(df)\n",
    "    # get total male samples\n",
    "    n_male_samples = len(df[df['gender'] == 'male'])\n",
    "    # get total female samples\n",
    "    n_female_samples = len(df[df['gender'] == 'female'])\n",
    "    print(\"Total samples:\", n_samples)\n",
    "    print(\"Total male samples:\", n_male_samples)\n",
    "    print(\"Total female samples:\", n_female_samples)\n",
    "    # initialize an empty array for all audio features\n",
    "    X = np.zeros((n_samples, vector_length))\n",
    "    # initialize an empty array for all audio labels (1 for male and 0 for female)\n",
    "    y = np.zeros((n_samples, 1))\n",
    "    for i, (filename, gender) in tqdm.tqdm(enumerate(zip(df['filename'], df['gender'])), \"Loading data\", total=n_samples):\n",
    "        features = np.load(filename)\n",
    "        X[i] = features\n",
    "        y[i] = label2int[gender]\n",
    "    # save the audio features and labels into files\n",
    "    # so we won't load each one of them next run\n",
    "    np.save(os.path.join(os.getenv(\"TONANALYSE_AUDIO_GENDER_NOTEBOOKS_DIR\"), \"results/features\"), X)\n",
    "    np.save(os.path.join(os.getenv(\"TONANALYSE_AUDIO_GENDER_NOTEBOOKS_DIR\"), \"results/\"), y)\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def split_data(X, y, test_size=0.1, valid_size=0.1):\n",
    "    # split training set and testing set\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=7)\n",
    "    # split training set and validation set\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=valid_size, random_state=7)\n",
    "    # return a dictionary of values\n",
    "    return {\n",
    "        \"X_train\": X_train,\n",
    "        \"X_valid\": X_valid,\n",
    "        \"X_test\": X_test,\n",
    "        \"y_train\": y_train,\n",
    "        \"y_valid\": y_valid,\n",
    "        \"y_test\": y_test\n",
    "    }\n",
    "\n",
    "import tensorflow as tf\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "  except RuntimeError as e:\n",
    "    print(e)\n",
    "def create_model(vector_length=128):\n",
    "    print(\"creating model ...\")\n",
    "    \"\"\"5 hidden dense layers from 256 units to 64, not the best model, but not bad.\"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(Dense(256, input_shape=(vector_length,)))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(256, activation=\"relu\"))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(128, activation=\"relu\"))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(128, activation=\"relu\"))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(64, activation=\"relu\"))\n",
    "    model.add(Dropout(0.3))\n",
    "    # one output neuron with sigmoid activation function, 0 means female, 1 means male\n",
    "    model.add(Dense(1, activation=\"sigmoid\"))\n",
    "    # using binary crossentropy as it's male/female classification (binary)\n",
    "    model.compile(loss=\"binary_crossentropy\", metrics=[\"accuracy\"], optimizer=\"adam\")\n",
    "    # print summary of the model\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jkH61xIZgWXp"
   },
   "source": [
    "### Test Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "id": "7gWd9jx8WX3r"
   },
   "outputs": [],
   "source": [
    "THRESHOLD = 500\n",
    "CHUNK_SIZE = 1024\n",
    "FORMAT = pyaudio.paInt16\n",
    "RATE = 16000\n",
    "\n",
    "SILENCE = 30\n",
    "\n",
    "def is_silent(snd_data):\n",
    "    \"Returns 'True' if below the 'silent' threshold\"\n",
    "    return max(snd_data) < THRESHOLD\n",
    "\n",
    "def normalize(snd_data):\n",
    "    \"Average the volume out\"\n",
    "    MAXIMUM = 16384\n",
    "    times = float(MAXIMUM)/max(abs(i) for i in snd_data)\n",
    "\n",
    "    r = array('h')\n",
    "    for i in snd_data:\n",
    "        r.append(int(i*times))\n",
    "    return r\n",
    "\n",
    "def trim(snd_data):\n",
    "    \"Trim the blank spots at the start and end\"\n",
    "    def _trim(snd_data):\n",
    "        snd_started = False\n",
    "        r = array('h')\n",
    "\n",
    "        for i in snd_data:\n",
    "            if not snd_started and abs(i)>THRESHOLD:\n",
    "                snd_started = True\n",
    "                r.append(i)\n",
    "\n",
    "            elif snd_started:\n",
    "                r.append(i)\n",
    "        return r\n",
    "\n",
    "    # Trim to the left\n",
    "    snd_data = _trim(snd_data)\n",
    "\n",
    "    # Trim to the right\n",
    "    snd_data.reverse()\n",
    "    snd_data = _trim(snd_data)\n",
    "    snd_data.reverse()\n",
    "    return snd_data\n",
    "\n",
    "def add_silence(snd_data, seconds):\n",
    "    \"Add silence to the start and end of 'snd_data' of length 'seconds' (float)\"\n",
    "    r = array('h', [0 for i in range(int(seconds*RATE))])\n",
    "    r.extend(snd_data)\n",
    "    r.extend([0 for i in range(int(seconds*RATE))])\n",
    "    return r\n",
    "\n",
    "\n",
    "def extract_feature(file_name, **kwargs):\n",
    "    \"\"\"\n",
    "    Extract feature from audio file `file_name`\n",
    "        Features supported:\n",
    "            - MFCC (mfcc)\n",
    "            - Chroma (chroma)\n",
    "            - MEL Spectrogram Frequency (mel)\n",
    "            - Contrast (contrast)\n",
    "            - Tonnetz (tonnetz)\n",
    "        e.g:\n",
    "        `features = extract_feature(path, mel=True, mfcc=True)`\n",
    "    \"\"\"\n",
    "    mfcc = kwargs.get(\"mfcc\")\n",
    "    chroma = kwargs.get(\"chroma\")\n",
    "    mel = kwargs.get(\"mel\")\n",
    "    contrast = kwargs.get(\"contrast\")\n",
    "    tonnetz = kwargs.get(\"tonnetz\")\n",
    "    X, sample_rate = librosa.core.load(file_name)\n",
    "    if chroma or contrast:\n",
    "        stft = np.abs(librosa.stft(X))\n",
    "    result = np.array([])\n",
    "    if mfcc:\n",
    "        mfccs = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=40).T, axis=0)\n",
    "        result = np.hstack((result, mfccs))\n",
    "    if chroma:\n",
    "        chroma = np.mean(librosa.feature.chroma_stft(S=stft, sr=sample_rate).T,axis=0)\n",
    "        result = np.hstack((result, chroma))\n",
    "    if mel:\n",
    "        mel = np.mean(librosa.feature.melspectrogram(y=X, sr=sample_rate).T,axis=0)\n",
    "        result = np.hstack((result, mel))\n",
    "    if contrast:\n",
    "        contrast = np.mean(librosa.feature.spectral_contrast(S=stft, sr=sample_rate).T,axis=0)\n",
    "        result = np.hstack((result, contrast))\n",
    "    if tonnetz:\n",
    "        tonnetz = np.mean(librosa.feature.tonnetz(y=librosa.effects.harmonic(X), sr=sample_rate).T,axis=0)\n",
    "        result = np.hstack((result, tonnetz))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "id": "ikiAX5Mm29yi"
   },
   "outputs": [],
   "source": [
    "from openpyxl import load_workbook\n",
    "\n",
    "\n",
    "def audio_splitter(input_folder, output_base_folder, output_base_excel_folder):\n",
    "    print(\"Starting audio splitting...\")\n",
    "    # Construct the model\n",
    "    model = create_model()\n",
    "    model.load_weights(f\"{os.getenv('TONANALYSE_AUDIO_GENDER_NOTEBOOKS_DIR')}/results/model.h5\")\n",
    "    # Iterate through each folder from ADs_IG_2013_wav to ADs_IG_2022_wav\n",
    "    years = os.getenv(\"YEARS\").split(\" \")\n",
    "\n",
    "    for folder_name in years:\n",
    "     \n",
    "    # for folder_name in range(2013, 2023):\n",
    "        input_path = f\"{input_folder}/ADs_IG_{folder_name}\"\n",
    "        output_folder = f\"{output_base_folder}/ADs_IG_{folder_name}\"\n",
    "        excel_output_folder = f\"{output_base_excel_folder}/ADs_IG_{folder_name}\"\n",
    "        \n",
    "        os.makedirs(output_folder, exist_ok=True)\n",
    "        files = [f for f in os.listdir(input_path) if f.endswith('.wav')]\n",
    "        print(\"files\", files)\n",
    "        files.sort()\n",
    "        print(\"files\", files)\n",
    "\n",
    "        # Iterate through each file in the current folder\n",
    "        for file_name in files:\n",
    "                audio_path = f\"{input_path}/{file_name}\"  # Path to the audio file\n",
    "                print(f\"P {audio_path}\")\n",
    "                audio = AudioSegment.from_wav(audio_path)\n",
    "\n",
    "                result_per_audio = []\n",
    "\n",
    "                # Initialize an empty list to store the segmented audio\n",
    "                voice_timestamp = []\n",
    "                segment_index = 0\n",
    "\n",
    "                # Load audio for diarization\n",
    "                own_file = {'audio': audio_path}  # Provide the audio file path\n",
    "                pipeline = Pipeline.from_pretrained('pyannote/speaker-diarization-3.1', use_auth_token=True)\n",
    "                if torch.cuda.is_available():\n",
    "                    pipeline.to(torch.device('cuda'))\n",
    "\n",
    "\n",
    "                from pyannote.audio.pipelines.utils.hook import ProgressHook\n",
    "                with ProgressHook() as hook:\n",
    "                    diarization = pipeline(own_file, hook=hook)\n",
    "\n",
    "\n",
    "                # Segmentation logic\n",
    "                for turn, _, speaker in diarization.itertracks(yield_label=True):\n",
    "                    segment_id = f\"segment_{segment_index}\"\n",
    "                    voice_timestamp.append((segment_id, turn.start, turn.end, speaker))\n",
    "                    segment_index += 1\n",
    "\n",
    "\n",
    "                # Loop through the voice_timestamp list and split the audio\n",
    "                for idx, (_, start, stop, speaker) in enumerate(voice_timestamp):\n",
    "                    segment = audio[start * 1000: stop * 1000]  # Extract the segment in milliseconds\n",
    "                    output_subfolder = f\"{output_folder}/{os.path.splitext(file_name)[0]}\"  # Create subfolder based on file name\n",
    "                    os.makedirs(output_subfolder, exist_ok=True)  # Create subfolder if it doesn't exist\n",
    "\n",
    "                    if((stop - start) < 0.05):\n",
    "                      print(f\"segment_{idx}.wav is too short!\")\n",
    "                      continue\n",
    "                    else:\n",
    "                      segment.export(f\"{output_subfolder}/segment_{idx}.wav\", format=\"wav\")\n",
    "\n",
    "                    # Extract features and reshape it (assuming you have an extract_feature function)\n",
    "                    features = extract_feature(f\"{output_subfolder}/segment_{idx}.wav\", format=\"wav\", mel=True).reshape(1, -1)\n",
    "\n",
    "                    # Predict the gender\n",
    "                    male_prob = model.predict(features)[0][0]\n",
    "                    female_prob = 1 - male_prob\n",
    "                    gender = \"male\" if male_prob > female_prob else \"female\"\n",
    "\n",
    "\n",
    "                    # Store segment information in the list\n",
    "                    duration = stop - start\n",
    "                    segID = f\"segment_{idx}.wav\"\n",
    "                    result_per_audio.append((file_name, segID, start,stop, duration, speaker, gender, male_prob, female_prob))\n",
    "\n",
    "\n",
    "                for result_tuple in result_per_audio:\n",
    "                  file_name, segID, turn_start, turn_end, duration, speaker, gender, male_prob, female_prob = result_tuple\n",
    "                  print(f\"file_name: {file_name}, segment_id: {segID}, turn.start: {turn_start}, turn.end: {turn_end}, duration: {duration}, speaker: {speaker}, gender: {gender}, male_prob: {male_prob}, female_prob: {female_prob}\")\n",
    "\n",
    "\n",
    "\n",
    "                # Create DataFrame from result_per_audio\n",
    "                df = pd.DataFrame(result_per_audio, columns=['file_name', 'segment_id', 'start', 'end', 'duration', 'speaker', 'gender', 'male_prob', 'female_prob'])\n",
    "\n",
    "                # Create directory for saving Excel file\n",
    "                excel_output_folder = f\"{output_base_excel_folder}/ADs_IG_{folder_name}\" #.wav\n",
    "                os.makedirs(excel_output_folder, exist_ok=True)  # Create output directory if it doesn't exist\n",
    "\n",
    "                # Save DataFrame to Excel\n",
    "                excel_file_path = f\"{excel_output_folder}/{os.path.splitext(file_name)[0]}.xlsx\"  # Excel file path\n",
    "                print(f\"Excel file path: {excel_file_path}\")\n",
    "                # Check if Excel file already exists\n",
    "\n",
    "\n",
    "                try:\n",
    "                    # Try to write to existing file\n",
    "                    with pd.ExcelWriter(excel_file_path, engine='openpyxl', mode='a', if_sheet_exists='replace') as writer:\n",
    "                        df.to_excel(writer, sheet_name='Gender_speaking_time', index=False)\n",
    "                        print(f\"Writing to {excel_file_path} completed!\") \n",
    "                except:\n",
    "                    # If file is corrupted or doesn't exist, create new file\n",
    "                    df.to_excel(excel_file_path, sheet_name='Gender_speaking_time', index=False, engine='openpyxl')\n",
    "                    print(f\"Writing to {excel_file_path} completed!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7V6Uy8EU-umc"
   },
   "source": [
    "### Main\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "c9ff155a579140e2bb19bfbadc4e032f",
      "4cbb11e0ee474f2eaf7ffa7e3583e9f8"
     ]
    },
    "executionInfo": {
     "elapsed": 14257,
     "status": "ok",
     "timestamp": 1708647221477,
     "user": {
      "displayName": "Thorben",
      "userId": "17213772383075988098"
     },
     "user_tz": -60
    },
    "id": "Mow9w3JKCPXh",
    "outputId": "e68d7dc1-90c8-4ad0-dbe6-b161d17297fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_folder /home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/ADs\n",
      "Starting audio splitting...\n",
      "creating model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/Final_Files/02. Tonanalyse/main_sound_recognition_FINAL-venv/lib/python3.12/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_6\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_6\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_36 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">33,024</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_30 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_37 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">65,792</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_31 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_38 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_32 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_39 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">16,512</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_33 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_40 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_34 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_41 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_36 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │        \u001b[38;5;34m33,024\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_30 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_37 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │        \u001b[38;5;34m65,792\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_31 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_38 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m32,896\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_32 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_39 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m16,512\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_33 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_40 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m8,256\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_34 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_41 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m65\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">156,545</span> (611.50 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m156,545\u001b[0m (611.50 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">156,545</span> (611.50 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m156,545\u001b[0m (611.50 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "files ['AD0253.wav']\n",
      "files ['AD0253.wav']\n",
      "P /home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/ADs/ADs_IG_2013/AD0253.wav\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2872ddb83634607aa574e9bee5105ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/Final_Files/02. \n",
       "Tonanalyse/main_sound_recognition_FINAL-venv/lib/python3.12/site-packages/pyannote/audio/models/blocks/pooling.py:1\n",
       "04: UserWarning: std(): degrees of freedom is &lt;= 0. Correction should be strictly less than the reduction factor \n",
       "(input numel divided by output numel). (Triggered internally at ../aten/src/ATen/native/ReduceOps.cpp:1823.)\n",
       "  std = sequences.std(dim=-1, correction=1)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/Final_Files/02. \n",
       "Tonanalyse/main_sound_recognition_FINAL-venv/lib/python3.12/site-packages/pyannote/audio/models/blocks/pooling.py:1\n",
       "04: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor \n",
       "(input numel divided by output numel). (Triggered internally at ../aten/src/ATen/native/ReduceOps.cpp:1823.)\n",
       "  std = sequences.std(dim=-1, correction=1)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "file_name: AD0253.wav, segment_id: segment_0.wav, turn.start: 22.79534375, turn.end: 28.81971875, duration: 6.024374999999999, speaker: SPEAKER_00, gender: male, male_prob: 0.930648922920227, female_prob: 0.06935107707977295\n",
      "Excel file path: /home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/Final_Files/03. Output Bild + Ton/01. output_lists/ADs_IG_2013/AD0253.xlsx\n",
      "Writing to /home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/Final_Files/03. Output Bild + Ton/01. output_lists/ADs_IG_2013/AD0253.xlsx completed!\n",
      "files ['AD0301.wav']\n",
      "files ['AD0301.wav']\n",
      "P /home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/ADs/ADs_IG_2014/AD0301.wav\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b27604596f054ceda631e2175e1dfc88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/Final_Files/02. \n",
       "Tonanalyse/main_sound_recognition_FINAL-venv/lib/python3.12/site-packages/pyannote/audio/models/blocks/pooling.py:1\n",
       "04: UserWarning: std(): degrees of freedom is &lt;= 0. Correction should be strictly less than the reduction factor \n",
       "(input numel divided by output numel). (Triggered internally at ../aten/src/ATen/native/ReduceOps.cpp:1823.)\n",
       "  std = sequences.std(dim=-1, correction=1)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/Final_Files/02. \n",
       "Tonanalyse/main_sound_recognition_FINAL-venv/lib/python3.12/site-packages/pyannote/audio/models/blocks/pooling.py:1\n",
       "04: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor \n",
       "(input numel divided by output numel). (Triggered internally at ../aten/src/ATen/native/ReduceOps.cpp:1823.)\n",
       "  std = sequences.std(dim=-1, correction=1)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
      "file_name: AD0301.wav, segment_id: segment_0.wav, turn.start: 3.4397187500000004, turn.end: 6.61221875, duration: 3.1725, speaker: SPEAKER_00, gender: male, male_prob: 0.9143445491790771, female_prob: 0.08565545082092285\n",
      "file_name: AD0301.wav, segment_id: segment_1.wav, turn.start: 8.974718750000001, turn.end: 13.63221875, duration: 4.657499999999999, speaker: SPEAKER_00, gender: female, male_prob: 0.30014267563819885, female_prob: 0.6998573243618011\n",
      "file_name: AD0301.wav, segment_id: segment_2.wav, turn.start: 14.30721875, turn.end: 15.47159375, duration: 1.1643749999999997, speaker: SPEAKER_00, gender: male, male_prob: 0.5567774772644043, female_prob: 0.4432225227355957\n",
      "file_name: AD0301.wav, segment_id: segment_3.wav, turn.start: 15.79221875, turn.end: 18.40784375, duration: 2.6156250000000014, speaker: SPEAKER_00, gender: female, male_prob: 0.267233669757843, female_prob: 0.732766330242157\n",
      "file_name: AD0301.wav, segment_id: segment_4.wav, turn.start: 19.01534375, turn.end: 19.69034375, duration: 0.6750000000000007, speaker: SPEAKER_00, gender: male, male_prob: 0.9456297159194946, female_prob: 0.05437028408050537\n",
      "file_name: AD0301.wav, segment_id: segment_5.wav, turn.start: 19.94346875, turn.end: 20.83784375, duration: 0.8943750000000001, speaker: SPEAKER_00, gender: male, male_prob: 0.9671359658241272, female_prob: 0.0328640341758728\n",
      "file_name: AD0301.wav, segment_id: segment_6.wav, turn.start: 28.296593750000003, turn.end: 29.309093750000002, duration: 1.0124999999999993, speaker: SPEAKER_00, gender: male, male_prob: 0.9488357305526733, female_prob: 0.05116426944732666\n",
      "file_name: AD0301.wav, segment_id: segment_7.wav, turn.start: 29.54534375, turn.end: 30.777218750000003, duration: 1.2318750000000023, speaker: SPEAKER_00, gender: male, male_prob: 0.8840404748916626, female_prob: 0.1159595251083374\n",
      "file_name: AD0301.wav, segment_id: segment_8.wav, turn.start: 31.43534375, turn.end: 32.29596875, duration: 0.8606249999999989, speaker: SPEAKER_00, gender: male, male_prob: 0.7077215909957886, female_prob: 0.2922784090042114\n",
      "file_name: AD0301.wav, segment_id: segment_9.wav, turn.start: 32.68409375, turn.end: 33.54471875, duration: 0.8606249999999989, speaker: SPEAKER_00, gender: male, male_prob: 0.750615119934082, female_prob: 0.24938488006591797\n",
      "file_name: AD0301.wav, segment_id: segment_10.wav, turn.start: 34.35471875, turn.end: 35.04659375, duration: 0.691874999999996, speaker: SPEAKER_00, gender: male, male_prob: 0.9604500532150269, female_prob: 0.039549946784973145\n",
      "file_name: AD0301.wav, segment_id: segment_11.wav, turn.start: 37.22346875, turn.end: 40.29471875, duration: 3.071249999999999, speaker: SPEAKER_00, gender: male, male_prob: 0.8239657878875732, female_prob: 0.17603421211242676\n",
      "file_name: AD0301.wav, segment_id: segment_12.wav, turn.start: 41.00346875, turn.end: 43.85534375, duration: 2.8518749999999997, speaker: SPEAKER_00, gender: female, male_prob: 0.2447330504655838, female_prob: 0.7552669495344162\n",
      "file_name: AD0301.wav, segment_id: segment_13.wav, turn.start: 44.51346875, turn.end: 45.509093750000005, duration: 0.995625000000004, speaker: SPEAKER_00, gender: male, male_prob: 0.8865110874176025, female_prob: 0.11348891258239746\n",
      "file_name: AD0301.wav, segment_id: segment_14.wav, turn.start: 47.06159375, turn.end: 48.39471875, duration: 1.3331250000000026, speaker: SPEAKER_00, gender: male, male_prob: 0.9465574622154236, female_prob: 0.053442537784576416\n",
      "Excel file path: /home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/Final_Files/03. Output Bild + Ton/01. output_lists/ADs_IG_2014/AD0301.xlsx\n",
      "Writing to /home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/Final_Files/03. Output Bild + Ton/01. output_lists/ADs_IG_2014/AD0301.xlsx completed!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "input_folder = os.getenv(\"ADS_DIR\")\n",
    "print(\"input_folder\", input_folder)\n",
    "output_base_folder = os.getenv(\"OUTPUT_BILD_PLUS_TON_LISTS_DIR\")\n",
    "output_base_excel_folder = os.getenv(\"OUTPUT_BILD_PLUS_TON_LISTS_DIR\")\n",
    "splitted_audios= audio_splitter(input_folder, output_base_folder, output_base_excel_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8ed4CsrdFsXR"
   },
   "source": [
    "### Total speaking time (Conclusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3572,
     "status": "ok",
     "timestamp": 1708647270619,
     "user": {
      "displayName": "Thorben",
      "userId": "17213772383075988098"
     },
     "user_tz": -60
    },
    "id": "EO353s5XJLkn",
    "outputId": "5130ae55-3dcc-4558-923a-537751a935a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "files ['AD0253.wav']\n",
      "Processing: /home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/ADs/ADs_IG_2013/AD0253.wav\n",
      "Total Duration: 28.8 seconds\n",
      "Excel file path: /home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/Final_Files/03. Output Bild + Ton/01. output_lists/ADs_IG_2013/AD0253.xlsx\n",
      "Excel file exists!\n",
      "files ['AD0301.wav']\n",
      "Processing: /home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/ADs/ADs_IG_2014/AD0301.wav\n",
      "Total Duration: 56.128 seconds\n",
      "Excel file path: /home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/Final_Files/03. Output Bild + Ton/01. output_lists/ADs_IG_2014/AD0301.xlsx\n",
      "Excel file exists!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from pydub import AudioSegment\n",
    "# Iterate through each folder from ADs_IG_2013_wav to ADs_IG_2022_wav\n",
    "years = os.getenv(\"YEARS\").split(\" \")\n",
    "\n",
    "for folder_name in years:\n",
    "# for folder_name in range(2013, 2023):\n",
    "    input_path = f\"{input_folder}/ADs_IG_{folder_name}\"\n",
    "    output_folder = f\"{output_base_folder}/ADs_IG_{folder_name}\"\n",
    "    files = [f for f in os.listdir(input_path) if f.endswith('.wav')]\n",
    "    files.sort()\n",
    "    print(\"files\", files)\n",
    "    # Iterate through each WAV file in the current folder\n",
    "    for wav_file_name in files:  # Changed from os.listdir(input_path)\n",
    "        # Load the WAV file using pydub\n",
    "            wav_file_path = os.path.join(input_path, wav_file_name)\n",
    "            audio = AudioSegment.from_wav(wav_file_path)\n",
    "            # Get the total duration in seconds\n",
    "            total_duration = len(audio) / 1000.0  # Convert milliseconds to seconds\n",
    "\n",
    "            print(\"Processing:\", wav_file_path)\n",
    "            print(\"Total Duration:\", total_duration, \"seconds\")\n",
    "\n",
    "            # Read Excel file into a pandas DataFrame\n",
    "            excel_file_path = os.path.join(output_base_excel_folder,f\"ADs_IG_{folder_name}\", f\"{os.path.splitext(wav_file_name)[0]}.xlsx\")\n",
    "            # df = pd.read_excel(excel_file_path)\n",
    "            print(\"Excel file path:\", excel_file_path)\n",
    "            if os.path.isfile(excel_file_path):\n",
    "                print(\"Excel file exists!\")\n",
    "                try:\n",
    "                    # Read the existing data\n",
    "                    df = pd.read_excel(excel_file_path, sheet_name='Gender_speaking_time')\n",
    "                    \n",
    "                    # Check if DataFrame is empty or missing required columns\n",
    "                    if df.empty or 'duration' not in df.columns or 'gender' not in df.columns:\n",
    "                        print(f\"Warning: Required columns missing in {excel_file_path}\")\n",
    "                        continue\n",
    "                        \n",
    "                    # Calculate total speaking time in seconds\n",
    "                    total_speaking_time = df['duration'].sum()\n",
    "                    \n",
    "                    # Calculate the percentage of total speaking time against the total duration\n",
    "                    percentage_of_total_duration = total_speaking_time / total_duration * 100\n",
    "                    \n",
    "                    # Calculate percentages\n",
    "                    df['Value_Percent'] = df['duration'] / total_speaking_time * 100\n",
    "                    \n",
    "                    # Create results DataFrame\n",
    "                    results_df = pd.DataFrame({\n",
    "                        'Metric': ['Total Speaking Time', 'Male Speaking Time', 'Female Speaking Time'],\n",
    "                        'Value_Seconds': [\n",
    "                            total_speaking_time, \n",
    "                            df[df['gender'] == 'male']['duration'].sum(), \n",
    "                            df[df['gender'] == 'female']['duration'].sum()\n",
    "                        ],\n",
    "                        'Value_Percent': [\n",
    "                            percentage_of_total_duration, \n",
    "                            df[df['gender'] == 'male']['Value_Percent'].sum(), \n",
    "                            df[df['gender'] == 'female']['Value_Percent'].sum()\n",
    "                        ]\n",
    "                    })\n",
    "                    \n",
    "                    # Combine data\n",
    "                    existing_data = pd.read_excel(excel_file_path, sheet_name='Gender_speaking_time')\n",
    "                    existing_data[''] = ''\n",
    "                    combined_data = pd.concat([existing_data, results_df], axis=1)\n",
    "                    \n",
    "                    # Write back to Excel\n",
    "                    with pd.ExcelWriter(excel_file_path, engine='openpyxl', mode='a', if_sheet_exists='replace') as writer:\n",
    "                        combined_data.to_excel(writer, sheet_name='Gender_speaking_time', index=False)\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {excel_file_path}: {str(e)}\")\n",
    "            else:\n",
    "                print(\"Excel file does not exist!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k49Dx-u6qMwE"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tDMwn1BCiYWW"
   },
   "source": [
    "# *2. Emotion Recognition from Transcription*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PP0b_vg7rrze"
   },
   "source": [
    "WhisperAI transcribes all audio files before distilbert analyzes the emotions\n",
    "\n",
    "\n",
    "WhisperAI: https://github.com/openai/whisper\n",
    "\n",
    "distilbert-base-uncased-emotion: https://huggingface.co/bhadresh-savani/distilbert-base-uncased-emotion?text=I+feel+a+bit+let+down"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AvqbjDgQn94R"
   },
   "source": [
    "## Whisper AI for Transcription\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 55409,
     "status": "ok",
     "timestamp": 1708602848456,
     "user": {
      "displayName": "Thorben Schlieffen",
      "userId": "06537630395058694731"
     },
     "user_tz": -60
    },
    "id": "21_SRoXnoXlR",
    "outputId": "9f5ed74f-d6bf-4427-dfb1-0f44cfc4b0b9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai-whisper in ./main_sound_recognition_FINAL-venv/lib/python3.12/site-packages (20240930)\n",
      "Requirement already satisfied: numba in ./main_sound_recognition_FINAL-venv/lib/python3.12/site-packages (from openai-whisper) (0.60.0)\n",
      "Requirement already satisfied: numpy in ./main_sound_recognition_FINAL-venv/lib/python3.12/site-packages (from openai-whisper) (1.26.4)\n",
      "Requirement already satisfied: torch in ./main_sound_recognition_FINAL-venv/lib/python3.12/site-packages (from openai-whisper) (2.5.1)\n",
      "Requirement already satisfied: tqdm in ./main_sound_recognition_FINAL-venv/lib/python3.12/site-packages (from openai-whisper) (4.67.0)\n",
      "Requirement already satisfied: more-itertools in ./main_sound_recognition_FINAL-venv/lib/python3.12/site-packages (from openai-whisper) (10.5.0)\n",
      "Requirement already satisfied: tiktoken in ./main_sound_recognition_FINAL-venv/lib/python3.12/site-packages (from openai-whisper) (0.8.0)\n",
      "Requirement already satisfied: triton>=2.0.0 in ./main_sound_recognition_FINAL-venv/lib/python3.12/site-packages (from openai-whisper) (3.1.0)\n",
      "Requirement already satisfied: filelock in ./main_sound_recognition_FINAL-venv/lib/python3.12/site-packages (from triton>=2.0.0->openai-whisper) (3.16.1)\n",
      "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in ./main_sound_recognition_FINAL-venv/lib/python3.12/site-packages (from numba->openai-whisper) (0.43.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in ./main_sound_recognition_FINAL-venv/lib/python3.12/site-packages (from tiktoken->openai-whisper) (2024.11.6)\n",
      "Requirement already satisfied: requests>=2.26.0 in ./main_sound_recognition_FINAL-venv/lib/python3.12/site-packages (from tiktoken->openai-whisper) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in ./main_sound_recognition_FINAL-venv/lib/python3.12/site-packages (from torch->openai-whisper) (4.12.2)\n",
      "Requirement already satisfied: networkx in ./main_sound_recognition_FINAL-venv/lib/python3.12/site-packages (from torch->openai-whisper) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in ./main_sound_recognition_FINAL-venv/lib/python3.12/site-packages (from torch->openai-whisper) (3.1.4)\n",
      "Requirement already satisfied: fsspec in ./main_sound_recognition_FINAL-venv/lib/python3.12/site-packages (from torch->openai-whisper) (2024.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in ./main_sound_recognition_FINAL-venv/lib/python3.12/site-packages (from torch->openai-whisper) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in ./main_sound_recognition_FINAL-venv/lib/python3.12/site-packages (from torch->openai-whisper) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in ./main_sound_recognition_FINAL-venv/lib/python3.12/site-packages (from torch->openai-whisper) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in ./main_sound_recognition_FINAL-venv/lib/python3.12/site-packages (from torch->openai-whisper) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in ./main_sound_recognition_FINAL-venv/lib/python3.12/site-packages (from torch->openai-whisper) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in ./main_sound_recognition_FINAL-venv/lib/python3.12/site-packages (from torch->openai-whisper) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in ./main_sound_recognition_FINAL-venv/lib/python3.12/site-packages (from torch->openai-whisper) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in ./main_sound_recognition_FINAL-venv/lib/python3.12/site-packages (from torch->openai-whisper) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in ./main_sound_recognition_FINAL-venv/lib/python3.12/site-packages (from torch->openai-whisper) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in ./main_sound_recognition_FINAL-venv/lib/python3.12/site-packages (from torch->openai-whisper) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in ./main_sound_recognition_FINAL-venv/lib/python3.12/site-packages (from torch->openai-whisper) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in ./main_sound_recognition_FINAL-venv/lib/python3.12/site-packages (from torch->openai-whisper) (12.4.127)\n",
      "Requirement already satisfied: setuptools in ./main_sound_recognition_FINAL-venv/lib/python3.12/site-packages (from torch->openai-whisper) (75.4.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in ./main_sound_recognition_FINAL-venv/lib/python3.12/site-packages (from torch->openai-whisper) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./main_sound_recognition_FINAL-venv/lib/python3.12/site-packages (from sympy==1.13.1->torch->openai-whisper) (1.3.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./main_sound_recognition_FINAL-venv/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./main_sound_recognition_FINAL-venv/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./main_sound_recognition_FINAL-venv/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./main_sound_recognition_FINAL-venv/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2024.8.30)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./main_sound_recognition_FINAL-venv/lib/python3.12/site-packages (from jinja2->torch->openai-whisper) (3.0.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/openai/whisper.git\n",
      "  Cloning https://github.com/openai/whisper.git to /tmp/pip-req-build-lkk85wuq\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/openai/whisper.git /tmp/pip-req-build-lkk85wuq\n",
      "  Resolved https://github.com/openai/whisper.git to commit 173ff7dd1d9fb1c4fddea0d41d704cfefeb8908c\n",
      "  Installing build dependencies ... \u001b[?25l^C\n",
      "\u001b[?25hcanceled\n",
      "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/openai/whisper.git\n",
      "  Cloning https://github.com/openai/whisper.git to /tmp/pip-req-build-j7ib_4mk\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/openai/whisper.git /tmp/pip-req-build-j7ib_4mk\n",
      "  Resolved https://github.com/openai/whisper.git to commit 173ff7dd1d9fb1c4fddea0d41d704cfefeb8908c\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: openai-whisper\n",
      "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for openai-whisper: filename=openai_whisper-20240930-py3-none-any.whl size=803557 sha256=0b5b6f49af049de278f3e1c201f5b228fee3e347a8edda57b27fdf6050188d63\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-b0t2m4hb/wheels/c3/03/25/5e0ba78bc27a3a089f137c9f1d92fdfce16d06996c071a016c\n",
      "Successfully built openai-whisper\n",
      "Installing collected packages: openai-whisper\n",
      "  Attempting uninstall: openai-whisper\n",
      "    Found existing installation: openai-whisper 20240930\n",
      "    Uninstalling openai-whisper-20240930:\n",
      "      Successfully uninstalled openai-whisper-20240930\n",
      "Successfully installed openai-whisper-20240930\n"
     ]
    }
   ],
   "source": [
    "# if not installed\n",
    "!pip install -U openai-whisper\n",
    "\n",
    "!pip install git+https://github.com/openai/whisper.git\n",
    "\n",
    "!pip install --upgrade --no-deps --force-reinstall git+https://github.com/openai/whisper.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6625,
     "status": "ok",
     "timestamp": 1708602915572,
     "user": {
      "displayName": "Thorben Schlieffen",
      "userId": "06537630395058694731"
     },
     "user_tz": -60
    },
    "id": "sGJ8gRNzobgc",
    "outputId": "688e1b38-02bd-47d9-f98c-42bdba5f70af"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: setuptools-rust in ./main_sound_recognition_FINAL-venv/lib/python3.12/site-packages (1.10.2)\n",
      "Requirement already satisfied: setuptools>=62.4 in ./main_sound_recognition_FINAL-venv/lib/python3.12/site-packages (from setuptools-rust) (75.4.0)\n",
      "Requirement already satisfied: semantic-version<3,>=2.8.2 in ./main_sound_recognition_FINAL-venv/lib/python3.12/site-packages (from setuptools-rust) (2.10.0)\n"
     ]
    }
   ],
   "source": [
    "# if not installed\n",
    "!pip install setuptools-rust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20751,
     "status": "ok",
     "timestamp": 1708602079178,
     "user": {
      "displayName": "Thorben Schlieffen",
      "userId": "06537630395058694731"
     },
     "user_tz": -60
    },
    "id": "GTmPWuaioekS",
    "outputId": "c4d49d44-b55e-4aeb-ca32-d67dcdeebef9"
   },
   "outputs": [],
   "source": [
    "# Optional\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d8KH9_OQrbFE"
   },
   "source": [
    "### Main\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 605541,
     "status": "ok",
     "timestamp": 1708603530986,
     "user": {
      "displayName": "Thorben Schlieffen",
      "userId": "06537630395058694731"
     },
     "user_tz": -60
    },
    "id": "e4zYYidAo0aC",
    "outputId": "db925520-3f2d-4042-aede-f3c9074e1301"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/Final_Files/02. Tonanalyse/main_sound_recognition_FINAL-venv/lib/python3.12/site-packages/whisper/__init__.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(fp, map_location=device)\n",
      "/home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/Final_Files/02. Tonanalyse/main_sound_recognition_FINAL-venv/lib/python3.12/site-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for AD0253.wav:  New Axe Apollo. Enter before midnight at axeapollo.com for your chance to go to space.\n",
      "Output will be saved in /home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/ADs/ADs_IG_2013/AD0253.txt .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/Final_Files/02. Tonanalyse/main_sound_recognition_FINAL-venv/lib/python3.12/site-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for AD0301.wav:  There's nothing stronger or more powerful than a dream. Even in darkness, dreams have an infinite capacity to see light. They outlive pain. They outlast heartbreak. They overcome doubt. They forget imperfection. They cannot be beaten, broken, or taken. Dreams fight. They persevere. They go on. Because determination will always trump disappointment. Desire will always triumph over defeat. And dreams, dreams will always prevail. Transcription by ESO. Translation by —\n",
      "Output will be saved in /home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/ADs/ADs_IG_2014/AD0301.txt .\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import whisper\n",
    "import torch\n",
    "\n",
    "# Input\n",
    "input_folder_path = os.getenv(\"ADS_DIR\")\n",
    "\n",
    "# Set model to GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = whisper.load_model(\"large\", device=device)\n",
    "\n",
    "# Iterate through each folder\n",
    "for folder_name in os.listdir(input_folder_path):\n",
    "    folder_path = os.path.join(input_folder_path, folder_name)\n",
    "\n",
    "    # Iterate to find .wav files\n",
    "    if os.path.isdir(folder_path):\n",
    "        for file_name in os.listdir(folder_path):\n",
    "            if file_name.endswith(\".wav\"):\n",
    "                audio_file_path = os.path.join(folder_path, file_name)\n",
    "\n",
    "                # Transcription of audio file\n",
    "                result = model.transcribe(audio_file_path)\n",
    "\n",
    "                # Outputs here are only TXT files. WhisperAI can also create SRT, VTT, TSV and JSON files. You can change it here.\n",
    "\n",
    "                # Create output\n",
    "                output_txt_path = os.path.join(folder_path, f\"{file_name.split('.')[0]}.txt\")\n",
    "\n",
    "                # Save Output in .txt file\n",
    "                with open(output_txt_path, 'w') as txt_file:\n",
    "                    txt_file.write(result['text'])\n",
    "\n",
    "                print(f\"Transcription for {file_name}: {result['text']}\")\n",
    "                print(f\"Output will be saved in {output_txt_path} .\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8d20XiSLo_FY"
   },
   "source": [
    "\n",
    "## Emotion Recognition (1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zZH1G38Y6AvO"
   },
   "source": [
    "### Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "id": "Lai6BjVVo_ue"
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "id": "rkjeJW2XiXhb"
   },
   "outputs": [],
   "source": [
    "# If not installed:\n",
    "#!pip install transformers\n",
    "#!pip install xlsxwriter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EVfTBXNV7jCH"
   },
   "source": [
    "### Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "id": "o-jZ1-Fjkgpd"
   },
   "outputs": [],
   "source": [
    "# Define emotionclassifier\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "# Create a text classification pipeline object\n",
    "classifier = pipeline(\"text-classification\", model='bhadresh-savani/distilbert-base-uncased-emotion', top_k=None)\n",
    "\n",
    "def classify_emotion_from_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "\n",
    "    # Split the text into words\n",
    "    words = content.split()\n",
    "\n",
    "    def classify_and_check(predictions, text):\n",
    "        if any(emotion['score'] > 0.8 for emotion in predictions[0]):\n",
    "            return {'Emotion': predictions[0][0]['label'], 'Probability': predictions[0][0]['score']}\n",
    "        else:\n",
    "            return {'Emotion': 'neutral', 'Probability': 1.0}\n",
    "\n",
    "    # Due to the model's limitation in analyzing coherent texts with more than 657 tokens (approximately 320 words),\n",
    "    # such lengthy texts will be split in half and analyzed independently. In our collection of Super Bowl ads from 2013 to 2022,\n",
    "    # only one ad (AD0290) exceeds this limit. Our AI model classified the first part as 'fear' and the second part as 'joy.'\n",
    "    # After a manual analysis of both parts, we would categorize the entire ad as 'joy.'\n",
    "\n",
    "    if len(words) > 320:\n",
    "        # Split the first row into two parts, each containing half of the words\n",
    "        half_length = len(words) // 2\n",
    "        first_row_part1 = ' '.join(words[:half_length])\n",
    "        first_row_part2 = ' '.join(words[half_length:])\n",
    "\n",
    "        # Classify emotions for the first part of the first row\n",
    "        first_row_part1_predictions = classifier(first_row_part1)\n",
    "\n",
    "        # Append information for the first part of the first row\n",
    "        emotions_and_scores_part1 = {\n",
    "            'AD-Number': file_path.split('/')[-1].split('.')[0],\n",
    "            'Transcription': first_row_part1,\n",
    "            'Word range': f'1-{half_length}',\n",
    "            **classify_and_check(first_row_part1_predictions, first_row_part1)\n",
    "        }\n",
    "\n",
    "        # Classify emotions for the second part of the first row\n",
    "        first_row_part2_predictions = classifier(first_row_part2)\n",
    "\n",
    "        # Append information for the second part of the first row\n",
    "        emotions_and_scores_part2 = {\n",
    "            'AD-Number': file_path.split('/')[-1].split('.')[0],\n",
    "            'Transcription': first_row_part2,\n",
    "            'Word range': f'{half_length + 1}-{len(words)}',\n",
    "            **classify_and_check(first_row_part2_predictions, first_row_part2)\n",
    "        }\n",
    "\n",
    "        # Combine information for the first row\n",
    "        emotions_and_scores = [emotions_and_scores_part1, emotions_and_scores_part2]\n",
    "    else:\n",
    "\n",
    "        # Classify emotions for the entire content\n",
    "        full_content_predictions = classifier(content)\n",
    "\n",
    "        # Append information for the first row (full content)\n",
    "        emotions_and_scores = [{\n",
    "            'AD-Number': file_path.split('/')[-1].split('.')[0],\n",
    "            'Transcription': content,\n",
    "            'Word range': f'1-{len(words)}',\n",
    "            **classify_and_check(full_content_predictions, content)\n",
    "        }]\n",
    "\n",
    "    # Classify emotions for each 20-word segment starting from the 2nd row with a 5-word shift\n",
    "    for start in range(0, len(words)-20, 5):\n",
    "        end = min(start + 20, len(words))\n",
    "        word_range = f'{start+1}-{end}'  # Adjust to avoid index out of range\n",
    "        text_segment = ' '.join(words[start:end])\n",
    "\n",
    "        # Classify emotions for the current segment\n",
    "        segment_predictions = classifier(text_segment)\n",
    "\n",
    "        # Append information for each segment\n",
    "        emotions_and_scores.append({\n",
    "            'AD-Number': file_path.split('/')[-1].split('.')[0],\n",
    "            'Transcription': text_segment,\n",
    "            'Word range': word_range,\n",
    "            **classify_and_check(segment_predictions, text_segment)\n",
    "        })\n",
    "\n",
    "\n",
    "    # Check for missing words\n",
    "    remaining_start = max(len(words) - 20, 0)  # Startpunkt für die letzten 20 Wörter\n",
    "    if remaining_start < len(words):\n",
    "        remaining_word_range = f'{remaining_start + 1}-{len(words)}'\n",
    "        remaining_text_segment = ' '.join(words[remaining_start:])\n",
    "        remaining_predictions = classifier(remaining_text_segment)\n",
    "\n",
    "        # Verwendung der classify_and_check-Funktion für die Emotionsklassifikation\n",
    "        remaining_emotion_info = classify_and_check(remaining_predictions, remaining_text_segment)\n",
    "\n",
    "        # Append information for remaining words directly\n",
    "        emotions_and_scores.append({\n",
    "          'AD-Number': file_path.split('/')[-1].split('.')[0],\n",
    "          'Transcription': remaining_text_segment,\n",
    "          'Word range': remaining_word_range,\n",
    "          'Emotion': remaining_emotion_info['Emotion'],\n",
    "          'Probability': remaining_emotion_info['Probability']\n",
    "        })\n",
    "\n",
    "\n",
    "    return emotions_and_scores, emotions_and_scores[0]['AD-Number']\n",
    "\n",
    "\n",
    "def extract_emotions_and_scores(text, predictions, ad_number):\n",
    "    # Extract emotions + probabilities and add \"Word range\" and \"Text segment\" information\n",
    "    emotions_and_scores = []\n",
    "\n",
    "    # Split text into words\n",
    "    words = text.split()\n",
    "    segment_size = 20\n",
    "\n",
    "    for emotion in predictions[0]:\n",
    "        if emotion['score'] > 0.8:\n",
    "            for start in range(0, len(words), segment_size):  # Adjust to 20 words per segment\n",
    "                end = min(start + segment_size, len(words))\n",
    "                word_range = f'{start + 1}-{end}'  # Adjust to avoid index out of range\n",
    "                text_segment = ' '.join(words[start:end])\n",
    "\n",
    "                # Classify emotions for the current segment using the global classifier\n",
    "                segment_predictions = classifier(text_segment)\n",
    "\n",
    "                # Only add relevant information for the first row\n",
    "                if start == 0:\n",
    "                    emotions_and_scores.append({\n",
    "                        'AD-Number': ad_number.split('/')[-1].split('.')[0],\n",
    "                        'Transcription': text,\n",
    "                        'Word range': f'1-{len(words)}',\n",
    "                        'Emotion': emotion['label'],\n",
    "                        'Probability': emotion['score']\n",
    "                    })\n",
    "\n",
    "                # Add information for subsequent rows\n",
    "                emotions_and_scores.append({\n",
    "                    'AD-Number': ad_number.split('/')[-1].split('.')[0],\n",
    "                    'Transcription': text_segment,\n",
    "                    'Word range': word_range,\n",
    "                    'Emotion': segment_predictions[0][0]['label'],  # Assuming top emotion from the model\n",
    "                    'Probability': segment_predictions[0][0]['score']\n",
    "\n",
    "                     })\n",
    "\n",
    "\n",
    "\n",
    "    return emotions_and_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "id": "mm1uQuTXkkiy"
   },
   "outputs": [],
   "source": [
    "def process_text_file(file_path, output_base_folder):\n",
    "    predictions, file_name = classify_emotion_from_file(file_path)\n",
    "\n",
    "    # Create directory for output\n",
    "    output_folder = os.path.join(output_base_folder, file_name)\n",
    "    output_folder_name = os.path.basename(output_folder)\n",
    "    output_folder_name = output_folder_name.replace(\".wav\", \"\")\n",
    "    output_folder = os.path.join(os.path.dirname(output_folder), output_folder_name)\n",
    "\n",
    "    # Go through all subfolders\n",
    "    for root, dirs, files in os.walk(output_base_folder):\n",
    "        for dir_name in dirs:\n",
    "            if dir_name not in file_path:\n",
    "                continue\n",
    "            # Create Excel directory\n",
    "            excel_file_path = os.path.join(root, dir_name, f\"{file_name}.xlsx\")\n",
    "            print(f\"excel_file_path: {excel_file_path}\")\n",
    "            try:\n",
    "                # Try to write to existing file with if_sheet_exists='replace'\n",
    "                with pd.ExcelWriter(excel_file_path, engine='openpyxl', mode='a', if_sheet_exists='replace') as writer:\n",
    "                    result_df_new = pd.DataFrame(predictions, columns=['AD-Number', 'Transcription', 'Word range', 'Emotion', 'Probability'])\n",
    "                    result_df_new.to_excel(writer, sheet_name=\"Transcription_and_Mood\", index=False)\n",
    "                    print(f\"File {excel_file_path} is updated.\")\n",
    "                    return\n",
    "            except FileNotFoundError:\n",
    "                # If file doesn't exist, create new one\n",
    "                with pd.ExcelWriter(excel_file_path, engine='openpyxl', mode='w') as writer:\n",
    "                    result_df_new = pd.DataFrame(predictions, columns=['AD-Number', 'Transcription', 'Word range', 'Emotion', 'Probability'])\n",
    "                    result_df_new.to_excel(writer, sheet_name=\"Transcription_and_Mood\", index=False)\n",
    "                    print(f\"File {excel_file_path} is created.\")\n",
    "                    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4FuVNTED7pKZ"
   },
   "source": [
    "### Input and Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4312,
     "status": "ok",
     "timestamp": 1708647348242,
     "user": {
      "displayName": "Thorben",
      "userId": "17213772383075988098"
     },
     "user_tz": -60
    },
    "id": "Qk6tnP-Akn_j",
    "outputId": "4ef1cdaf-3096-4e6c-aada-fdff31b13a99"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "excel_file_path: /home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/Final_Files/03. Output Bild + Ton/01. output_lists/ADs_IG_2013/AD0253.xlsx\n",
      "File /home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/Final_Files/03. Output Bild + Ton/01. output_lists/ADs_IG_2013/AD0253.xlsx is updated.\n",
      "excel_file_path: /home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/Final_Files/03. Output Bild + Ton/01. output_lists/ADs_IG_2014/AD0301.xlsx\n",
      "File /home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/Final_Files/03. Output Bild + Ton/01. output_lists/ADs_IG_2014/AD0301.xlsx is updated.\n"
     ]
    }
   ],
   "source": [
    "# Input\n",
    "# Loop through all files in the folder\n",
    "input_folder_path = os.getenv(\"ADS_DIR\")\n",
    "output_base_folder = os.getenv(\"OUTPUT_BILD_PLUS_TON_LISTS_DIR\")\n",
    "for root, dirs, files in os.walk(input_folder_path):\n",
    "    for file_name in files:\n",
    "        if file_name.endswith(\".txt\"):\n",
    "            file_path = os.path.join(root, file_name)\n",
    "            process_text_file(file_path, output_base_folder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wZJr6yGgqF6f"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j-tItGZ9W5IH"
   },
   "source": [
    "## Combination of emotion from image and audio (2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Jefnv7pX004"
   },
   "source": [
    "## Emotion from WhisperAI Intervalls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9SdbABSwwcSz"
   },
   "source": [
    "### Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "id": "u6wKKv3zIpvM"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "from transformers import pipeline\n",
    "from openpyxl import load_workbook\n",
    "from openpyxl.utils.dataframe import dataframe_to_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1629,
     "status": "ok",
     "timestamp": 1709415738181,
     "user": {
      "displayName": "Flavio Kuka",
      "userId": "09654499042520950340"
     },
     "user_tz": -60
    },
    "id": "hwbaqOc41Ydh",
    "outputId": "317679be-e946-435b-cfc1-c267981072b3"
   },
   "outputs": [],
   "source": [
    "# Optional\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X8KcwHSh0k1r"
   },
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {
    "id": "dUku8083IvH0"
   },
   "outputs": [],
   "source": [
    "# Funktion zum Emotionsklassifikation und Überprüfung\n",
    "def classify_and_check(predictions):\n",
    "    if any(emotion['score'] > 0.8 for emotion in predictions[0]):\n",
    "        return {'Emotion': predictions[0][0]['label'], 'Probability': predictions[0][0]['score']}\n",
    "    else:\n",
    "        return {'Emotion': 'neutral', 'Probability': 1.0}\n",
    "\n",
    "# Funktion zum Emotionsklassifikation\n",
    "def classify_emotion(text):\n",
    "    classifier = pipeline(\"text-classification\", model='bhadresh-savani/distilbert-base-uncased-emotion', top_k=None)\n",
    "    result = classifier(text)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "id": "4xdz4hR6I7Mz"
   },
   "outputs": [],
   "source": [
    "def process_json_file(json_file_path, excel_file_path):\n",
    "    # Create empty list\n",
    "    all_excel_data = []\n",
    "    print(f\"excel_file_path: {excel_file_path}\")\n",
    "\n",
    "    # Existing table\n",
    "    existing_df = pd.read_excel(excel_file_path, sheet_name='Transcription_and_Mood')\n",
    "\n",
    "    # load all JSON data\n",
    "    with open(json_file_path) as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # extract data name\n",
    "    ad_number = os.path.splitext(os.path.basename(json_file_path))[0]\n",
    "\n",
    "    # extract whole text\n",
    "    full_text = data.get(\"text\", \"\")\n",
    "\n",
    "    # extract relevant information\n",
    "    data_list = data.get(\"segments\", [])\n",
    "\n",
    "    # loop through all segments\n",
    "    for segment in data_list:\n",
    "        row = {\n",
    "            'AD-Number': ad_number,\n",
    "            'ID-Number': segment.get(\"id\", \"\"),\n",
    "            'Start': segment.get(\"start\", \"\"),\n",
    "            'End': segment.get(\"end\", \"\"),\n",
    "            'Transcription': segment.get(\"text\", \"\")\n",
    "        }\n",
    "\n",
    "        # classify emotion\n",
    "        emotion_result = classify_emotion(row['Transcription'])\n",
    "        row.update(classify_and_check(emotion_result))\n",
    "\n",
    "        all_excel_data.append(row)\n",
    "\n",
    "    # add first row\n",
    "    first_row = {\n",
    "        'AD-Number': ad_number,\n",
    "        'ID-Number': '',\n",
    "        'Start': '',\n",
    "        'End': '',\n",
    "        'Transcription': full_text\n",
    "    }\n",
    "\n",
    "    if len(full_text) > 320:\n",
    "      # Copy the emotion from the first table\n",
    "      first_row.update({'Emotion': existing_df.at[0, 'Emotion'], 'Probability': 1.0})\n",
    "\n",
    "    else:\n",
    "      # classify emotion + add results\n",
    "      emotion_result_first_row = classify_emotion(first_row['Transcription'])\n",
    "      first_row.update(classify_and_check(emotion_result_first_row))\n",
    "\n",
    "    all_excel_data.insert(0, first_row)\n",
    "\n",
    "    # Create a new DataFrame with your new data\n",
    "    new_data_df = pd.DataFrame(all_excel_data)\n",
    "\n",
    "    # Insert the new DataFrame\n",
    "    existing_df[' '] = ''\n",
    "    existing_df = pd.concat([existing_df, new_data_df], axis=1)\n",
    "\n",
    "    # Load the existing workbook using openpyxl\n",
    "    workbook = load_workbook(excel_file_path)\n",
    "\n",
    "    # get the 'Transcription_and_Mood' sheet\n",
    "    sheet = workbook['Transcription_and_Mood']\n",
    "\n",
    "    # Write the updated DataFrame to the Excel sheet\n",
    "    for r_idx, row in enumerate(dataframe_to_rows(existing_df, index=False, header=True), 1):\n",
    "        for c_idx, value in enumerate(row, 1):\n",
    "            sheet.cell(row=r_idx, column=c_idx, value=value)\n",
    "\n",
    "    # Save the updated workbook\n",
    "    workbook.save(excel_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QWeqtLQ20paw"
   },
   "source": [
    "### Run Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "id": "Ka2AVThyb5xc"
   },
   "outputs": [],
   "source": [
    "# Path to the folder containing the frames and the excel lists\n",
    "\n",
    "json_file_folder_path = f'{os.getenv(\"TONANALYSE_DIR\")}/JSON_Dateien'\n",
    "os.makedirs(json_file_folder_path, exist_ok=True)\n",
    "excel_file_folder_path = os.getenv(\"OUTPUT_BILD_PLUS_TON_LISTS_DIR\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {
    "id": "6eANlqg81VzB"
   },
   "outputs": [],
   "source": [
    "# OPTIONAL: So that the loops starts with the years in an alphabetical order\n",
    "years = []\n",
    "for year in os.listdir(excel_file_folder_path):\n",
    "  years.append(year)\n",
    "years.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "id": "8_wd05jv1xMS"
   },
   "outputs": [],
   "source": [
    "for year in years:\n",
    "    # Set paths\n",
    "    json_files_year_path = os.path.join(json_file_folder_path, f'{year}_json')\n",
    "    excel_file_year_path = os.path.join(excel_file_folder_path, year)\n",
    "    \n",
    "    # Create JSON directory if it doesn't exist\n",
    "    os.makedirs(json_files_year_path, exist_ok=True)\n",
    "    \n",
    "    # Create sets of base file names\n",
    "    json_files_year_set = {os.path.splitext(file)[0] for file in os.listdir(json_files_year_path)}\n",
    "    excel_files_year_set = {os.path.splitext(file)[0] for file in os.listdir(excel_file_year_path)}\n",
    "        # Find common base names\n",
    "    common_base_names = json_files_year_set.intersection(excel_files_year_set)\n",
    "\n",
    "    # Iterate over common base names\n",
    "    for base_name in common_base_names:\n",
    "        json_file_path = os.path.join(json_files_year_path, f'{base_name}.json')\n",
    "        excel_file_path = os.path.join(excel_file_year_path, f'{base_name}.xlsx')\n",
    "        try:\n",
    "          process_json_file(json_file_path, excel_file_path)\n",
    "        except:\n",
    "          print(f\"fail: {base_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V0kMYUJ5YDoG"
   },
   "source": [
    "## Analysis Emotion Image & Audio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q52BXAJzYn8R"
   },
   "source": [
    "### Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5128,
     "status": "ok",
     "timestamp": 1709458592187,
     "user": {
      "displayName": "Flavio Kuka",
      "userId": "09654499042520950340"
     },
     "user_tz": -60
    },
    "id": "8YU1Z5y3q-yh",
    "outputId": "d4190464-069a-4fcd-e522-671536d77e26"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: XlsxWriter in ./main_sound_recognition_FINAL-venv/lib/python3.12/site-packages (3.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install XlsxWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "id": "uRA3k5WtvvVn"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from collections import Counter\n",
    "import os\n",
    "from openpyxl import load_workbook\n",
    "from openpyxl.utils.dataframe import dataframe_to_rows\n",
    "import math\n",
    "import xlsxwriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 27444,
     "status": "ok",
     "timestamp": 1709458457633,
     "user": {
      "displayName": "Flavio Kuka",
      "userId": "09654499042520950340"
     },
     "user_tz": -60
    },
    "id": "WjftN_ykvu4H",
    "outputId": "adb49219-d6ef-4c7e-9eec-d529f9baa424"
   },
   "outputs": [],
   "source": [
    "# Optional\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {
    "id": "ZFldY_ruwruD"
   },
   "outputs": [],
   "source": [
    "# Create the mapping dictionary for frames to seconds\n",
    "mapping_dict = {}\n",
    "for i in range(0, 4800, 10):\n",
    "    new_number = ((i // 10) // 3) + 1\n",
    "    mapping_dict[i] = new_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {
    "id": "3Gl45arPgKy3"
   },
   "outputs": [],
   "source": [
    "# Create the mapping for the emotions from text to image\n",
    "emotion_mapping_dict = {\n",
    "    'joy': 'happy',\n",
    "    'love': 'happy',\n",
    "    'neutral': 'neutral',\n",
    "    'anger': 'angry',\n",
    "    'surprise': 'surprise',\n",
    "    'fear': 'fear',\n",
    "    'sadness': 'sad'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4rEo3MezwgEO"
   },
   "source": [
    "### Find corresponding Frame_Nr for the second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {
    "id": "YYieyE2CwPOH"
   },
   "outputs": [],
   "source": [
    "def comparison_emotions_image_audio(excel_file_path):\n",
    "  # Read the Sheets\n",
    "  emotion_image = pd.read_excel(f'{excel_file_path}', sheet_name='Predictions')\n",
    "  emotion_audio = pd.read_excel(f'{excel_file_path}', sheet_name='Transcription_and_Mood')\n",
    "\n",
    "  # Create new columns\n",
    "  emotion_audio['Frame_Nr'] = ''\n",
    "  emotion_audio['Emotions'] = ''\n",
    "  emotion_audio['Dominant_Emotion'] = ''\n",
    "  emotion_audio['Correct_%'] = ''\n",
    "  emotion_audio['Equal_Emotions'] = ''\n",
    "\n",
    "  for index, start_second in emotion_audio['Start'].items():\n",
    "    # Check if start_second is not NaN\n",
    "    if pd.notna(start_second):\n",
    "        # Define the seconds in the interval\n",
    "        end_second = emotion_audio.at[index, 'End']\n",
    "        seconds_interval = list(range(int(start_second)+1, int(end_second)+1))\n",
    "\n",
    "        # Create a list of corresponding frames\n",
    "        frames = []\n",
    "        for second in seconds_interval:\n",
    "            for key, value in mapping_dict.items():\n",
    "                if value == round(second):\n",
    "                    frames.append(key)\n",
    "        # Save the corresponding frames to the dataframe\n",
    "        emotion_audio.at[index, 'Frame_Nr'] = frames\n",
    "\n",
    "  # Find corresponding emotion\n",
    "  for index_audio, frame_audio in emotion_audio['Frame_Nr'].items():\n",
    "    corresponding_emotions = []\n",
    "    if str(frame_audio) != '':\n",
    "      # Iterate for each frame_nr\n",
    "      for frame_nr_audio in frame_audio:\n",
    "        for index_image, frame_video in emotion_image['video_frame'].items():\n",
    "          # Find the corresponding frame_nr from the image analysis\n",
    "          frame_nr_video = frame_video[13:]\n",
    "          frame_nr_video = frame_nr_video.split('.')[0]\n",
    "          if str(frame_nr_audio) == str(frame_nr_video):\n",
    "            # Find the corresponding emotion\n",
    "            corresponding_emotion = emotion_image.at[index_image, 'emotion_prediction']\n",
    "            if corresponding_emotion != '-':\n",
    "              corresponding_emotions.append(corresponding_emotion)\n",
    "      # Save the identified emotions\n",
    "      emotion_audio.at[index_audio, 'Emotions'] = corresponding_emotions\n",
    "\n",
    "  # Find dominant emotion\n",
    "  for index_audio, emotions_image in emotion_audio['Emotions'].items():\n",
    "    if len(emotions_image) > 0:\n",
    "      dominant_emotion = Counter(emotions_image).most_common(1)[0][0]\n",
    "      emotion_audio.at[index_audio, 'Dominant_Emotion'] = dominant_emotion\n",
    "\n",
    "  # Find Correct % and if both Emotions are equal\n",
    "  for index_audio, total_emotion_audio in emotion_audio['Emotion.1'].items():\n",
    "    if str(total_emotion_audio) != 'nan':\n",
    "      transformed_emotion = emotion_mapping_dict[str(total_emotion_audio)]\n",
    "      emotions_from_image = emotion_audio.at[index_audio, 'Emotions']\n",
    "      correct_emotion_count = emotions_from_image.count(transformed_emotion)\n",
    "      total_emotions = len(emotions_from_image)\n",
    "      if total_emotions != 0:\n",
    "        emotion_audio.at[index_audio, 'Correct_%'] = (correct_emotion_count/ total_emotions)\n",
    "      else:\n",
    "        emotion_audio.at[index_audio, 'Correct_%'] = 0\n",
    "\n",
    "      dominant_emotion_image = emotion_audio.at[index_audio, 'Dominant_Emotion']\n",
    "      if  transformed_emotion == dominant_emotion_image:\n",
    "        emotion_audio.at[index_audio, 'Equal_Emotions'] = 1\n",
    "      else:\n",
    "        emotion_audio.at[index_audio, 'Equal_Emotions'] = 0\n",
    "\n",
    "  # Insert summary information\n",
    "  emotion_audio['   '] = ''\n",
    "  emotion_audio['Average_Correct_%'] = ''\n",
    "  emotion_audio['Average_Equal_Emotions'] = ''\n",
    "  if len(list(emotion_audio['Emotion.1'].items())) > 0:\n",
    "      emotion_audio['Correct_%'] = pd.to_numeric(emotion_audio['Correct_%'], errors='coerce')\n",
    "      emotion_audio['Equal_Emotions'] = pd.to_numeric(emotion_audio['Equal_Emotions'], errors='coerce')\n",
    "      emotion_audio.at[0, 'Average_Correct_%'] = emotion_audio['Correct_%'].mean()\n",
    "      emotion_audio.at[0, 'Average_Equal_Emotions'] = emotion_audio['Equal_Emotions'].mean()\n",
    "\n",
    "  # Read exel file\n",
    "  excel_sheets = pd.read_excel(excel_file_path, sheet_name=None)\n",
    "\n",
    "  # Replace \"Unnamed\" columns with empty strings in all dataframes\n",
    "  for sheet_name, df in excel_sheets.items():\n",
    "      df.columns = [col if 'Unnamed' not in str(col) else '' for col in df.columns]\n",
    "\n",
    "  # Update the 'Transcription_and_Mood' sheet in the dictionary\n",
    "  excel_sheets['Transcription_and_Mood'] = emotion_audio\n",
    "\n",
    "  # Save the modified dictionary of dataframes back to the Excel file\n",
    "  with pd.ExcelWriter(excel_file_path, engine='xlsxwriter') as writer:\n",
    "      for sheet_name, df in excel_sheets.items():\n",
    "          df.to_excel(writer, sheet_name=sheet_name, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bC-h8259yoPl"
   },
   "source": [
    "### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {
    "id": "TkGyHorj_lJQ"
   },
   "outputs": [],
   "source": [
    "excel_file_folder_path = os.getenv(\"OUTPUT_BILD_PLUS_TON_LISTS_DIR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {
    "id": "-oFx1bf2Yn8k"
   },
   "outputs": [],
   "source": [
    "# OPTIONAL: So that the loops starts with the years in an alphabetical order\n",
    "years = []\n",
    "for year in os.listdir(excel_file_folder_path):\n",
    "  years.append(year)\n",
    "years.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 445207,
     "status": "ok",
     "timestamp": 1709459334450,
     "user": {
      "displayName": "Flavio Kuka",
      "userId": "09654499042520950340"
     },
     "user_tz": -60
    },
    "id": "wIKg409kYn8l",
    "outputId": "8927ad7b-6b8a-476d-8b43-0d16592a9f24"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AD0253.xlsx\n",
      "AD0253\n",
      "AD0301\n",
      "AD0301.xlsx\n"
     ]
    }
   ],
   "source": [
    "for year in years:\n",
    "    # Set path\n",
    "    excel_file_year_path = os.path.join(excel_file_folder_path, year)\n",
    "\n",
    "    # Create list of files\n",
    "    excel_files = os.listdir(excel_file_year_path)\n",
    "\n",
    "    # Iterate over files\n",
    "    for excel_file in excel_files:\n",
    "      excel_file_path = os.path.join(excel_file_year_path, excel_file)\n",
    "\n",
    "      try:\n",
    "        comparison_emotions_image_audio(excel_file_path)\n",
    "      except:\n",
    "        print(excel_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dOSIkq0O2vI5"
   },
   "source": [
    "# *3. Acoustic Indices (1.0.1)*\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gB0pxe3s2-Gd"
   },
   "source": [
    "This module aims to extract audio features of the given ads (audio-files)\n",
    "\n",
    "Acoustic Indices: https://github.com/patriceguyot/Acoustic_Indices\n",
    "\n",
    "Pydub: https://github.com/jiaaro/pydub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FTjB61sx3Hx3"
   },
   "source": [
    "### Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "id": "Po2USJPg3WOj"
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "9gd3_BUK3a5t",
    "outputId": "2f3e149e-944a-4adc-ea4a-d29acdb9aaef"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in ./main_sound_recognition_FINAL-venv/lib/python3.12/site-packages (1.26.4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scipy in ./main_sound_recognition_FINAL-venv/lib/python3.12/site-packages (1.14.1)\n",
      "Requirement already satisfied: numpy<2.3,>=1.23.5 in ./main_sound_recognition_FINAL-venv/lib/python3.12/site-packages (from scipy) (1.26.4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in ./main_sound_recognition_FINAL-venv/lib/python3.12/site-packages (3.9.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./main_sound_recognition_FINAL-venv/lib/python3.12/site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in ./main_sound_recognition_FINAL-venv/lib/python3.12/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./main_sound_recognition_FINAL-venv/lib/python3.12/site-packages (from matplotlib) (4.54.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in ./main_sound_recognition_FINAL-venv/lib/python3.12/site-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: numpy>=1.23 in ./main_sound_recognition_FINAL-venv/lib/python3.12/site-packages (from matplotlib) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in ./main_sound_recognition_FINAL-venv/lib/python3.12/site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in ./main_sound_recognition_FINAL-venv/lib/python3.12/site-packages (from matplotlib) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in ./main_sound_recognition_FINAL-venv/lib/python3.12/site-packages (from matplotlib) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./main_sound_recognition_FINAL-venv/lib/python3.12/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in ./main_sound_recognition_FINAL-venv/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyyaml in ./main_sound_recognition_FINAL-venv/lib/python3.12/site-packages (6.0.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: librosa in ./main_sound_recognition_FINAL-venv/lib/python3.12/site-packages (0.10.2.post1)\n",
      "Requirement already satisfied: audioread>=2.1.9 in ./main_sound_recognition_FINAL-venv/lib/python3.12/site-packages (from librosa) (3.0.1)\n",
      "Requirement already satisfied: numpy!=1.22.0,!=1.22.1,!=1.22.2,>=1.20.3 in ./main_sound_recognition_FINAL-venv/lib/python3.12/site-packages (from librosa) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.2.0 in ./main_sound_recognition_FINAL-venv/lib/python3.12/site-packages (from librosa) (1.14.1)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in ./main_sound_recognition_FINAL-venv/lib/python3.12/site-packages (from librosa) (1.5.2)\n",
      "Requirement already satisfied: joblib>=0.14 in ./main_sound_recognition_FINAL-venv/lib/python3.12/site-packages (from librosa) (1.4.2)\n",
      "Requirement already satisfied: decorator>=4.3.0 in ./main_sound_recognition_FINAL-venv/lib/python3.12/site-packages (from librosa) (5.1.1)\n",
      "Requirement already satisfied: numba>=0.51.0 in ./main_sound_recognition_FINAL-venv/lib/python3.12/site-packages (from librosa) (0.60.0)\n",
      "Requirement already satisfied: soundfile>=0.12.1 in ./main_sound_recognition_FINAL-venv/lib/python3.12/site-packages (from librosa) (0.12.1)\n",
      "Requirement already satisfied: pooch>=1.1 in ./main_sound_recognition_FINAL-venv/lib/python3.12/site-packages (from librosa) (1.8.2)\n",
      "Requirement already satisfied: soxr>=0.3.2 in ./main_sound_recognition_FINAL-venv/lib/python3.12/site-packages (from librosa) (0.5.0.post1)\n",
      "Requirement already satisfied: typing-extensions>=4.1.1 in ./main_sound_recognition_FINAL-venv/lib/python3.12/site-packages (from librosa) (4.12.2)\n",
      "Requirement already satisfied: lazy-loader>=0.1 in ./main_sound_recognition_FINAL-venv/lib/python3.12/site-packages (from librosa) (0.4)\n",
      "Requirement already satisfied: msgpack>=1.0 in ./main_sound_recognition_FINAL-venv/lib/python3.12/site-packages (from librosa) (1.1.0)\n",
      "Requirement already satisfied: packaging in ./main_sound_recognition_FINAL-venv/lib/python3.12/site-packages (from lazy-loader>=0.1->librosa) (24.2)\n",
      "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in ./main_sound_recognition_FINAL-venv/lib/python3.12/site-packages (from numba>=0.51.0->librosa) (0.43.0)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in ./main_sound_recognition_FINAL-venv/lib/python3.12/site-packages (from pooch>=1.1->librosa) (4.3.6)\n",
      "Requirement already satisfied: requests>=2.19.0 in ./main_sound_recognition_FINAL-venv/lib/python3.12/site-packages (from pooch>=1.1->librosa) (2.32.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./main_sound_recognition_FINAL-venv/lib/python3.12/site-packages (from scikit-learn>=0.20.0->librosa) (3.5.0)\n",
      "Requirement already satisfied: cffi>=1.0 in ./main_sound_recognition_FINAL-venv/lib/python3.12/site-packages (from soundfile>=0.12.1->librosa) (1.17.1)\n",
      "Requirement already satisfied: pycparser in ./main_sound_recognition_FINAL-venv/lib/python3.12/site-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.22)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./main_sound_recognition_FINAL-venv/lib/python3.12/site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./main_sound_recognition_FINAL-venv/lib/python3.12/site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./main_sound_recognition_FINAL-venv/lib/python3.12/site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./main_sound_recognition_FINAL-venv/lib/python3.12/site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2024.8.30)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python_speech_features in ./main_sound_recognition_FINAL-venv/lib/python3.12/site-packages (0.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pydub in ./main_sound_recognition_FINAL-venv/lib/python3.12/site-packages (0.25.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy\n",
    "!pip install scipy\n",
    "!pip install matplotlib\n",
    "!pip install pyyaml\n",
    "\n",
    "!pip install librosa\n",
    "!pip install python_speech_features\n",
    "\n",
    "!pip install pydub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "CGxhU3OO3tz8"
   },
   "outputs": [],
   "source": [
    "import yaml\n",
    "from scipy import signal\n",
    "from csv import writer\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "import cv2\n",
    "import librosa\n",
    "import librosa.display\n",
    "from python_speech_features import mfcc\n",
    "import wave\n",
    "import audioop\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# compression rate\n",
    "from pydub import AudioSegment\n",
    "\n",
    "# Excel Export\n",
    "import pandas as pd\n",
    "from openpyxl.styles import Font"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18683,
     "status": "ok",
     "timestamp": 1708809950260,
     "user": {
      "displayName": "Thorben Schlieffen",
      "userId": "06537630395058694731"
     },
     "user_tz": -60
    },
    "id": "A_uNmIVF3eSl",
    "outputId": "19df4cc5-2f87-4a59-be7f-afd15c8d9044"
   },
   "outputs": [],
   "source": [
    "  # Optional\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JuvMI44hotqJ"
   },
   "source": [
    "### Copy & import relevant files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {
    "id": "dqqAeHkK4BC9"
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "# replace window='hanning' with window='hann' in the config_014_butter.yaml file\n",
    "with open(F\"{os.getenv(\"TONANALYSE_ACOUSTIC_INDICES_QUELLCODE_DIR\")}/yaml/config_014_butter.yaml\", 'r') as file:\n",
    "    content = file.read()\n",
    "content = content.replace('hanning', 'hann')\n",
    "with open(F\"{os.getenv(\"TONANALYSE_ACOUSTIC_INDICES_QUELLCODE_DIR\")}/yaml/config_014_butter.yaml\", 'w') as file:\n",
    "    file.write(content)\n",
    "\n",
    "\n",
    "shutil.copy(F\"{os.getenv(\"TONANALYSE_ACOUSTIC_INDICES_QUELLCODE_DIR\")}/compute_indice.py\", \".\")\n",
    "shutil.copy(F\"{os.getenv(\"TONANALYSE_ACOUSTIC_INDICES_QUELLCODE_DIR\")}/acoustic_index.py\", \".\")\n",
    "shutil.copy(F\"{os.getenv(\"TONANALYSE_ACOUSTIC_INDICES_QUELLCODE_DIR\")}/yaml/config_014_butter.yaml\", \".\")\n",
    "\n",
    "config_file = os.path.join(\"./config_014_butter.yaml\")\n",
    "\n",
    "audio_dir = os.getenv(\"ADS_DIR\")\n",
    "output_dir = os.getenv(\"OUTPUT_BILD_PLUS_TON_LISTS_DIR\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {
    "id": "6OyVSFqEo69Q"
   },
   "outputs": [],
   "source": [
    "# import sys\n",
    "# sys.path.append('/content/drive/MyDrive/SuperBowl_Project_FUB/Colab_Notebooks/Acoustic_Indices')\n",
    "\n",
    "from compute_indice import *\n",
    "from acoustic_index import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HHJzI9D7pJH9"
   },
   "source": [
    "### Run the Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_output_file = False\n",
    "config_file = os.path.join(os.getenv(\"TONANALYSE_ACOUSTIC_INDICES_QUELLCODE_DIR\"), \"yaml\", \"config_014_butter.yaml\")\n",
    "audio_dir = os.getenv(\"ADS_DIR\")\n",
    "output_dir = os.getenv(\"OUTPUT_BILD_PLUS_TON_LISTS_DIR\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 193
    },
    "executionInfo": {
     "elapsed": 270,
     "status": "error",
     "timestamp": 1708809965539,
     "user": {
      "displayName": "Thorben Schlieffen",
      "userId": "06537630395058694731"
     },
     "user_tz": -60
    },
    "id": "bD_Pds7h4XrV",
    "outputId": "13cff867-5690-4225-936d-23d10f5d67d0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config file:  /home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/Final_Files/02. Tonanalyse/Acoustic_Indices/00 Quellcode/yaml/config_014_butter.yaml\n"
     ]
    }
   ],
   "source": [
    "# Set config file\n",
    "yml_file = config_file\n",
    "print(\"Config file: \", yml_file)\n",
    "with open(yml_file, 'r') as stream:\n",
    "    data_config = yaml.load(stream, Loader=yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 299,
     "status": "ok",
     "timestamp": 1708808977152,
     "user": {
      "displayName": "Thorben Schlieffen",
      "userId": "06537630395058694731"
     },
     "user_tz": -60
    },
    "id": "_Ly6BYmJ4aiF",
    "outputId": "a9b7f926-c3d3-47ed-cc80-23bb0c3de9c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- 2 files found in the directory /home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/ADs :\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get audio files\n",
    "all_audio_file_path = []\n",
    "for path, subdirs, files in os.walk(audio_dir):\n",
    "    for name in files:\n",
    "        if name.endswith(\".wav\") and not name.startswith(\".\"):\n",
    "            all_audio_file_path.append(os.path.join(path, name))\n",
    "\n",
    "all_audio_file_path = sorted(all_audio_file_path)\n",
    "\n",
    "print(\"-\", len(all_audio_file_path), \"files found in the directory\", audio_dir, ':\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ay5j1mBspcaz"
   },
   "source": [
    "### In case the ad list needs to be modified, use the list below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "id": "uHSMrIdPpoYu"
   },
   "outputs": [],
   "source": [
    "#all_audio_file_path = []\n",
    "#print(all_audio_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "id": "Z8ps4GcE4eFk"
   },
   "outputs": [],
   "source": [
    "# Initialize an empty DataFrame for all ads\n",
    "all_data = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A3um4wUgpsng"
   },
   "source": [
    "### additional values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {
    "id": "69jImfOx4hCI"
   },
   "outputs": [],
   "source": [
    "def calculate_additional_values(y):\n",
    "    duration = librosa.get_duration(y=y)\n",
    "    tempo, _ = librosa.beat.beat_track(y=y)\n",
    "    db_values = librosa.amplitude_to_db(np.abs(librosa.stft(y)), ref=np.max)\n",
    "    avg_db = np.mean(db_values)\n",
    "    min_db = np.min(db_values)\n",
    "    max_db = np.max(db_values)\n",
    "    max_db_value = np.max(db_values)\n",
    "    return duration, tempo, avg_db, min_db, max_db, max_db_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {
    "id": "EpIplz6wqOYq"
   },
   "outputs": [],
   "source": [
    "def compress_wav_to_mp3(input_wav_path, output_mp3_path, bitrate='192k'):\n",
    "    audio = AudioSegment.from_wav(input_wav_path)\n",
    "    audio.export(output_mp3_path, format='mp3', bitrate=bitrate)\n",
    "\n",
    "def measure_compression_ratio(original_size, compressed_size, original_duration):\n",
    "    compression_ratio = ((original_size - compressed_size) / original_size) * 100\n",
    "    return compression_ratio\n",
    "\n",
    "def measure_compression_ratio_per_second(compression_ratio, original_duration):\n",
    "    compression_ratio_per_second = compression_ratio / original_duration\n",
    "    return compression_ratio_per_second\n",
    "\n",
    "def delete_file(file_path):\n",
    "    \"\"\"\n",
    "    Delete a file if it exists.\n",
    "\n",
    "    Parameters:\n",
    "    - file_path: Path to the file to be deleted.\n",
    "    \"\"\"\n",
    "    if os.path.exists(file_path):\n",
    "        os.remove(file_path)\n",
    "        print(f\"File {file_path} deleted.\")\n",
    "    else:\n",
    "        print(f\"File {file_path} does not exist.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MNVJCe7tqWYS"
   },
   "source": [
    "### Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "id": "IgEqSfcs4p77"
   },
   "outputs": [],
   "source": [
    "#parser = argparse.ArgumentParser()\n",
    "#parser.add_argument(\"config_file\", help='yaml config file', nargs='?', const='yaml/config_014_butter.yaml', default='yaml/config_014_butter.yaml', type=str)\n",
    "#parser.add_argument(\"audio_dir\", help='audio directory', nargs='?', const='audio_files', default='audio_files', type=str)\n",
    "#parser.add_argument(\"output_csv_file\", help='output csv file', nargs='?', const='dict_all.csv', default='dict_all.csv', type=str)\n",
    "#args =parser.parse_args()\n",
    "\n",
    "if single_output_file:\n",
    "\n",
    "  print(\"audio directory: \", audio_dir)\n",
    "  print(\"output_excel_file: \", output_excel_file)\n",
    "\n",
    "  for idx_file, filename in enumerate(all_audio_file_path):\n",
    "\n",
    "      print(f'###### CURRENT AD: {filename} ######')\n",
    "      print(f'###### - {all_audio_file_path.index(filename)} / {len(all_audio_file_path)} - ######')\n",
    "\n",
    "      # Read signal -------------------------------------\n",
    "      file = AudioFile(filename, verbose=True)\n",
    "\n",
    "      # Pre-processing -----------------------------------------------------------------------------------\n",
    "      if 'Filtering' in data_config:\n",
    "          if data_config['Filtering']['type'] == 'butterworth':\n",
    "              print('- Pre-processing - High-Pass Filtering:', data_config['Filtering'])\n",
    "              freq_filter = data_config['Filtering']['frequency']\n",
    "              Wn = freq_filter/float(file.niquist)\n",
    "              order = data_config['Filtering']['order']\n",
    "              [b,a] = signal.butter(order, Wn, btype='highpass')\n",
    "              # to plot the frequency response\n",
    "              #w, h = signal.freqz(b, a, worN=2000)\n",
    "              #plt.plot((file.sr * 0.5 / np.pi) * w, abs(h))\n",
    "              #plt.show()\n",
    "              file.process_filtering(signal.filtfilt(b, a, file.sig_float))\n",
    "          elif data_config['Filtering']['type'] == 'windowed_sinc':\n",
    "              print('- Pre-processing - High-Pass Filtering:', data_config['Filtering'])\n",
    "              freq_filter = data_config['Filtering']['frequency']\n",
    "              fc = freq_filter / float(file.sr)\n",
    "              roll_off = data_config['Filtering']['roll_off']\n",
    "              b = roll_off / float(file.sr)\n",
    "              N = int(np.ceil((4 / b)))\n",
    "              if not N % 2: N += 1  # Make sure that N is odd.\n",
    "              n = np.arange(N)\n",
    "              # Compute a low-pass filter.\n",
    "              h = np.sinc(2 * fc * (n - (N - 1) / 2.))\n",
    "              w = np.blackman(N)\n",
    "              h = h * w\n",
    "              h = h / np.sum(h)\n",
    "              # Create a high-pass filter from the low-pass filter through spectral inversion.\n",
    "              h = -h\n",
    "              h[(N - 1) / 2] += 1\n",
    "              file.process_filtering(np.convolve(file.sig_float, h))\n",
    "\n",
    "      # Compute Indices -----------------------------------------------------------------------------------\n",
    "      print('- Compute Indices')\n",
    "      ci = data_config['Indices']  # use to simplify the notation\n",
    "      for index_name in ci:  # iterate over the index names (key of dictionary in the yml file)\n",
    "\n",
    "          if index_name == 'Acoustic_Complexity_Index':\n",
    "              print('\\tCompute', index_name)\n",
    "              spectro, _ = compute_spectrogram(file, **ci[index_name]['spectro'])\n",
    "              methodToCall = globals().get(ci[index_name]['function'])\n",
    "              j_bin = int(ci[index_name]['arguments']['j_bin'] * file.sr / ci[index_name]['spectro']['windowHop'])  # transform j_bin in samples\n",
    "              main_value, temporal_values = methodToCall(spectro, j_bin)\n",
    "              file.indices[index_name] = Index(index_name, temporal_values=temporal_values, main_value=main_value)\n",
    "\n",
    "          elif index_name == 'Acoustic_Diversity_Index':\n",
    "              print('\\tCompute', index_name)\n",
    "              methodToCall = globals().get(ci[index_name]['function'])\n",
    "              freq_band_Hz = ci[index_name]['arguments']['max_freq'] / ci[index_name]['arguments']['freq_step']\n",
    "              windowLength = int(file.sr / freq_band_Hz)\n",
    "              spectro, _ = compute_spectrogram(file, windowLength=windowLength, windowHop=windowLength, scale_audio=True, square=False, windowType='hamming', centered=False, normalized=False)\n",
    "              main_value = methodToCall(spectro, freq_band_Hz, **ci[index_name]['arguments'])\n",
    "              file.indices[index_name] = Index(index_name, main_value=main_value)\n",
    "\n",
    "          elif index_name == 'Acoustic_Evenness_Index':\n",
    "              print('\\tCompute', index_name)\n",
    "              methodToCall = globals().get(ci[index_name]['function'])\n",
    "              freq_band_Hz = ci[index_name]['arguments']['max_freq'] / ci[index_name]['arguments']['freq_step']\n",
    "              windowLength = int(file.sr / freq_band_Hz)\n",
    "              spectro, _ = compute_spectrogram(file, windowLength=windowLength, windowHop=windowLength, scale_audio=True, square=False, windowType='hamming', centered=False, normalized=False)\n",
    "              main_value = methodToCall(spectro, freq_band_Hz, **ci[index_name]['arguments'])\n",
    "              file.indices[index_name] = Index(index_name, main_value=main_value)\n",
    "\n",
    "          elif index_name == 'Bio_acoustic_Index':\n",
    "              print('\\tCompute', index_name)\n",
    "              spectro, frequencies = compute_spectrogram(file, **ci[index_name]['spectro'])\n",
    "              methodToCall = globals().get(ci[index_name]['function'])\n",
    "              main_value = methodToCall(spectro, frequencies, **ci[index_name]['arguments'])\n",
    "              file.indices[index_name] = Index(index_name, main_value=main_value)\n",
    "\n",
    "          elif index_name == 'Normalized_Difference_Sound_Index':\n",
    "              print('\\tCompute', index_name)\n",
    "              methodToCall = globals().get(ci[index_name]['function'])\n",
    "              main_value = methodToCall(file, **ci[index_name]['arguments'])\n",
    "              file.indices[index_name] = Index(index_name, main_value=main_value)\n",
    "\n",
    "          elif index_name == 'RMS_energy':\n",
    "              print('\\tCompute', index_name)\n",
    "              methodToCall = globals().get(ci[index_name]['function'])\n",
    "              temporal_values = methodToCall(file, **ci[index_name]['arguments'])\n",
    "              file.indices[index_name] = Index(index_name, temporal_values=temporal_values)\n",
    "\n",
    "          elif index_name == 'Spectral_centroid':\n",
    "              print('\\tCompute', index_name)\n",
    "              spectro, frequencies = compute_spectrogram(file, **ci[index_name]['spectro'])\n",
    "              methodToCall = globals().get(ci[index_name]['function'])\n",
    "              temporal_values = methodToCall(spectro, frequencies)\n",
    "              file.indices[index_name] = Index(index_name, temporal_values=temporal_values)\n",
    "\n",
    "          elif index_name == 'Spectral_Entropy':\n",
    "              print('\\tCompute', index_name)\n",
    "              spectro, _ = compute_spectrogram(file, **ci[index_name]['spectro'])\n",
    "              methodToCall = globals().get(ci[index_name]['function'])\n",
    "              main_value = methodToCall(spectro)\n",
    "              file.indices[index_name] = Index(index_name, main_value=main_value)\n",
    "\n",
    "          elif index_name == 'Temporal_Entropy':\n",
    "              print('\\tCompute', index_name)\n",
    "              methodToCall = globals().get(ci[index_name]['function'])\n",
    "              main_value = methodToCall(file, **ci[index_name]['arguments'])\n",
    "              file.indices[index_name] = Index(index_name, main_value=main_value)\n",
    "\n",
    "          elif index_name == 'ZCR':\n",
    "              print('\\tCompute', index_name)\n",
    "              methodToCall = globals().get(ci[index_name]['function'])\n",
    "              temporal_values = methodToCall(file, **ci[index_name]['arguments'])\n",
    "              file.indices[index_name] = Index(index_name, temporal_values=temporal_values)\n",
    "\n",
    "          elif index_name == 'Wave_SNR':\n",
    "              print('\\tCompute', index_name)\n",
    "              methodToCall = globals().get(ci[index_name]['function'])\n",
    "              values = methodToCall(file, **ci[index_name]['arguments'])\n",
    "              file.indices[index_name] = Index(index_name, values=values)\n",
    "\n",
    "          elif index_name == 'NB_peaks':\n",
    "              print('\\tCompute', index_name)\n",
    "              spectro, frequencies = compute_spectrogram(file, **ci[index_name]['spectro'])\n",
    "              methodToCall = globals().get(ci[index_name]['function'])\n",
    "              main_value = methodToCall(spectro, frequencies, **ci[index_name]['arguments'])\n",
    "              file.indices[index_name] = Index(index_name, main_value=main_value)\n",
    "\n",
    "          elif index_name == 'Acoustic_Diversity_Index_NR': # Acoustic_Diversity_Index with Noise Removed spectrograms\n",
    "              print('\\tCompute', index_name)\n",
    "              methodToCall = globals().get(ci[index_name]['function'])\n",
    "              freq_band_Hz = ci[index_name]['arguments']['max_freq'] / ci[index_name]['arguments']['freq_step']\n",
    "              windowLength = int(file.sr / freq_band_Hz)\n",
    "              spectro,_ = compute_spectrogram(file, windowLength=windowLength, windowHop= windowLength, scale_audio=True, square=False, windowType='hamming', centered=False, normalized= False )\n",
    "              spectro_noise_removed = remove_noiseInSpectro(spectro, **ci[index_name]['remove_noiseInSpectro'])\n",
    "              main_value = methodToCall(spectro_noise_removed, freq_band_Hz, **ci[index_name]['arguments'])\n",
    "              file.indices[index_name] = Index(index_name, main_value=main_value)\n",
    "\n",
    "          elif index_name == 'Acoustic_Evenness_Index_NR': # Acoustic_Evenness_Index with Noise Removed spectrograms\n",
    "              print('\\tCompute', index_name)\n",
    "              methodToCall = globals().get(ci[index_name]['function'])\n",
    "              freq_band_Hz = ci[index_name]['arguments']['max_freq'] / ci[index_name]['arguments']['freq_step']\n",
    "              windowLength = int(file.sr / freq_band_Hz)\n",
    "              spectro,_ = compute_spectrogram(file, windowLength=windowLength, windowHop= windowLength, scale_audio=True, square=False, windowType='hamming', centered=False, normalized= False )\n",
    "              spectro_noise_removed = remove_noiseInSpectro(spectro, **ci[index_name]['remove_noiseInSpectro'])\n",
    "              main_value = methodToCall(spectro_noise_removed, freq_band_Hz, **ci[index_name]['arguments'])\n",
    "              file.indices[index_name] = Index(index_name, main_value=main_value)\n",
    "\n",
    "          elif index_name == 'Bio_acoustic_Index_NR': # Bio_acoustic_Index with Noise Removed spectrograms\n",
    "              print('\\tCompute', index_name)\n",
    "              spectro, frequencies = compute_spectrogram(file, **ci[index_name]['spectro'])\n",
    "              spectro_noise_removed = remove_noiseInSpectro(spectro, **ci[index_name]['remove_noiseInSpectro'])\n",
    "              methodToCall = globals().get(ci[index_name]['function'])\n",
    "              main_value = methodToCall(spectro_noise_removed, frequencies, **ci[index_name]['arguments'])\n",
    "              file.indices[index_name] = Index(index_name, main_value=main_value)\n",
    "\n",
    "          elif index_name == 'Spectral_Entropy_NR': # Spectral_Entropy with Noise Removed spectrograms\n",
    "              print('\\tCompute', index_name)\n",
    "              spectro, _ = compute_spectrogram(file, **ci[index_name]['spectro'])\n",
    "              spectro_noise_removed = remove_noiseInSpectro(spectro, **ci[index_name]['remove_noiseInSpectro'])\n",
    "              methodToCall = globals().get(ci[index_name]['function'])\n",
    "              main_value = methodToCall(spectro_noise_removed)\n",
    "              file.indices[index_name] = Index(index_name, main_value=main_value)\n",
    "\n",
    "      # Output Indices -----------------------------------------------------------------------------------\n",
    "      #if idx_file == 0: # wenn ertse Datei\n",
    "      #    with open(output_csv_file, 'w') as f_object:\n",
    "      #        writer_object = writer(f_object)\n",
    "      #        keys = ['filename']\n",
    "      #        values = [file.file_name]\n",
    "      #        for idx, current_index in file.indices.items():\n",
    "      #            for key, value in current_index.__dict__.items():\n",
    "      #                if key != 'name':\n",
    "      #                    keys.append(idx + '__' + key)\n",
    "      #                    values.append(value)\n",
    "      #        writer_object.writerow(keys)\n",
    "      #        writer_object.writerow(values)\n",
    "      #        f_object.close()\n",
    "      #else: # alles nach der ersten Datei\n",
    "      #    with open(output_csv_file, 'a') as f_object:\n",
    "      #        writer_object = writer(f_object)\n",
    "      #        values = [file.file_name]\n",
    "      #        for idx, current_index in file.indices.items():\n",
    "      #            for key, value in current_index.__dict__.items():\n",
    "      #                if key != 'name':\n",
    "      #                    values.append(value)\n",
    "      #        writer_object.writerow(values)\n",
    "      #        f_object.close()\n",
    "      #print(\"\\n\")\n",
    "      # Create a dictionary to store data for the current file\n",
    "\n",
    "      file_data = {'filename': file.file_name}\n",
    "\n",
    "      for idx, current_index in file.indices.items():\n",
    "          for key, value in current_index.__dict__.items():\n",
    "              if key != 'name':\n",
    "                  file_data[idx + '__' + key] = value\n",
    "\n",
    "      # Calculate additional values\n",
    "      additional_values = calculate_additional_values(file.sig_float)\n",
    "\n",
    "      # compression rate\n",
    "      original_size = os.path.getsize(filename)\n",
    "      audio = AudioSegment.from_wav(filename)\n",
    "      original_duration = audio.duration_seconds\n",
    "      mp3_path = os.path.join(filename.replace(\".wav\", \".mp3\"))\n",
    "      compress_wav_to_mp3(filename, mp3_path)\n",
    "      compressed_size = os.path.getsize(mp3_path)\n",
    "      compression_ratio = measure_compression_ratio(original_size, compressed_size, original_duration)\n",
    "      compression_ratio_per_second = measure_compression_ratio_per_second(compression_ratio, original_duration)\n",
    "      delete_file(mp3_path)\n",
    "\n",
    "      # Append additional values to the file_data dictionary\n",
    "      duration, tempo, avg_db, min_db, max_db, max_db_value = additional_values\n",
    "      file_data['duration'] = duration\n",
    "      file_data['tempo'] = tempo\n",
    "      file_data['avg_db'] = avg_db\n",
    "      file_data['min_db'] = min_db\n",
    "      file_data['max_db'] = max_db\n",
    "      file_data['max_db_value'] = max_db_value\n",
    "      file_data['compression_ratio'] = compression_ratio\n",
    "      file_data['compression_ratio_per_second'] = compression_ratio_per_second\n",
    "\n",
    "      # Append the data for the current file to the DataFrame\n",
    "      all_data = all_data.append(file_data, ignore_index=True)\n",
    "\n",
    "  #To Excel\n",
    "  with pd.ExcelWriter(output_excel_file, engine='openpyxl') as writer:\n",
    "    all_data.to_excel(writer, sheet_name='Acoustic_Indices', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 765261,
     "status": "ok",
     "timestamp": 1708809758797,
     "user": {
      "displayName": "Thorben Schlieffen",
      "userId": "06537630395058694731"
     },
     "user_tz": -60
    },
    "id": "REsHzfJY4rGb",
    "outputId": "414a3890-bff3-445e-f8ae-da9ca87c79f8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "audio directory:  /home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/ADs\n",
      "output_directory:  /home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/Final_Files/03. Output Bild + Ton/01. output_lists\n",
      "###### CURRENT AD: /home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/ADs/ADs_IG_2013/AD0253.wav ######\n",
      "###### - 0 / 2 - ######\n",
      "Read the audio file: /home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/ADs/ADs_IG_2013/AD0253.wav\n",
      "\tSuccessful read of the audio file: /home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/ADs/ADs_IG_2013/AD0253.wav\n",
      "\tThe audio file contains more than one channel. Only the channel 0 will be used.\n",
      "- Pre-processing - High-Pass Filtering: {'type': 'butterworth', 'order': 8, 'frequency': 300}\n",
      "- Compute Indices\n",
      "\tCompute Acoustic_Complexity_Index\n",
      "\tCompute Acoustic_Diversity_Index\n",
      "\tCompute Acoustic_Evenness_Index\n",
      "\tCompute Bio_acoustic_Index\n",
      "\tCompute Normalized_Difference_Sound_Index\n",
      "\tCompute RMS_energy\n",
      "\tCompute Spectral_centroid\n",
      "\tCompute Spectral_Entropy\n",
      "\tCompute Temporal_Entropy\n",
      "\tCompute ZCR\n",
      "\tCompute Wave_SNR\n",
      "\tCompute NB_peaks\n",
      "\tCompute Acoustic_Diversity_Index_NR\n",
      "\tCompute Acoustic_Evenness_Index_NR\n",
      "\tCompute Bio_acoustic_Index_NR\n",
      "\tCompute Spectral_Entropy_NR\n",
      "File /home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/ADs/ADs_IG_2013/AD0253.mp3 deleted.\n",
      "/home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/Final_Files/03. Output Bild + Ton/01. output_lists/ADs_IG_2013/AD0253.xlsx\n",
      "###### CURRENT AD: /home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/ADs/ADs_IG_2014/AD0301.wav ######\n",
      "###### - 1 / 2 - ######\n",
      "Read the audio file: /home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/ADs/ADs_IG_2014/AD0301.wav\n",
      "\tSuccessful read of the audio file: /home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/ADs/ADs_IG_2014/AD0301.wav\n",
      "\tThe audio file contains more than one channel. Only the channel 0 will be used.\n",
      "- Pre-processing - High-Pass Filtering: {'type': 'butterworth', 'order': 8, 'frequency': 300}\n",
      "- Compute Indices\n",
      "\tCompute Acoustic_Complexity_Index\n",
      "\tCompute Acoustic_Diversity_Index\n",
      "\tCompute Acoustic_Evenness_Index\n",
      "\tCompute Bio_acoustic_Index\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_53646/3521325419.py:238: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  all_data = all_data.append(file_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tCompute Normalized_Difference_Sound_Index\n",
      "\tCompute RMS_energy\n",
      "\tCompute Spectral_centroid\n",
      "\tCompute Spectral_Entropy\n",
      "\tCompute Temporal_Entropy\n",
      "\tCompute ZCR\n",
      "\tCompute Wave_SNR\n",
      "\tCompute NB_peaks\n",
      "\tCompute Acoustic_Diversity_Index_NR\n",
      "\tCompute Acoustic_Evenness_Index_NR\n",
      "\tCompute Bio_acoustic_Index_NR\n",
      "\tCompute Spectral_Entropy_NR\n",
      "File /home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/ADs/ADs_IG_2014/AD0301.mp3 deleted.\n",
      "/home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/Final_Files/03. Output Bild + Ton/01. output_lists/ADs_IG_2014/AD0301.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_53646/3521325419.py:238: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  all_data = all_data.append(file_data, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "if not single_output_file:\n",
    "\n",
    "  print(\"audio directory: \", audio_dir)\n",
    "  print(\"output_directory: \", output_dir)\n",
    "\n",
    "  for idx_file, filename in enumerate(all_audio_file_path):\n",
    "\n",
    "      ad_name = filename.split('/')[-1]\n",
    "      year_folder_name = filename.split('/')[-2]\n",
    "      #print(ad_name, year_folder_name)\n",
    "\n",
    "      print(f'###### CURRENT AD: {filename} ######')\n",
    "      print(f'###### - {all_audio_file_path.index(filename)} / {len(all_audio_file_path)} - ######')\n",
    "\n",
    "      # Initialize an empty DataFrame for each individual ad\n",
    "      all_data = pd.DataFrame()\n",
    "\n",
    "      # Read signal -------------------------------------\n",
    "      file = AudioFile(filename, verbose=True)\n",
    "\n",
    "      # Pre-processing -----------------------------------------------------------------------------------\n",
    "      if 'Filtering' in data_config:\n",
    "          if data_config['Filtering']['type'] == 'butterworth':\n",
    "              print('- Pre-processing - High-Pass Filtering:', data_config['Filtering'])\n",
    "              freq_filter = data_config['Filtering']['frequency']\n",
    "              Wn = freq_filter/float(file.niquist)\n",
    "              order = data_config['Filtering']['order']\n",
    "              [b,a] = signal.butter(order, Wn, btype='highpass')\n",
    "              # to plot the frequency response\n",
    "              #w, h = signal.freqz(b, a, worN=2000)\n",
    "              #plt.plot((file.sr * 0.5 / np.pi) * w, abs(h))\n",
    "              #plt.show()\n",
    "              file.process_filtering(signal.filtfilt(b, a, file.sig_float))\n",
    "          elif data_config['Filtering']['type'] == 'windowed_sinc':\n",
    "              print('- Pre-processing - High-Pass Filtering:', data_config['Filtering'])\n",
    "              freq_filter = data_config['Filtering']['frequency']\n",
    "              fc = freq_filter / float(file.sr)\n",
    "              roll_off = data_config['Filtering']['roll_off']\n",
    "              b = roll_off / float(file.sr)\n",
    "              N = int(np.ceil((4 / b)))\n",
    "              if not N % 2: N += 1  # Make sure that N is odd.\n",
    "              n = np.arange(N)\n",
    "              # Compute a low-pass filter.\n",
    "              h = np.sinc(2 * fc * (n - (N - 1) / 2.))\n",
    "              w = np.blackman(N)\n",
    "              h = h * w\n",
    "              h = h / np.sum(h)\n",
    "              # Create a high-pass filter from the low-pass filter through spectral inversion.\n",
    "              h = -h\n",
    "              h[(N - 1) / 2] += 1\n",
    "              file.process_filtering(np.convolve(file.sig_float, h))\n",
    "\n",
    "      # Compute Indices -----------------------------------------------------------------------------------\n",
    "      print('- Compute Indices')\n",
    "      ci = data_config['Indices']  # use to simplify the notation\n",
    "      for index_name in ci:  # iterate over the index names (key of dictionary in the yml file)\n",
    "\n",
    "          if index_name == 'Acoustic_Complexity_Index':\n",
    "              print('\\tCompute', index_name)\n",
    "              spectro, _ = compute_spectrogram(file, **ci[index_name]['spectro'])\n",
    "              methodToCall = globals().get(ci[index_name]['function'])\n",
    "              j_bin = int(ci[index_name]['arguments']['j_bin'] * file.sr / ci[index_name]['spectro']['windowHop'])  # transform j_bin in samples\n",
    "              main_value, temporal_values = methodToCall(spectro, j_bin)\n",
    "              file.indices[index_name] = Index(index_name, temporal_values=temporal_values, main_value=main_value)\n",
    "\n",
    "          elif index_name == 'Acoustic_Diversity_Index':\n",
    "              print('\\tCompute', index_name)\n",
    "              methodToCall = globals().get(ci[index_name]['function'])\n",
    "              freq_band_Hz = ci[index_name]['arguments']['max_freq'] / ci[index_name]['arguments']['freq_step']\n",
    "              windowLength = int(file.sr / freq_band_Hz)\n",
    "              spectro, _ = compute_spectrogram(file, windowLength=windowLength, windowHop=windowLength, scale_audio=True, square=False, windowType='hamming', centered=False, normalized=False)\n",
    "              main_value = methodToCall(spectro, freq_band_Hz, **ci[index_name]['arguments'])\n",
    "              file.indices[index_name] = Index(index_name, main_value=main_value)\n",
    "\n",
    "          elif index_name == 'Acoustic_Evenness_Index':\n",
    "              print('\\tCompute', index_name)\n",
    "              methodToCall = globals().get(ci[index_name]['function'])\n",
    "              freq_band_Hz = ci[index_name]['arguments']['max_freq'] / ci[index_name]['arguments']['freq_step']\n",
    "              windowLength = int(file.sr / freq_band_Hz)\n",
    "              spectro, _ = compute_spectrogram(file, windowLength=windowLength, windowHop=windowLength, scale_audio=True, square=False, windowType='hamming', centered=False, normalized=False)\n",
    "              main_value = methodToCall(spectro, freq_band_Hz, **ci[index_name]['arguments'])\n",
    "              file.indices[index_name] = Index(index_name, main_value=main_value)\n",
    "\n",
    "          elif index_name == 'Bio_acoustic_Index':\n",
    "              print('\\tCompute', index_name)\n",
    "              spectro, frequencies = compute_spectrogram(file, **ci[index_name]['spectro'])\n",
    "              methodToCall = globals().get(ci[index_name]['function'])\n",
    "              main_value = methodToCall(spectro, frequencies, **ci[index_name]['arguments'])\n",
    "              file.indices[index_name] = Index(index_name, main_value=main_value)\n",
    "\n",
    "          elif index_name == 'Normalized_Difference_Sound_Index':\n",
    "              print('\\tCompute', index_name)\n",
    "              methodToCall = globals().get(ci[index_name]['function'])\n",
    "              main_value = methodToCall(file, **ci[index_name]['arguments'])\n",
    "              file.indices[index_name] = Index(index_name, main_value=main_value)\n",
    "\n",
    "          elif index_name == 'RMS_energy':\n",
    "              print('\\tCompute', index_name)\n",
    "              methodToCall = globals().get(ci[index_name]['function'])\n",
    "              temporal_values = methodToCall(file, **ci[index_name]['arguments'])\n",
    "              file.indices[index_name] = Index(index_name, temporal_values=temporal_values)\n",
    "\n",
    "          elif index_name == 'Spectral_centroid':\n",
    "              print('\\tCompute', index_name)\n",
    "              spectro, frequencies = compute_spectrogram(file, **ci[index_name]['spectro'])\n",
    "              methodToCall = globals().get(ci[index_name]['function'])\n",
    "              temporal_values = methodToCall(spectro, frequencies)\n",
    "              file.indices[index_name] = Index(index_name, temporal_values=temporal_values)\n",
    "\n",
    "          elif index_name == 'Spectral_Entropy':\n",
    "              print('\\tCompute', index_name)\n",
    "              spectro, _ = compute_spectrogram(file, **ci[index_name]['spectro'])\n",
    "              methodToCall = globals().get(ci[index_name]['function'])\n",
    "              main_value = methodToCall(spectro)\n",
    "              file.indices[index_name] = Index(index_name, main_value=main_value)\n",
    "\n",
    "          elif index_name == 'Temporal_Entropy':\n",
    "              print('\\tCompute', index_name)\n",
    "              methodToCall = globals().get(ci[index_name]['function'])\n",
    "              main_value = methodToCall(file, **ci[index_name]['arguments'])\n",
    "              file.indices[index_name] = Index(index_name, main_value=main_value)\n",
    "\n",
    "          elif index_name == 'ZCR':\n",
    "              print('\\tCompute', index_name)\n",
    "              methodToCall = globals().get(ci[index_name]['function'])\n",
    "              temporal_values = methodToCall(file, **ci[index_name]['arguments'])\n",
    "              file.indices[index_name] = Index(index_name, temporal_values=temporal_values)\n",
    "\n",
    "          elif index_name == 'Wave_SNR':\n",
    "              print('\\tCompute', index_name)\n",
    "              methodToCall = globals().get(ci[index_name]['function'])\n",
    "              values = methodToCall(file, **ci[index_name]['arguments'])\n",
    "              file.indices[index_name] = Index(index_name, values=values)\n",
    "\n",
    "          elif index_name == 'NB_peaks':\n",
    "              print('\\tCompute', index_name)\n",
    "              spectro, frequencies = compute_spectrogram(file, **ci[index_name]['spectro'])\n",
    "              methodToCall = globals().get(ci[index_name]['function'])\n",
    "              main_value = methodToCall(spectro, frequencies, **ci[index_name]['arguments'])\n",
    "              file.indices[index_name] = Index(index_name, main_value=main_value)\n",
    "\n",
    "          elif index_name == 'Acoustic_Diversity_Index_NR': # Acoustic_Diversity_Index with Noise Removed spectrograms\n",
    "              print('\\tCompute', index_name)\n",
    "              methodToCall = globals().get(ci[index_name]['function'])\n",
    "              freq_band_Hz = ci[index_name]['arguments']['max_freq'] / ci[index_name]['arguments']['freq_step']\n",
    "              windowLength = int(file.sr / freq_band_Hz)\n",
    "              spectro,_ = compute_spectrogram(file, windowLength=windowLength, windowHop= windowLength, scale_audio=True, square=False, windowType='hamming', centered=False, normalized= False )\n",
    "              spectro_noise_removed = remove_noiseInSpectro(spectro, **ci[index_name]['remove_noiseInSpectro'])\n",
    "              main_value = methodToCall(spectro_noise_removed, freq_band_Hz, **ci[index_name]['arguments'])\n",
    "              file.indices[index_name] = Index(index_name, main_value=main_value)\n",
    "\n",
    "          elif index_name == 'Acoustic_Evenness_Index_NR': # Acoustic_Evenness_Index with Noise Removed spectrograms\n",
    "              print('\\tCompute', index_name)\n",
    "              methodToCall = globals().get(ci[index_name]['function'])\n",
    "              freq_band_Hz = ci[index_name]['arguments']['max_freq'] / ci[index_name]['arguments']['freq_step']\n",
    "              windowLength = int(file.sr / freq_band_Hz)\n",
    "              spectro,_ = compute_spectrogram(file, windowLength=windowLength, windowHop= windowLength, scale_audio=True, square=False, windowType='hamming', centered=False, normalized= False )\n",
    "              spectro_noise_removed = remove_noiseInSpectro(spectro, **ci[index_name]['remove_noiseInSpectro'])\n",
    "              main_value = methodToCall(spectro_noise_removed, freq_band_Hz, **ci[index_name]['arguments'])\n",
    "              file.indices[index_name] = Index(index_name, main_value=main_value)\n",
    "\n",
    "          elif index_name == 'Bio_acoustic_Index_NR': # Bio_acoustic_Index with Noise Removed spectrograms\n",
    "              print('\\tCompute', index_name)\n",
    "              spectro, frequencies = compute_spectrogram(file, **ci[index_name]['spectro'])\n",
    "              spectro_noise_removed = remove_noiseInSpectro(spectro, **ci[index_name]['remove_noiseInSpectro'])\n",
    "              methodToCall = globals().get(ci[index_name]['function'])\n",
    "              main_value = methodToCall(spectro_noise_removed, frequencies, **ci[index_name]['arguments'])\n",
    "              file.indices[index_name] = Index(index_name, main_value=main_value)\n",
    "\n",
    "          elif index_name == 'Spectral_Entropy_NR': # Spectral_Entropy with Noise Removed spectrograms\n",
    "              print('\\tCompute', index_name)\n",
    "              spectro, _ = compute_spectrogram(file, **ci[index_name]['spectro'])\n",
    "              spectro_noise_removed = remove_noiseInSpectro(spectro, **ci[index_name]['remove_noiseInSpectro'])\n",
    "              methodToCall = globals().get(ci[index_name]['function'])\n",
    "              main_value = methodToCall(spectro_noise_removed)\n",
    "              file.indices[index_name] = Index(index_name, main_value=main_value)\n",
    "\n",
    "      # Output Indices -----------------------------------------------------------------------------------\n",
    "      #if idx_file == 0: # wenn ertse Datei\n",
    "      #    with open(output_csv_file, 'w') as f_object:\n",
    "      #        writer_object = writer(f_object)\n",
    "      #        keys = ['filename']\n",
    "      #        values = [file.file_name]\n",
    "      #        for idx, current_index in file.indices.items():\n",
    "      #            for key, value in current_index.__dict__.items():\n",
    "      #                if key != 'name':\n",
    "      #                    keys.append(idx + '__' + key)\n",
    "      #                    values.append(value)\n",
    "      #        writer_object.writerow(keys)\n",
    "      #        writer_object.writerow(values)\n",
    "      #        f_object.close()\n",
    "      #else: # alles nach der ersten Datei\n",
    "      #    with open(output_csv_file, 'a') as f_object:\n",
    "      #        writer_object = writer(f_object)\n",
    "      #        values = [file.file_name]\n",
    "      #        for idx, current_index in file.indices.items():\n",
    "      #            for key, value in current_index.__dict__.items():\n",
    "      #                if key != 'name':\n",
    "      #                    values.append(value)\n",
    "      #        writer_object.writerow(values)\n",
    "      #        f_object.close()\n",
    "      #print(\"\\n\")\n",
    "      # Create a dictionary to store data for the current file\n",
    "\n",
    "      file_data = {'filename': file.file_name}\n",
    "\n",
    "      for idx, current_index in file.indices.items():\n",
    "          for key, value in current_index.__dict__.items():\n",
    "              if key != 'name':\n",
    "                  file_data[idx + '__' + key] = value\n",
    "\n",
    "      # Calculate additional values\n",
    "      additional_values = calculate_additional_values(file.sig_float)\n",
    "\n",
    "      # compression rate\n",
    "      original_size = os.path.getsize(filename)\n",
    "      audio = AudioSegment.from_wav(filename)\n",
    "      original_duration = audio.duration_seconds\n",
    "      mp3_path = os.path.join(filename.replace(\".wav\", \".mp3\"))\n",
    "      compress_wav_to_mp3(filename, mp3_path)\n",
    "      compressed_size = os.path.getsize(mp3_path)\n",
    "      compression_ratio = measure_compression_ratio(original_size, compressed_size, original_duration)\n",
    "      compression_ratio_per_second = measure_compression_ratio_per_second(compression_ratio, original_duration)\n",
    "      delete_file(mp3_path)\n",
    "\n",
    "      # Append additional values to the file_data dictionary\n",
    "      duration, tempo, avg_db, min_db, max_db, max_db_value = additional_values\n",
    "      file_data['duration'] = duration\n",
    "      file_data['tempo'] = tempo\n",
    "      file_data['avg_db'] = avg_db\n",
    "      file_data['min_db'] = min_db\n",
    "      file_data['max_db'] = max_db\n",
    "      file_data['max_db_value'] = max_db_value\n",
    "      file_data['compression_ratio'] = compression_ratio\n",
    "      file_data['compression_ratio_per_second'] = compression_ratio_per_second\n",
    "\n",
    "      # Append the data for the current file to the DataFrame\n",
    "      all_data = all_data.append(file_data, ignore_index=True)\n",
    "\n",
    "      output_xlsx_file = os.path.join(output_dir, year_folder_name[0:11] + \"/\" + ad_name[0:7] + \"xlsx\")\n",
    "\n",
    "      print(output_xlsx_file)\n",
    "\n",
    "      #To Excel\n",
    "      #with pd.ExcelWriter(output_xlsx_file, engine='openpyxl', mode='a', if_sheet_exists='overlay') as writer:\n",
    "      #  all_data.to_excel(writer, sheet_name='Acoustic_Indices', index=False)\n",
    "      try:\n",
    "          with pd.ExcelWriter(output_xlsx_file, engine='openpyxl', mode='a', if_sheet_exists='overlay') as writer:\n",
    "              all_data.to_excel(writer, sheet_name='Acoustic_Indices', index=False)\n",
    "\n",
    "      except Exception as e:\n",
    "          print(f\"Error: {e}\")\n",
    "\n",
    "          # Versuche, die Datei zu löschen (falls sie existiert)\n",
    "          if os.path.exists(output_xlsx_file):\n",
    "              os.remove(output_xlsx_file)\n",
    "              print(f\"File '{output_xlsx_file}' deleted.\")\n",
    "\n",
    "          # Erstelle eine neue leere Datei\n",
    "          with pd.ExcelWriter(output_xlsx_file, engine='openpyxl', mode='w') as writer:\n",
    "              all_data.to_excel(writer, sheet_name='Acoustic_Indices', index=False)\n",
    "              print(f\"New file '{output_xlsx_file}' created and written.\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "provenance": [
    {
     "file_id": "1e6gtuF1-nQAflcMlP0kbDHvSxYIb0aG3",
     "timestamp": 1705574462824
    },
    {
     "file_id": "https://github.com/pyannote/pyannote-audio/blob/develop/tutorials/intro.ipynb",
     "timestamp": 1703704769456
    }
   ]
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "2d52b4a045c14f44903b010261527ed6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "LabelModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "LabelModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "LabelView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_35ae3d30cf45414a98a606b2c9ba40be",
      "placeholder": "​",
      "style": "IPY_MODEL_504a095f33214bd4b36646d088b49e5e",
      "value": "Token is valid (permission: write)."
     }
    },
    "34222225ba97456694d7298c879b4b19": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "LabelModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "LabelModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "LabelView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a5865557a7474b0ebeed270b592f38ac",
      "placeholder": "​",
      "style": "IPY_MODEL_45d6c82b2ec74893ae2b5a86eeb1daff",
      "value": "Your token has been saved to /root/.cache/huggingface/token"
     }
    },
    "35ae3d30cf45414a98a606b2c9ba40be": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3ff01220d0d449f99775a49369910f63": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "LabelModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "LabelModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "LabelView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a9bf397a62944000ae68a482a8ae20c1",
      "placeholder": "​",
      "style": "IPY_MODEL_aa5d8610fb084fa59991f262fed05bad",
      "value": "Your token has been saved in your configured git credential helpers (store)."
     }
    },
    "45d6c82b2ec74893ae2b5a86eeb1daff": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4cbb11e0ee474f2eaf7ffa7e3583e9f8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "504a095f33214bd4b36646d088b49e5e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6c2729e3f4b84d349a112da2ccf77b25": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": "center",
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": "flex",
      "flex": null,
      "flex_flow": "column",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "50%"
     }
    },
    "8a7f1963cc7848a48b2c8a5ab1ff17a3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a5865557a7474b0ebeed270b592f38ac": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a9bf397a62944000ae68a482a8ae20c1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "aa5d8610fb084fa59991f262fed05bad": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c9ff155a579140e2bb19bfbadc4e032f": {
     "model_module": "@jupyter-widgets/output",
     "model_module_version": "1.0.0",
     "model_name": "OutputModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/output",
      "_model_module_version": "1.0.0",
      "_model_name": "OutputModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/output",
      "_view_module_version": "1.0.0",
      "_view_name": "OutputView",
      "layout": "IPY_MODEL_4cbb11e0ee474f2eaf7ffa7e3583e9f8",
      "msg_id": "",
      "outputs": [
       {
        "data": {
         "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">segmentation         <span style=\"color: #729c1f; text-decoration-color: #729c1f\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span> <span style=\"color: #800080; text-decoration-color: #800080\">100%</span> <span style=\"color: #808000; text-decoration-color: #808000\">0:00:00</span>\nspeaker_counting     <span style=\"color: #729c1f; text-decoration-color: #729c1f\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span> <span style=\"color: #800080; text-decoration-color: #800080\">100%</span> <span style=\"color: #808000; text-decoration-color: #808000\">0:00:00</span>\nembeddings           <span style=\"color: #729c1f; text-decoration-color: #729c1f\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span> <span style=\"color: #800080; text-decoration-color: #800080\">100%</span> <span style=\"color: #808000; text-decoration-color: #808000\">0:00:08</span>\ndiscrete_diarization <span style=\"color: #729c1f; text-decoration-color: #729c1f\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span> <span style=\"color: #800080; text-decoration-color: #800080\">100%</span> <span style=\"color: #808000; text-decoration-color: #808000\">0:00:00</span>\n</pre>\n",
         "text/plain": "segmentation         \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m100%\u001b[0m \u001b[33m0:00:00\u001b[0m\nspeaker_counting     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m100%\u001b[0m \u001b[33m0:00:00\u001b[0m\nembeddings           \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m100%\u001b[0m \u001b[33m0:00:08\u001b[0m\ndiscrete_diarization \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m100%\u001b[0m \u001b[33m0:00:00\u001b[0m\n"
        },
        "metadata": {},
        "output_type": "display_data"
       }
      ]
     }
    },
    "d0647072ab8044c7a1ff53b9e01f7189": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "VBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "VBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "VBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_2d52b4a045c14f44903b010261527ed6",
       "IPY_MODEL_3ff01220d0d449f99775a49369910f63",
       "IPY_MODEL_34222225ba97456694d7298c879b4b19",
       "IPY_MODEL_d9bce56fe264426b85e729cf46a3973f"
      ],
      "layout": "IPY_MODEL_6c2729e3f4b84d349a112da2ccf77b25"
     }
    },
    "d9bce56fe264426b85e729cf46a3973f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "LabelModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "LabelModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "LabelView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e9be5a21ad9046d181c258ea1487de5c",
      "placeholder": "​",
      "style": "IPY_MODEL_8a7f1963cc7848a48b2c8a5ab1ff17a3",
      "value": "Login successful"
     }
    },
    "e9be5a21ad9046d181c258ea1487de5c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
