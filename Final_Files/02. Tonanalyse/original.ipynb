{"cells":[{"cell_type":"markdown","metadata":{"id":"dE96Mw7hUjlE"},"source":["#**Automatic Audio Recognition**\n","This script includes 3 different models which analyze different parts of audio in super bowl ads.\n","1. *Gender specific speaking time* (and durations of speaking parts)\n","2. *Emotion recognition from Transcription* (uses only transcription from WhisperAI for analysis)\n","3. *Acoustic Indizes* (many different indicators like min/max_energy, db and tempo)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"54LsLpyqT-lM"},"source":["# *1. Gender specific speaking time*"]},{"cell_type":"markdown","metadata":{"id":"jLzGh7XjtR9V"},"source":["Initially, the audio file will be segmented before gender recognition can take place\n","\n","Audio segmentation: https://github.com/pyannote/pyannote-audio\n","\n","Gender recognition: https://github.com/x4nth055/gender-recognition-by-voice"]},{"cell_type":"markdown","metadata":{"id":"tckHJKZnYnp7"},"source":["### Installation"]},{"cell_type":"code","execution_count":45,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":115024,"status":"ok","timestamp":1708638592563,"user":{"displayName":"Thorben","userId":"17213772383075988098"},"user_tz":-60},"id":"IL25VsVSgzOw","outputId":"39d1663c-2a19-4910-d1cb-72952e68ea61"},"outputs":[],"source":["# !pip install -qq pyannote.audio==3.1.1\n","# !pip install -qq ipython==7.34.0\n","# !pip install -qq tensorflow\n","# !pip install -qq scikit-learn\n","# !pip install -qq numpy\n","# !pip install -qq pandas\n","# !pip install -qq tqdm\n","# !sudo apt-get install libportaudio2\n","# !sudo apt-get install python3-pyaudio\n","# !pip install -qq librosa\n","# !pip install -qq utils\n","\n","# !pip -qq install pydub\n","# !pip install tqdm\n","from pydub import AudioSegment\n","from tqdm import tqdm\n","from pyannote.audio import Pipeline\n","import torch\n","\n","import pyaudio\n","import os\n","import wave\n","import librosa\n","import numpy as np\n","import pandas as pd\n","import tqdm\n","import locale\n","\n","\n","\n","def getpreferredencoding(do_setlocale = True):\n","    return \"UTF-8\"\n","locale.getpreferredencoding = getpreferredencoding"]},{"cell_type":"markdown","metadata":{"id":"5MclWK2GYnp_"},"source":["### Mandatory Login\n","\n","To load the speaker diarization pipeline,\n","\n","* accept the user conditions on [hf.co/pyannote/speaker-diarization-3.1](https://hf.co/pyannote/speaker-diarization-3.1)\n","* accept the user conditions on [hf.co/pyannote/segmentation-3.0](https://hf.co/pyannote/segmentation-3.0)\n","* login using `notebook_login` below"]},{"cell_type":"code","execution_count":46,"metadata":{"id":"jh3EV9Y2wFXC"},"outputs":[],"source":["# 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"]},{"cell_type":"code","execution_count":47,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":145,"referenced_widgets":["d0647072ab8044c7a1ff53b9e01f7189","2d52b4a045c14f44903b010261527ed6","3ff01220d0d449f99775a49369910f63","34222225ba97456694d7298c879b4b19","d9bce56fe264426b85e729cf46a3973f","6c2729e3f4b84d349a112da2ccf77b25","35ae3d30cf45414a98a606b2c9ba40be","504a095f33214bd4b36646d088b49e5e","a9bf397a62944000ae68a482a8ae20c1","aa5d8610fb084fa59991f262fed05bad","a5865557a7474b0ebeed270b592f38ac","45d6c82b2ec74893ae2b5a86eeb1daff","e9be5a21ad9046d181c258ea1487de5c","8a7f1963cc7848a48b2c8a5ab1ff17a3"]},"executionInfo":{"elapsed":284,"status":"ok","timestamp":1708638630457,"user":{"displayName":"Thorben","userId":"17213772383075988098"},"user_tz":-60},"id":"r5u7VMb-YnqB","outputId":"1ddafd98-b25a-4ac0-b544-0b2a0419522e"},"outputs":[],"source":["import os\n","from dotenv import load_dotenv\n","from huggingface_hub import login\n","\n","# Load environment variables from the .env file\n","load_dotenv()\n","\n","# Retrieve the Hugging Face API token from the environment\n","hf_token = os.getenv(\"HF_API_KEY\")\n","# Log in to Hugging Face\n","login(token=hf_token)"]},{"cell_type":"markdown","metadata":{"id":"tjNgvcnOWRIF"},"source":["## Audio Splitter Method Definition\n","\n"]},{"cell_type":"markdown","metadata":{"id":"4eqTRtgxgc0A"},"source":["### Utils\n"]},{"cell_type":"code","execution_count":48,"metadata":{"id":"tRqtCGZegfjr"},"outputs":[{"name":"stdout","output_type":"stream","text":["Physical devices cannot be modified after being initialized\n"]}],"source":["from sys import byteorder\n","from array import array\n","from struct import pack\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, LSTM, Dropout\n","from sklearn.model_selection import train_test_split\n","\n","\n","label2int = {\n","    \"male\": 1,\n","    \"female\": 0\n","}\n","\n","\n","def load_data(vector_length=128):\n","    \"\"\"A function to load gender recognition dataset from `data` folder\n","    After the second run, this will load from results/features.npy and results/labels.npy files\n","    as it is much faster!\"\"\"\n","    # make sure results folder exists\n","    if not os.path.isdir(\"results\"):\n","        os.mkdir(\"results\")\n","    # if features & labels already loaded individually and bundled, load them from there instead\n","    if os.path.isfile(os.path.join(os.getenv(\"TONANALYSE_AUDIO_GENDER_NOTEBOOKS_DIR\"), \"results/features.npy\")) and os.path.isfile(\"results/labels.npy\"):\n","        X = np.load(os.path.join(os.getenv(\"TONANALYSE_AUDIO_GENDER_NOTEBOOKS_DIR\"), \"results/features.npy\"))\n","        y = np.load(os.path.join(os.getenv(\"TONANALYSE_AUDIO_GENDER_NOTEBOOKS_DIR\"), \"results/labels.npy\"))\n","        return X, y\n","    # read dataframe\n","    df = pd.read_csv(os.path.join(os.getenv(\"TONANALYSE_AUDIO_GENDER_NOTEBOOKS_DIR\"), \"balanced-all.csv\"))\n","    # get total samples\n","    n_samples = len(df)\n","    # get total male samples\n","    n_male_samples = len(df[df['gender'] == 'male'])\n","    # get total female samples\n","    n_female_samples = len(df[df['gender'] == 'female'])\n","    print(\"Total samples:\", n_samples)\n","    print(\"Total male samples:\", n_male_samples)\n","    print(\"Total female samples:\", n_female_samples)\n","    # initialize an empty array for all audio features\n","    X = np.zeros((n_samples, vector_length))\n","    # initialize an empty array for all audio labels (1 for male and 0 for female)\n","    y = np.zeros((n_samples, 1))\n","    for i, (filename, gender) in tqdm.tqdm(enumerate(zip(df['filename'], df['gender'])), \"Loading data\", total=n_samples):\n","        features = np.load(filename)\n","        X[i] = features\n","        y[i] = label2int[gender]\n","    # save the audio features and labels into files\n","    # so we won't load each one of them next run\n","    np.save(os.path.join(os.getenv(\"TONANALYSE_AUDIO_GENDER_NOTEBOOKS_DIR\"), \"results/features\"), X)\n","    np.save(os.path.join(os.getenv(\"TONANALYSE_AUDIO_GENDER_NOTEBOOKS_DIR\"), \"results/\"), y)\n","    return X, y\n","\n","\n","def split_data(X, y, test_size=0.1, valid_size=0.1):\n","    # split training set and testing set\n","    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=7)\n","    # split training set and validation set\n","    X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=valid_size, random_state=7)\n","    # return a dictionary of values\n","    return {\n","        \"X_train\": X_train,\n","        \"X_valid\": X_valid,\n","        \"X_test\": X_test,\n","        \"y_train\": y_train,\n","        \"y_valid\": y_valid,\n","        \"y_test\": y_test\n","    }\n","\n","import tensorflow as tf\n","gpus = tf.config.experimental.list_physical_devices('GPU')\n","if gpus:\n","  try:\n","    for gpu in gpus:\n","      tf.config.experimental.set_memory_growth(gpu, True)\n","  except RuntimeError as e:\n","    print(e)\n","def create_model(vector_length=128):\n","    print(\"creating model ...\")\n","    \"\"\"5 hidden dense layers from 256 units to 64, not the best model, but not bad.\"\"\"\n","    model = Sequential()\n","    model.add(Dense(256, input_shape=(vector_length,)))\n","    model.add(Dropout(0.3))\n","    model.add(Dense(256, activation=\"relu\"))\n","    model.add(Dropout(0.3))\n","    model.add(Dense(128, activation=\"relu\"))\n","    model.add(Dropout(0.3))\n","    model.add(Dense(128, activation=\"relu\"))\n","    model.add(Dropout(0.3))\n","    model.add(Dense(64, activation=\"relu\"))\n","    model.add(Dropout(0.3))\n","    # one output neuron with sigmoid activation function, 0 means female, 1 means male\n","    model.add(Dense(1, activation=\"sigmoid\"))\n","    # using binary crossentropy as it's male/female classification (binary)\n","    model.compile(loss=\"binary_crossentropy\", metrics=[\"accuracy\"], optimizer=\"adam\")\n","    # print summary of the model\n","    model.summary()\n","    return model\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"jkH61xIZgWXp"},"source":["### Test Definition"]},{"cell_type":"code","execution_count":49,"metadata":{"id":"7gWd9jx8WX3r"},"outputs":[],"source":["THRESHOLD = 500\n","CHUNK_SIZE = 1024\n","FORMAT = pyaudio.paInt16\n","RATE = 16000\n","\n","SILENCE = 30\n","\n","def is_silent(snd_data):\n","    \"Returns 'True' if below the 'silent' threshold\"\n","    return max(snd_data) < THRESHOLD\n","\n","def normalize(snd_data):\n","    \"Average the volume out\"\n","    MAXIMUM = 16384\n","    times = float(MAXIMUM)/max(abs(i) for i in snd_data)\n","\n","    r = array('h')\n","    for i in snd_data:\n","        r.append(int(i*times))\n","    return r\n","\n","def trim(snd_data):\n","    \"Trim the blank spots at the start and end\"\n","    def _trim(snd_data):\n","        snd_started = False\n","        r = array('h')\n","\n","        for i in snd_data:\n","            if not snd_started and abs(i)>THRESHOLD:\n","                snd_started = True\n","                r.append(i)\n","\n","            elif snd_started:\n","                r.append(i)\n","        return r\n","\n","    # Trim to the left\n","    snd_data = _trim(snd_data)\n","\n","    # Trim to the right\n","    snd_data.reverse()\n","    snd_data = _trim(snd_data)\n","    snd_data.reverse()\n","    return snd_data\n","\n","def add_silence(snd_data, seconds):\n","    \"Add silence to the start and end of 'snd_data' of length 'seconds' (float)\"\n","    r = array('h', [0 for i in range(int(seconds*RATE))])\n","    r.extend(snd_data)\n","    r.extend([0 for i in range(int(seconds*RATE))])\n","    return r\n","\n","\n","def extract_feature(file_name, **kwargs):\n","    \"\"\"\n","    Extract feature from audio file `file_name`\n","        Features supported:\n","            - MFCC (mfcc)\n","            - Chroma (chroma)\n","            - MEL Spectrogram Frequency (mel)\n","            - Contrast (contrast)\n","            - Tonnetz (tonnetz)\n","        e.g:\n","        `features = extract_feature(path, mel=True, mfcc=True)`\n","    \"\"\"\n","    mfcc = kwargs.get(\"mfcc\")\n","    chroma = kwargs.get(\"chroma\")\n","    mel = kwargs.get(\"mel\")\n","    contrast = kwargs.get(\"contrast\")\n","    tonnetz = kwargs.get(\"tonnetz\")\n","    X, sample_rate = librosa.core.load(file_name)\n","    if chroma or contrast:\n","        stft = np.abs(librosa.stft(X))\n","    result = np.array([])\n","    if mfcc:\n","        mfccs = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=40).T, axis=0)\n","        result = np.hstack((result, mfccs))\n","    if chroma:\n","        chroma = np.mean(librosa.feature.chroma_stft(S=stft, sr=sample_rate).T,axis=0)\n","        result = np.hstack((result, chroma))\n","    if mel:\n","        mel = np.mean(librosa.feature.melspectrogram(y=X, sr=sample_rate).T,axis=0)\n","        result = np.hstack((result, mel))\n","    if contrast:\n","        contrast = np.mean(librosa.feature.spectral_contrast(S=stft, sr=sample_rate).T,axis=0)\n","        result = np.hstack((result, contrast))\n","    if tonnetz:\n","        tonnetz = np.mean(librosa.feature.tonnetz(y=librosa.effects.harmonic(X), sr=sample_rate).T,axis=0)\n","        result = np.hstack((result, tonnetz))\n","    return result"]},{"cell_type":"code","execution_count":50,"metadata":{"id":"ikiAX5Mm29yi"},"outputs":[],"source":["from openpyxl import load_workbook\n","\n","\n","def audio_splitter(input_folder, output_base_folder, output_base_excel_folder):\n","    print(\"Starting audio splitting...\")\n","    # Construct the model\n","    model = create_model()\n","    model.load_weights(f\"{os.getenv('TONANALYSE_AUDIO_GENDER_NOTEBOOKS_DIR')}/results/model.h5\")\n","    # Iterate through each folder from ADs_IG_2013_wav to ADs_IG_2022_wav\n","    years = os.getenv(\"YEARS\").split(\" \")\n","\n","    for folder_name in years:\n","     \n","    # for folder_name in range(2013, 2023):\n","        input_path = f\"{input_folder}/ADs_IG_{folder_name}\"\n","        output_folder = f\"{output_base_folder}/ADs_IG_{folder_name}\"\n","        excel_output_folder = f\"{output_base_excel_folder}/ADs_IG_{folder_name}\"\n","\n","        os.makedirs(output_folder, exist_ok=True)\n","        files = [f for f in os.listdir(input_path) if f.endswith('.wav')]\n","        print(\"files\", files)\n","        files.sort()\n","        print(\"files\", files)\n","\n","        # Iterate through each file in the current folder\n","        for file_name in files:\n","                audio_path = f\"{input_path}/{file_name}\"  # Path to the audio file\n","                print(f\"P {audio_path}\")\n","                audio = AudioSegment.from_wav(audio_path)\n","\n","                result_per_audio = []\n","\n","                # Initialize an empty list to store the segmented audio\n","                voice_timestamp = []\n","                segment_index = 0\n","\n","                # Load audio for diarization\n","                own_file = {'audio': audio_path}  # Provide the audio file path\n","                pipeline = Pipeline.from_pretrained('pyannote/speaker-diarization-3.1', use_auth_token=True)\n","                if torch.cuda.is_available():\n","                    pipeline.to(torch.device('cuda'))\n","\n","\n","                from pyannote.audio.pipelines.utils.hook import ProgressHook\n","                with ProgressHook() as hook:\n","                    diarization = pipeline(own_file, hook=hook)\n","\n","\n","                # Segmentation logic\n","                for turn, _, speaker in diarization.itertracks(yield_label=True):\n","                    segment_id = f\"segment_{segment_index}\"\n","                    voice_timestamp.append((segment_id, turn.start, turn.end, speaker))\n","                    segment_index += 1\n","\n","\n","                # Loop through the voice_timestamp list and split the audio\n","                for idx, (_, start, stop, speaker) in enumerate(voice_timestamp):\n","                    segment = audio[start * 1000: stop * 1000]  # Extract the segment in milliseconds\n","                    output_subfolder = f\"{output_folder}/{os.path.splitext(file_name)[0]}\"  # Create subfolder based on file name\n","                    os.makedirs(output_subfolder, exist_ok=True)  # Create subfolder if it doesn't exist\n","\n","                    if((stop - start) < 0.05):\n","                      print(f\"segment_{idx}.wav is too short!\")\n","                      continue\n","                    else:\n","                      segment.export(f\"{output_subfolder}/segment_{idx}.wav\", format=\"wav\")\n","\n","                    # Extract features and reshape it (assuming you have an extract_feature function)\n","                    features = extract_feature(f\"{output_subfolder}/segment_{idx}.wav\", format=\"wav\", mel=True).reshape(1, -1)\n","\n","                    # Predict the gender\n","                    male_prob = model.predict(features)[0][0]\n","                    female_prob = 1 - male_prob\n","                    gender = \"male\" if male_prob > female_prob else \"female\"\n","\n","\n","                    # Store segment information in the list\n","                    duration = stop - start\n","                    segID = f\"segment_{idx}.wav\"\n","                    result_per_audio.append((file_name, segID, start,stop, duration, speaker, gender, male_prob, female_prob))\n","\n","\n","                for result_tuple in result_per_audio:\n","                  file_name, segID, turn_start, turn_end, duration, speaker, gender, male_prob, female_prob = result_tuple\n","                  print(f\"file_name: {file_name}, segment_id: {segID}, turn.start: {turn_start}, turn.end: {turn_end}, duration: {duration}, speaker: {speaker}, gender: {gender}, male_prob: {male_prob}, female_prob: {female_prob}\")\n","\n","\n","\n","                # Create DataFrame from result_per_audio\n","                df = pd.DataFrame(result_per_audio, columns=['file_name', 'segment_id', 'start', 'end', 'duration', 'speaker', 'gender', 'male_prob', 'female_prob'])\n","\n","                # Create directory for saving Excel file\n","                excel_output_folder = f\"{output_base_excel_folder}/ADs_IG_{folder_name}\" #.wav\n","                os.makedirs(excel_output_folder, exist_ok=True)  # Create output directory if it doesn't exist\n","\n","                # Save DataFrame to Excel\n","                excel_file_path = f\"{excel_output_folder}/{os.path.splitext(file_name)[0]}.xlsx\"  # Excel file path\n","                print(f\"Excel file path: {excel_file_path}\")\n","                # Check if Excel file already exists\n","\n","\n","                try:\n","                    # Try to write to existing file\n","                    with pd.ExcelWriter(excel_file_path, engine='openpyxl', mode='a', if_sheet_exists='replace') as writer:\n","                        df.to_excel(writer, sheet_name='Gender_speaking_time', index=False)\n","                        print(f\"Writing to {excel_file_path} completed!\") \n","                except:\n","                    # If file is corrupted or doesn't exist, create new file\n","                    df.to_excel(excel_file_path, sheet_name='Gender_speaking_time', index=False, engine='openpyxl')\n","                    print(f\"Writing to {excel_file_path} completed!\")\n","\n"]},{"cell_type":"markdown","metadata":{"id":"7V6Uy8EU-umc"},"source":["### Main\n","\n","---\n","\n","\n"]},{"cell_type":"code","execution_count":51,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["c9ff155a579140e2bb19bfbadc4e032f","4cbb11e0ee474f2eaf7ffa7e3583e9f8"]},"executionInfo":{"elapsed":14257,"status":"ok","timestamp":1708647221477,"user":{"displayName":"Thorben","userId":"17213772383075988098"},"user_tz":-60},"id":"Mow9w3JKCPXh","outputId":"e68d7dc1-90c8-4ad0-dbe6-b161d17297fb"},"outputs":[{"name":"stdout","output_type":"stream","text":["input_folder /home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/ADs\n","Starting audio splitting...\n","creating model ...\n"]},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_6\"</span>\n","</pre>\n"],"text/plain":["\u001b[1mModel: \"sequential_6\"\u001b[0m\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n","│ dense_36 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">33,024</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dropout_30 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dense_37 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">65,792</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dropout_31 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dense_38 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dropout_32 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dense_39 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">16,512</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dropout_33 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dense_40 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dropout_34 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dense_41 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │\n","└─────────────────────────────────┴────────────────────────┴───────────────┘\n","</pre>\n"],"text/plain":["┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n","│ dense_36 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │        \u001b[38;5;34m33,024\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dropout_30 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dense_37 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │        \u001b[38;5;34m65,792\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dropout_31 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dense_38 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m32,896\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dropout_32 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dense_39 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m16,512\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dropout_33 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dense_40 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m8,256\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dropout_34 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dense_41 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m65\u001b[0m │\n","└─────────────────────────────────┴────────────────────────┴───────────────┘\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">156,545</span> (611.50 KB)\n","</pre>\n"],"text/plain":["\u001b[1m Total params: \u001b[0m\u001b[38;5;34m156,545\u001b[0m (611.50 KB)\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">156,545</span> (611.50 KB)\n","</pre>\n"],"text/plain":["\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m156,545\u001b[0m (611.50 KB)\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n","</pre>\n"],"text/plain":["\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["files ['AD0253.wav']\n","files ['AD0253.wav']\n","P /home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/ADs/ADs_IG_2013/AD0253.wav\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"14d5807fb5884fbfaf75a176b03c3cad","version_major":2,"version_minor":0},"text/plain":["Output()"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/Final_Files/02. \n","Tonanalyse/main_sound_recognition_FINAL-venv/lib/python3.12/site-packages/pyannote/audio/models/blocks/pooling.py:1\n","04: UserWarning: std(): degrees of freedom is &lt;= 0. Correction should be strictly less than the reduction factor \n","(input numel divided by output numel). (Triggered internally at ../aten/src/ATen/native/ReduceOps.cpp:1823.)\n","  std = sequences.std(dim=-1, correction=1)\n","</pre>\n"],"text/plain":["/home/arkastor/Development/Commercial-Brand-Differentiating-Message-Analysis/Final_Files/02. \n","Tonanalyse/main_sound_recognition_FINAL-venv/lib/python3.12/site-packages/pyannote/audio/models/blocks/pooling.py:1\n","04: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor \n","(input numel divided by output numel). (Triggered internally at ../aten/src/ATen/native/ReduceOps.cpp:1823.)\n","  std = sequences.std(dim=-1, correction=1)\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"],"text/plain":[]},"metadata":{},"output_type":"display_data"},{"ename":"RuntimeError","evalue":"CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_56226/3958681844.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0moutput_base_folder\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetenv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"TONANALYSE_SPLITTED_AUDIOS_DIR\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0moutput_base_excel_folder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetenv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"OUTPUT_BILD_PLUS_TON_LISTS_DIR\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0msplitted_audios\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0maudio_splitter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_base_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_base_excel_folder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipykernel_56226/3398347650.py\u001b[0m in \u001b[0;36maudio_splitter\u001b[0;34m(input_folder, output_base_folder, output_base_excel_folder)\u001b[0m\n\u001b[1;32m     44\u001b[0m                 \u001b[0;32mfrom\u001b[0m \u001b[0mpyannote\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maudio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipelines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhook\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mProgressHook\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mProgressHook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m                     \u001b[0mdiarization\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mown_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/Development/Commercial-Brand-Differentiating-Message-Analysis/Final_Files/02. Tonanalyse/main_sound_recognition_FINAL-venv/lib/python3.12/site-packages/pyannote/audio/core/pipeline.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, file, **kwargs)\u001b[0m\n\u001b[1;32m    323\u001b[0m             \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mProtocolFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlazy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/Development/Commercial-Brand-Differentiating-Message-Analysis/Final_Files/02. Tonanalyse/main_sound_recognition_FINAL-venv/lib/python3.12/site-packages/pyannote/audio/pipelines/speaker_diarization.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, file, num_speakers, min_speakers, max_speakers, return_embeddings, hook)\u001b[0m\n\u001b[1;32m    512\u001b[0m             \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 514\u001b[0;31m             embeddings = self.get_embeddings(\n\u001b[0m\u001b[1;32m    515\u001b[0m                 \u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m                 \u001b[0mbinarized_segmentations\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/Development/Commercial-Brand-Differentiating-Message-Analysis/Final_Files/02. Tonanalyse/main_sound_recognition_FINAL-venv/lib/python3.12/site-packages/pyannote/audio/pipelines/speaker_diarization.py\u001b[0m in \u001b[0;36mget_embeddings\u001b[0;34m(self, file, binary_segmentations, exclude_overlap, hook)\u001b[0m\n\u001b[1;32m    347\u001b[0m             \u001b[0;31m# (batch_size, num_frames) torch.Tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 349\u001b[0;31m             embedding_batch: np.ndarray = self._embedding(\n\u001b[0m\u001b[1;32m    350\u001b[0m                 \u001b[0mwaveform_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m             \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/Development/Commercial-Brand-Differentiating-Message-Analysis/Final_Files/02. Tonanalyse/main_sound_recognition_FINAL-venv/lib/python3.12/site-packages/pyannote/audio/pipelines/speaker_verification.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, waveforms, masks)\u001b[0m\n\u001b[1;32m    704\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatch_warnings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    705\u001b[0m                     \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimplefilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 706\u001b[0;31m                     embeddings = self.model_(\n\u001b[0m\u001b[1;32m    707\u001b[0m                         \u001b[0mwaveforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmasks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    708\u001b[0m                     \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/Development/Commercial-Brand-Differentiating-Message-Analysis/Final_Files/02. Tonanalyse/main_sound_recognition_FINAL-venv/lib/python3.12/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/Development/Commercial-Brand-Differentiating-Message-Analysis/Final_Files/02. Tonanalyse/main_sound_recognition_FINAL-venv/lib/python3.12/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_forward_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_forward_pre_hooks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/Development/Commercial-Brand-Differentiating-Message-Analysis/Final_Files/02. Tonanalyse/main_sound_recognition_FINAL-venv/lib/python3.12/site-packages/pyannote/audio/models/embedding/wespeaker/__init__.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, waveforms, weights)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0mfbank\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_fbank\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwaveforms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfbank\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/Development/Commercial-Brand-Differentiating-Message-Analysis/Final_Files/02. Tonanalyse/main_sound_recognition_FINAL-venv/lib/python3.12/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/Development/Commercial-Brand-Differentiating-Message-Analysis/Final_Files/02. Tonanalyse/main_sound_recognition_FINAL-venv/lib/python3.12/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_forward_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_forward_pre_hooks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/Development/Commercial-Brand-Differentiating-Message-Analysis/Final_Files/02. Tonanalyse/main_sound_recognition_FINAL-venv/lib/python3.12/site-packages/pyannote/audio/models/embedding/wespeaker/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, weights)\u001b[0m\n\u001b[1;32m    209\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/Development/Commercial-Brand-Differentiating-Message-Analysis/Final_Files/02. Tonanalyse/main_sound_recognition_FINAL-venv/lib/python3.12/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/Development/Commercial-Brand-Differentiating-Message-Analysis/Final_Files/02. Tonanalyse/main_sound_recognition_FINAL-venv/lib/python3.12/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_forward_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_forward_pre_hooks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/Development/Commercial-Brand-Differentiating-Message-Analysis/Final_Files/02. Tonanalyse/main_sound_recognition_FINAL-venv/lib/python3.12/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/Development/Commercial-Brand-Differentiating-Message-Analysis/Final_Files/02. Tonanalyse/main_sound_recognition_FINAL-venv/lib/python3.12/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/Development/Commercial-Brand-Differentiating-Message-Analysis/Final_Files/02. Tonanalyse/main_sound_recognition_FINAL-venv/lib/python3.12/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_forward_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_forward_pre_hooks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/Development/Commercial-Brand-Differentiating-Message-Analysis/Final_Files/02. Tonanalyse/main_sound_recognition_FINAL-venv/lib/python3.12/site-packages/pyannote/audio/models/embedding/wespeaker/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshortcut\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/Development/Commercial-Brand-Differentiating-Message-Analysis/Final_Files/02. Tonanalyse/main_sound_recognition_FINAL-venv/lib/python3.12/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/Development/Commercial-Brand-Differentiating-Message-Analysis/Final_Files/02. Tonanalyse/main_sound_recognition_FINAL-venv/lib/python3.12/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_forward_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_forward_pre_hooks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/Development/Commercial-Brand-Differentiating-Message-Analysis/Final_Files/02. Tonanalyse/main_sound_recognition_FINAL-venv/lib/python3.12/site-packages/torch/nn/modules/batchnorm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    191\u001b[0m         \u001b[0mused\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mnormalization\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0;32min\u001b[0m \u001b[0meval\u001b[0m \u001b[0mmode\u001b[0m \u001b[0mwhen\u001b[0m \u001b[0mbuffers\u001b[0m \u001b[0mare\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m         \"\"\"\n\u001b[0;32m--> 193\u001b[0;31m         return F.batch_norm(\n\u001b[0m\u001b[1;32m    194\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m             \u001b[0;31m# If buffers are not to be tracked, ensure that they won't be updated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/Development/Commercial-Brand-Differentiating-Message-Analysis/Final_Files/02. Tonanalyse/main_sound_recognition_FINAL-venv/lib/python3.12/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2810\u001b[0m         \u001b[0m_verify_batch_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2811\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2812\u001b[0;31m     return torch.batch_norm(\n\u001b[0m\u001b[1;32m   2813\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2814\u001b[0m         \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}],"source":["\n","input_folder = os.getenv(\"ADS_DIR\")\n","print(\"input_folder\", input_folder)\n","output_base_folder =os.getenv(\"TONANALYSE_SPLITTED_AUDIOS_DIR\")\n","output_base_excel_folder = os.getenv(\"OUTPUT_BILD_PLUS_TON_LISTS_DIR\")\n","splitted_audios= audio_splitter(input_folder, output_base_folder, output_base_excel_folder)"]},{"cell_type":"markdown","metadata":{"id":"8ed4CsrdFsXR"},"source":["### Total speaking time (Conclusion)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3572,"status":"ok","timestamp":1708647270619,"user":{"displayName":"Thorben","userId":"17213772383075988098"},"user_tz":-60},"id":"EO353s5XJLkn","outputId":"5130ae55-3dcc-4558-923a-537751a935a2"},"outputs":[],"source":["import os\n","import pandas as pd\n","from pydub import AudioSegment\n","\n","# Define input and output base folders\n","input_folder = '/content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Input_audio'\n","output_base_excel_folder = '/content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/output_lists/output_lists'\n","\n","# Iterate through each folder from ADs_IG_2013_wav to ADs_IG_2022_wav\n","\n","for folder_name in range(2013, 2023):\n","    input_path = f\"{input_folder}/ADs_IG_{folder_name}_wav\"\n","    output_folder = f\"{output_base_excel_folder}/ADs_IG_{folder_name}\" #_wav\n","\n","    # Iterate through each WAV file in the current folder\n","    for wav_file_name in os.listdir(input_path):\n","        if wav_file_name.endswith(\".wav\"):\n","            # Load the WAV file using pydub\n","            wav_file_path = os.path.join(input_path, wav_file_name)\n","            audio = AudioSegment.from_wav(wav_file_path)\n","\n","            # Get the total duration in seconds\n","            total_duration = len(audio) / 1000.0  # Convert milliseconds to seconds\n","\n","            print(\"Processing:\", wav_file_path)\n","            print(\"Total Duration:\", total_duration, \"seconds\")\n","\n","            # Read Excel file into a pandas DataFrame\n","            excel_file_path = os.path.join(output_folder, f\"{os.path.splitext(wav_file_name)[0]}.xlsx\")\n","            # df = pd.read_excel(excel_file_path)\n","\n","            # Check if 'Gender_speaking_time' sheet exists in the Excel file\n","            with pd.ExcelWriter(excel_file_path, engine='openpyxl', mode='a', if_sheet_exists='replace') as writer:\n","                if 'Gender_speaking_time' in writer.sheets:\n","                    # Append to existing 'Gender_speaking_time' sheet\n","                    df = pd.read_excel(excel_file_path, sheet_name='Gender_speaking_time')  # Read data from the correct sheet\n","                else:\n","                    # Create a new 'Gender_speaking_time' sheet\n","                    df = pd.DataFrame()  # Create an empty DataFrame if the sheet doesn't exist\n","\n","            # Calculate total speaking time in seconds\n","            total_speaking_time = df['duration'].sum()\n","\n","            # Calculate the percentage of total speaking time against the total duration\n","            percentage_of_total_duration = total_speaking_time / total_duration * 100\n","\n","            # Calculate percentages\n","            df['Value_Percent'] = df['duration'] / total_speaking_time * 100\n","\n","            # Create a DataFrame with the results\n","            results_df = pd.DataFrame({\n","                'Metric': ['Total Speaking Time', 'Male Speaking Time', 'Female Speaking Time'],\n","                'Value_Seconds': [total_speaking_time, df[df['gender'] == 'male']['duration'].sum(), df[df['gender'] == 'female']['duration'].sum()],\n","                'Value_Percent': [percentage_of_total_duration, df[df['gender'] == 'male']['Value_Percent'].sum(), df[df['gender'] == 'female']['Value_Percent'].sum()]\n","            })\n","\n","            sheet_name = 'Gender_speaking_time'\n","\n","            existing_data = pd.read_excel(excel_file_path, sheet_name)\n","            existing_data[''] = ''\n","\n","            # Step 3: Concatenate the existing data, the empty column, and the new DataFrame\n","            combined_data = pd.concat([existing_data, results_df], axis=1)\n","\n","            # Step 3: Load the Excel file\n","            book = load_workbook(excel_file_path)\n","\n","            # Step 4: Get the writer object for the Excel file\n","            writer = pd.ExcelWriter(excel_file_path, engine='openpyxl')\n","            writer.book = book\n","\n","            # Step 5: Write the combined data to the fourth sheet\n","            combined_data.to_excel(writer, sheet_name, index=False, header=True)\n","\n","            # Step 6: Save the changes\n","            writer.save()\n","            writer.close()"]},{"cell_type":"markdown","metadata":{"id":"k49Dx-u6qMwE"},"source":["\n","\n","---\n","\n"]},{"cell_type":"markdown","metadata":{"id":"tDMwn1BCiYWW"},"source":["# *2. Emotion Recognition from Transcription*"]},{"cell_type":"markdown","metadata":{"id":"PP0b_vg7rrze"},"source":["WhisperAI transcribes all audio files before distilbert analyzes the emotions\n","\n","\n","WhisperAI: https://github.com/openai/whisper\n","\n","distilbert-base-uncased-emotion: https://huggingface.co/bhadresh-savani/distilbert-base-uncased-emotion?text=I+feel+a+bit+let+down"]},{"cell_type":"markdown","metadata":{"id":"AvqbjDgQn94R"},"source":["## Whisper AI for Transcription\n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":55409,"status":"ok","timestamp":1708602848456,"user":{"displayName":"Thorben Schlieffen","userId":"06537630395058694731"},"user_tz":-60},"id":"21_SRoXnoXlR","outputId":"9f5ed74f-d6bf-4427-dfb1-0f44cfc4b0b9"},"outputs":[],"source":["# if not installed\n","!pip install -U openai-whisper\n","\n","!pip install git+https://github.com/openai/whisper.git\n","\n","!pip install --upgrade --no-deps --force-reinstall git+https://github.com/openai/whisper.git"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6625,"status":"ok","timestamp":1708602915572,"user":{"displayName":"Thorben Schlieffen","userId":"06537630395058694731"},"user_tz":-60},"id":"sGJ8gRNzobgc","outputId":"688e1b38-02bd-47d9-f98c-42bdba5f70af"},"outputs":[],"source":["# if not installed\n","!pip install setuptools-rust"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20751,"status":"ok","timestamp":1708602079178,"user":{"displayName":"Thorben Schlieffen","userId":"06537630395058694731"},"user_tz":-60},"id":"GTmPWuaioekS","outputId":"c4d49d44-b55e-4aeb-ca32-d67dcdeebef9"},"outputs":[],"source":["# Optional\n","# from google.colab import drive\n","# drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"d8KH9_OQrbFE"},"source":["### Main\n","\n","---"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":605541,"status":"ok","timestamp":1708603530986,"user":{"displayName":"Thorben Schlieffen","userId":"06537630395058694731"},"user_tz":-60},"id":"e4zYYidAo0aC","outputId":"db925520-3f2d-4042-aede-f3c9074e1301"},"outputs":[],"source":["import os\n","import whisper\n","import torch\n","\n","# Input\n","input_folder_path = '/content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Input_audio'\n","\n","# Set model to CPU\n","device = torch.device('cpu')\n","model = whisper.load_model(\"large\", device=device)\n","\n","# Iterate through each folder\n","for folder_name in os.listdir(input_folder_path):\n","    folder_path = os.path.join(input_folder_path, folder_name)\n","\n","    # Iterate to find .wav files\n","    if os.path.isdir(folder_path):\n","        for file_name in os.listdir(folder_path):\n","            if file_name.endswith(\".wav\"):\n","                audio_file_path = os.path.join(folder_path, file_name)\n","\n","                # Transcription of audio file\n","                result = model.transcribe(audio_file_path)\n","\n","                # Outputs here are only TXT files. WhisperAI can also create SRT, VTT, TSV and JSON files. You can change it here.\n","\n","                # Create output\n","                output_txt_path = os.path.join(folder_path, f\"{file_name.split('.')[0]}.txt\")\n","\n","                # Save Output in .txt file\n","                with open(output_txt_path, 'w') as txt_file:\n","                    txt_file.write(result['text'])\n","\n","                print(f\"Transcription for {file_name}: {result['text']}\")\n","                print(f\"Output will be saved in {output_txt_path} .\")\n"]},{"cell_type":"markdown","metadata":{"id":"8d20XiSLo_FY"},"source":["\n","## Emotion Recognition (1)"]},{"cell_type":"markdown","metadata":{"id":"zZH1G38Y6AvO"},"source":["### Set up"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Lai6BjVVo_ue"},"outputs":[],"source":["from transformers import pipeline\n","import pandas as pd\n","import os"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rkjeJW2XiXhb"},"outputs":[],"source":["# If not installed:\n","#!pip install transformers\n","#!pip install xlsxwriter"]},{"cell_type":"markdown","metadata":{"id":"EVfTBXNV7jCH"},"source":["### Main"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o-jZ1-Fjkgpd"},"outputs":[],"source":["# Define emotionclassifier\n","\n","from transformers import pipeline\n","\n","# Create a text classification pipeline object\n","classifier = pipeline(\"text-classification\", model='bhadresh-savani/distilbert-base-uncased-emotion', top_k=None)\n","\n","def classify_emotion_from_file(file_path):\n","    with open(file_path, 'r', encoding='utf-8') as file:\n","        content = file.read()\n","\n","    # Split the text into words\n","    words = content.split()\n","\n","    def classify_and_check(predictions, text):\n","        if any(emotion['score'] > 0.8 for emotion in predictions[0]):\n","            return {'Emotion': predictions[0][0]['label'], 'Probability': predictions[0][0]['score']}\n","        else:\n","            return {'Emotion': 'neutral', 'Probability': 1.0}\n","\n","    # Due to the model's limitation in analyzing coherent texts with more than 657 tokens (approximately 320 words),\n","    # such lengthy texts will be split in half and analyzed independently. In our collection of Super Bowl ads from 2013 to 2022,\n","    # only one ad (AD0290) exceeds this limit. Our AI model classified the first part as 'fear' and the second part as 'joy.'\n","    # After a manual analysis of both parts, we would categorize the entire ad as 'joy.'\n","\n","    if len(words) > 320:\n","        # Split the first row into two parts, each containing half of the words\n","        half_length = len(words) // 2\n","        first_row_part1 = ' '.join(words[:half_length])\n","        first_row_part2 = ' '.join(words[half_length:])\n","\n","        # Classify emotions for the first part of the first row\n","        first_row_part1_predictions = classifier(first_row_part1)\n","\n","        # Append information for the first part of the first row\n","        emotions_and_scores_part1 = {\n","            'AD-Number': file_path.split('/')[-1].split('.')[0],\n","            'Transcription': first_row_part1,\n","            'Word range': f'1-{half_length}',\n","            **classify_and_check(first_row_part1_predictions, first_row_part1)\n","        }\n","\n","        # Classify emotions for the second part of the first row\n","        first_row_part2_predictions = classifier(first_row_part2)\n","\n","        # Append information for the second part of the first row\n","        emotions_and_scores_part2 = {\n","            'AD-Number': file_path.split('/')[-1].split('.')[0],\n","            'Transcription': first_row_part2,\n","            'Word range': f'{half_length + 1}-{len(words)}',\n","            **classify_and_check(first_row_part2_predictions, first_row_part2)\n","        }\n","\n","        # Combine information for the first row\n","        emotions_and_scores = [emotions_and_scores_part1, emotions_and_scores_part2]\n","    else:\n","\n","        # Classify emotions for the entire content\n","        full_content_predictions = classifier(content)\n","\n","        # Append information for the first row (full content)\n","        emotions_and_scores = [{\n","            'AD-Number': file_path.split('/')[-1].split('.')[0],\n","            'Transcription': content,\n","            'Word range': f'1-{len(words)}',\n","            **classify_and_check(full_content_predictions, content)\n","        }]\n","\n","    # Classify emotions for each 20-word segment starting from the 2nd row with a 5-word shift\n","    for start in range(0, len(words)-20, 5):\n","        end = min(start + 20, len(words))\n","        word_range = f'{start+1}-{end}'  # Adjust to avoid index out of range\n","        text_segment = ' '.join(words[start:end])\n","\n","        # Classify emotions for the current segment\n","        segment_predictions = classifier(text_segment)\n","\n","        # Append information for each segment\n","        emotions_and_scores.append({\n","            'AD-Number': file_path.split('/')[-1].split('.')[0],\n","            'Transcription': text_segment,\n","            'Word range': word_range,\n","            **classify_and_check(segment_predictions, text_segment)\n","        })\n","\n","\n","    # Check for missing words\n","    remaining_start = max(len(words) - 20, 0)  # Startpunkt für die letzten 20 Wörter\n","    if remaining_start < len(words):\n","        remaining_word_range = f'{remaining_start + 1}-{len(words)}'\n","        remaining_text_segment = ' '.join(words[remaining_start:])\n","        remaining_predictions = classifier(remaining_text_segment)\n","\n","        # Verwendung der classify_and_check-Funktion für die Emotionsklassifikation\n","        remaining_emotion_info = classify_and_check(remaining_predictions, remaining_text_segment)\n","\n","        # Append information for remaining words directly\n","        emotions_and_scores.append({\n","          'AD-Number': file_path.split('/')[-1].split('.')[0],\n","          'Transcription': remaining_text_segment,\n","          'Word range': remaining_word_range,\n","          'Emotion': remaining_emotion_info['Emotion'],\n","          'Probability': remaining_emotion_info['Probability']\n","        })\n","\n","\n","    return emotions_and_scores, emotions_and_scores[0]['AD-Number']\n","\n","\n","def extract_emotions_and_scores(text, predictions, ad_number):\n","    # Extract emotions + probabilities and add \"Word range\" and \"Text segment\" information\n","    emotions_and_scores = []\n","\n","    # Split text into words\n","    words = text.split()\n","    segment_size = 20\n","\n","    for emotion in predictions[0]:\n","        if emotion['score'] > 0.8:\n","            for start in range(0, len(words), segment_size):  # Adjust to 20 words per segment\n","                end = min(start + segment_size, len(words))\n","                word_range = f'{start + 1}-{end}'  # Adjust to avoid index out of range\n","                text_segment = ' '.join(words[start:end])\n","\n","                # Classify emotions for the current segment using the global classifier\n","                segment_predictions = classifier(text_segment)\n","\n","                # Only add relevant information for the first row\n","                if start == 0:\n","                    emotions_and_scores.append({\n","                        'AD-Number': ad_number.split('/')[-1].split('.')[0],\n","                        'Transcription': text,\n","                        'Word range': f'1-{len(words)}',\n","                        'Emotion': emotion['label'],\n","                        'Probability': emotion['score']\n","                    })\n","\n","                # Add information for subsequent rows\n","                emotions_and_scores.append({\n","                    'AD-Number': ad_number.split('/')[-1].split('.')[0],\n","                    'Transcription': text_segment,\n","                    'Word range': word_range,\n","                    'Emotion': segment_predictions[0][0]['label'],  # Assuming top emotion from the model\n","                    'Probability': segment_predictions[0][0]['score']\n","\n","                     })\n","\n","\n","\n","    return emotions_and_scores\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mm1uQuTXkkiy"},"outputs":[],"source":["def process_text_file(file_path, output_base_folder):\n","\n","    predictions, file_name = classify_emotion_from_file(file_path)\n","\n","    # Create directory for output\n","    output_folder = os.path.join(output_base_folder, file_name)\n","    output_folder_name = os.path.basename(output_folder)\n","    output_folder_name = output_folder_name.replace(\".wav\", \"\")\n","    output_folder = os.path.join(os.path.dirname(output_folder), output_folder_name)\n","\n","    # Go through all subfolders\n","    for root, dirs, files in os.walk(output_base_folder):\n","        for dir_name in dirs:\n","            if dir_name not in file_path:\n","                continue\n","\n","            # Create Excel directory\n","            excel_file_path = os.path.join(root, dir_name, f\"{file_name}.xlsx\")\n","\n","            try:\n","                # Try to open existing excel file\n","                with pd.ExcelFile(excel_file_path) as xls:\n","\n","                    # If file exists, add new sheet with predicitions\n","                    result_df_existing = pd.read_excel(xls)\n","                    result_df_new = pd.DataFrame(predictions, columns=['AD-Number', 'Transcription', 'Word range', 'Emotion', 'Probability'])\n","                    result_df_existing = pd.concat([result_df_new], ignore_index=True)\n","\n","                    # save updated data in excel file\n","                    with pd.ExcelWriter(excel_file_path, engine='openpyxl', mode='a') as writer:\n","                        result_df_existing.to_excel(writer, sheet_name=\"Transcription_and_Mood\", index=False)\n","                        print(f\"File {excel_file_path} is updated.\")\n","                        return\n","            except FileNotFoundError:\n","\n","                   # If there is no matching excel file, create a new one\n","\n","                    with pd.ExcelWriter(excel_file_path, engine='openpyxl', mode='w') as writer:\n","                        # Create excel file\n","                        result_df_new = pd.DataFrame(predictions, columns=['AD-Number', 'Transcription', 'Word range', 'Emotion', 'Probability'])\n","                        result_df_new.to_excel(writer, sheet_name=\"Transcription_and_Mood\", index=False)\n","                        print(f\"File {excel_file_path} is created.\")\n","                        break\n"]},{"cell_type":"markdown","metadata":{"id":"4FuVNTED7pKZ"},"source":["### Input and Output"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4312,"status":"ok","timestamp":1708647348242,"user":{"displayName":"Thorben","userId":"17213772383075988098"},"user_tz":-60},"id":"Qk6tnP-Akn_j","outputId":"4ef1cdaf-3096-4e6c-aada-fdff31b13a99"},"outputs":[],"source":["# Input\n","input_folder_path = '/content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Input_audio'\n","output_base_folder = '/content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/output_lists/output_lists'\n","\n","# Loop through all files in the folder\n","for root, dirs, files in os.walk(input_folder_path):\n","    for file_name in files:\n","        if file_name.endswith(\".txt\"):\n","            file_path = os.path.join(root, file_name)\n","            process_text_file(file_path, output_base_folder)\n"]},{"cell_type":"markdown","metadata":{"id":"wZJr6yGgqF6f"},"source":["\n","\n","---\n","\n"]},{"cell_type":"markdown","metadata":{"id":"j-tItGZ9W5IH"},"source":["## Combination of emotion from image and audio (2)\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"0Jefnv7pX004"},"source":["## Emotion from WhisperAI Intervalls"]},{"cell_type":"markdown","metadata":{"id":"9SdbABSwwcSz"},"source":["### Set Up"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u6wKKv3zIpvM"},"outputs":[],"source":["import pandas as pd\n","import json\n","import os\n","from transformers import pipeline\n","from openpyxl import load_workbook\n","from openpyxl.utils.dataframe import dataframe_to_rows"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1629,"status":"ok","timestamp":1709415738181,"user":{"displayName":"Flavio Kuka","userId":"09654499042520950340"},"user_tz":-60},"id":"hwbaqOc41Ydh","outputId":"317679be-e946-435b-cfc1-c267981072b3"},"outputs":[],"source":["# Optional\n","# from google.colab import drive\n","# drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"X8KcwHSh0k1r"},"source":["### Functions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dUku8083IvH0"},"outputs":[],"source":["# Funktion zum Emotionsklassifikation und Überprüfung\n","def classify_and_check(predictions):\n","    if any(emotion['score'] > 0.8 for emotion in predictions[0]):\n","        return {'Emotion': predictions[0][0]['label'], 'Probability': predictions[0][0]['score']}\n","    else:\n","        return {'Emotion': 'neutral', 'Probability': 1.0}\n","\n","# Funktion zum Emotionsklassifikation\n","def classify_emotion(text):\n","    classifier = pipeline(\"text-classification\", model='bhadresh-savani/distilbert-base-uncased-emotion', top_k=None)\n","    result = classifier(text)\n","    return result"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4xdz4hR6I7Mz"},"outputs":[],"source":["def process_json_file(json_file_path, excel_file_path):\n","    # Create empty list\n","    all_excel_data = []\n","\n","    # Existing table\n","    existing_df = pd.read_excel(excel_file_path, sheet_name='Transcription_and_Mood')\n","\n","    # load all JSON data\n","    with open(json_file_path) as f:\n","        data = json.load(f)\n","\n","    # extract data name\n","    ad_number = os.path.splitext(os.path.basename(json_file_path))[0]\n","\n","    # extract whole text\n","    full_text = data.get(\"text\", \"\")\n","\n","    # extract relevant information\n","    data_list = data.get(\"segments\", [])\n","\n","    # loop through all segments\n","    for segment in data_list:\n","        row = {\n","            'AD-Number': ad_number,\n","            'ID-Number': segment.get(\"id\", \"\"),\n","            'Start': segment.get(\"start\", \"\"),\n","            'End': segment.get(\"end\", \"\"),\n","            'Transcription': segment.get(\"text\", \"\")\n","        }\n","\n","        # classify emotion\n","        emotion_result = classify_emotion(row['Transcription'])\n","        row.update(classify_and_check(emotion_result))\n","\n","        all_excel_data.append(row)\n","\n","    # add first row\n","    first_row = {\n","        'AD-Number': ad_number,\n","        'ID-Number': '',\n","        'Start': '',\n","        'End': '',\n","        'Transcription': full_text\n","    }\n","\n","    if len(full_text) > 320:\n","      # Copy the emotion from the first table\n","      first_row.update({'Emotion': existing_df.at[0, 'Emotion'], 'Probability': 1.0})\n","\n","    else:\n","      # classify emotion + add results\n","      emotion_result_first_row = classify_emotion(first_row['Transcription'])\n","      first_row.update(classify_and_check(emotion_result_first_row))\n","\n","    all_excel_data.insert(0, first_row)\n","\n","    # Create a new DataFrame with your new data\n","    new_data_df = pd.DataFrame(all_excel_data)\n","\n","    # Insert the new DataFrame\n","    existing_df[' '] = ''\n","    existing_df = pd.concat([existing_df, new_data_df], axis=1)\n","\n","    # Load the existing workbook using openpyxl\n","    workbook = load_workbook(excel_file_path)\n","\n","    # get the 'Transcription_and_Mood' sheet\n","    sheet = workbook['Transcription_and_Mood']\n","\n","    # Write the updated DataFrame to the Excel sheet\n","    for r_idx, row in enumerate(dataframe_to_rows(existing_df, index=False, header=True), 1):\n","        for c_idx, value in enumerate(row, 1):\n","            sheet.cell(row=r_idx, column=c_idx, value=value)\n","\n","    # Save the updated workbook\n","    workbook.save(excel_file_path)"]},{"cell_type":"markdown","metadata":{"id":"QWeqtLQ20paw"},"source":["### Run Code"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ka2AVThyb5xc"},"outputs":[],"source":["# Path to the folder containing the frames and the excel lists\n","json_file_folder_path = '/content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/JSON_Dateien'\n","excel_file_folder_path = '/content/drive/MyDrive/SuperBowl_Project_FUB/output_lists'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6eANlqg81VzB"},"outputs":[],"source":["# OPTIONAL: So that the loops starts with the years in an alphabetical order\n","years = []\n","for year in os.listdir(excel_file_folder_path):\n","  years.append(year)\n","years.sort()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8_wd05jv1xMS"},"outputs":[],"source":["for year in years:\n","    # Set paths\n","    json_files_year_path = os.path.join(json_file_folder_path, f'{year}_json')\n","    excel_file_year_path = os.path.join(excel_file_folder_path, year)\n","\n","    # Create sets of base file names\n","    json_files_year_set = {os.path.splitext(file)[0] for file in os.listdir(json_files_year_path)}\n","    excel_files_year_set = {os.path.splitext(file)[0] for file in os.listdir(excel_file_year_path)}\n","\n","    # Find common base names\n","    common_base_names = json_files_year_set.intersection(excel_files_year_set)\n","\n","    # Iterate over common base names\n","    for base_name in common_base_names:\n","        json_file_path = os.path.join(json_files_year_path, f'{base_name}.json')\n","        excel_file_path = os.path.join(excel_file_year_path, f'{base_name}.xlsx')\n","        try:\n","          process_json_file(json_file_path, excel_file_path)\n","        except:\n","          print(f\"fail: {base_name}\")"]},{"cell_type":"markdown","metadata":{"id":"V0kMYUJ5YDoG"},"source":["## Analysis Emotion Image & Audio"]},{"cell_type":"markdown","metadata":{"id":"q52BXAJzYn8R"},"source":["### Set Up"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5128,"status":"ok","timestamp":1709458592187,"user":{"displayName":"Flavio Kuka","userId":"09654499042520950340"},"user_tz":-60},"id":"8YU1Z5y3q-yh","outputId":"d4190464-069a-4fcd-e522-671536d77e26"},"outputs":[],"source":["!pip install XlsxWriter"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uRA3k5WtvvVn"},"outputs":[],"source":["import pandas as pd\n","import re\n","from collections import Counter\n","import os\n","from openpyxl import load_workbook\n","from openpyxl.utils.dataframe import dataframe_to_rows\n","import math\n","import xlsxwriter"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":27444,"status":"ok","timestamp":1709458457633,"user":{"displayName":"Flavio Kuka","userId":"09654499042520950340"},"user_tz":-60},"id":"WjftN_ykvu4H","outputId":"adb49219-d6ef-4c7e-9eec-d529f9baa424"},"outputs":[],"source":["# Optional\n","# from google.colab import drive\n","# drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZFldY_ruwruD"},"outputs":[],"source":["# Create the mapping dictionary for frames to seconds\n","mapping_dict = {}\n","for i in range(0, 4800, 10):\n","    new_number = ((i // 10) // 3) + 1\n","    mapping_dict[i] = new_number"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3Gl45arPgKy3"},"outputs":[],"source":["# Create the mapping for the emotions from text to image\n","emotion_mapping_dict = {\n","    'joy': 'happy',\n","    'love': 'happy',\n","    'neutral': 'neutral',\n","    'anger': 'angry',\n","    'surprise': 'surprise',\n","    'fear': 'fear',\n","    'sadness': 'sad'\n","}"]},{"cell_type":"markdown","metadata":{"id":"4rEo3MezwgEO"},"source":["### Find corresponding Frame_Nr for the second"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YYieyE2CwPOH"},"outputs":[],"source":["def comparison_emotions_image_audio(excel_file_path):\n","  # Read the Sheets\n","  emotion_image = pd.read_excel(f'{excel_file_path}', sheet_name='Predictions')\n","  emotion_audio = pd.read_excel(f'{excel_file_path}', sheet_name='Transcription_and_Mood')\n","\n","  # Create new columns\n","  emotion_audio['Frame_Nr'] = ''\n","  emotion_audio['Emotions'] = ''\n","  emotion_audio['Dominant_Emotion'] = ''\n","  emotion_audio['Correct_%'] = ''\n","  emotion_audio['Equal_Emotions'] = ''\n","\n","  for index, start_second in emotion_audio['Start'].items():\n","    # Check if start_second is not NaN\n","    if pd.notna(start_second):\n","        # Define the seconds in the interval\n","        end_second = emotion_audio.at[index, 'End']\n","        seconds_interval = list(range(int(start_second)+1, int(end_second)+1))\n","\n","        # Create a list of corresponding frames\n","        frames = []\n","        for second in seconds_interval:\n","            for key, value in mapping_dict.items():\n","                if value == round(second):\n","                    frames.append(key)\n","        # Save the corresponding frames to the dataframe\n","        emotion_audio.at[index, 'Frame_Nr'] = frames\n","\n","  # Find corresponding emotion\n","  for index_audio, frame_audio in emotion_audio['Frame_Nr'].items():\n","    corresponding_emotions = []\n","    if str(frame_audio) != '':\n","      # Iterate for each frame_nr\n","      for frame_nr_audio in frame_audio:\n","        for index_image, frame_video in emotion_image['video_frame'].items():\n","          # Find the corresponding frame_nr from the image analysis\n","          frame_nr_video = frame_video[13:]\n","          frame_nr_video = frame_nr_video.split('.')[0]\n","          if str(frame_nr_audio) == str(frame_nr_video):\n","            # Find the corresponding emotion\n","            corresponding_emotion = emotion_image.at[index_image, 'emotion_prediction']\n","            if corresponding_emotion != '-':\n","              corresponding_emotions.append(corresponding_emotion)\n","      # Save the identified emotions\n","      emotion_audio.at[index_audio, 'Emotions'] = corresponding_emotions\n","\n","  # Find dominant emotion\n","  for index_audio, emotions_image in emotion_audio['Emotions'].items():\n","    if len(emotions_image) > 0:\n","      dominant_emotion = Counter(emotions_image).most_common(1)[0][0]\n","      emotion_audio.at[index_audio, 'Dominant_Emotion'] = dominant_emotion\n","\n","  # Find Correct % and if both Emotions are equal\n","  for index_audio, total_emotion_audio in emotion_audio['Emotion.1'].items():\n","    if str(total_emotion_audio) != 'nan':\n","      transformed_emotion = emotion_mapping_dict[str(total_emotion_audio)]\n","      emotions_from_image = emotion_audio.at[index_audio, 'Emotions']\n","      correct_emotion_count = emotions_from_image.count(transformed_emotion)\n","      total_emotions = len(emotions_from_image)\n","      if total_emotions != 0:\n","        emotion_audio.at[index_audio, 'Correct_%'] = (correct_emotion_count/ total_emotions)\n","      else:\n","        emotion_audio.at[index_audio, 'Correct_%'] = 0\n","\n","      dominant_emotion_image = emotion_audio.at[index_audio, 'Dominant_Emotion']\n","      if  transformed_emotion == dominant_emotion_image:\n","        emotion_audio.at[index_audio, 'Equal_Emotions'] = 1\n","      else:\n","        emotion_audio.at[index_audio, 'Equal_Emotions'] = 0\n","\n","  # Insert summary information\n","  emotion_audio['   '] = ''\n","  emotion_audio['Average_Correct_%'] = ''\n","  emotion_audio['Average_Equal_Emotions'] = ''\n","  if len(list(emotion_audio['Emotion.1'].items())) > 0:\n","      emotion_audio['Correct_%'] = pd.to_numeric(emotion_audio['Correct_%'], errors='coerce')\n","      emotion_audio['Equal_Emotions'] = pd.to_numeric(emotion_audio['Equal_Emotions'], errors='coerce')\n","      emotion_audio.at[0, 'Average_Correct_%'] = emotion_audio['Correct_%'].mean()\n","      emotion_audio.at[0, 'Average_Equal_Emotions'] = emotion_audio['Equal_Emotions'].mean()\n","\n","  # Read exel file\n","  excel_sheets = pd.read_excel(excel_file_path, sheet_name=None)\n","\n","  # Replace \"Unnamed\" columns with empty strings in all dataframes\n","  for sheet_name, df in excel_sheets.items():\n","      df.columns = [col if 'Unnamed' not in str(col) else '' for col in df.columns]\n","\n","  # Update the 'Transcription_and_Mood' sheet in the dictionary\n","  excel_sheets['Transcription_and_Mood'] = emotion_audio\n","\n","  # Save the modified dictionary of dataframes back to the Excel file\n","  with pd.ExcelWriter(excel_file_path, engine='xlsxwriter') as writer:\n","      for sheet_name, df in excel_sheets.items():\n","          df.to_excel(writer, sheet_name=sheet_name, index=False)"]},{"cell_type":"markdown","metadata":{"id":"bC-h8259yoPl"},"source":["### Run"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TkGyHorj_lJQ"},"outputs":[],"source":["excel_file_folder_path = '/content/drive/MyDrive/SuperBowl_Project_FUB/output_lists'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-oFx1bf2Yn8k"},"outputs":[],"source":["# OPTIONAL: So that the loops starts with the years in an alphabetical order\n","years = []\n","for year in os.listdir(excel_file_folder_path):\n","  years.append(year)\n","years.sort()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":445207,"status":"ok","timestamp":1709459334450,"user":{"displayName":"Flavio Kuka","userId":"09654499042520950340"},"user_tz":-60},"id":"wIKg409kYn8l","outputId":"8927ad7b-6b8a-476d-8b43-0d16592a9f24"},"outputs":[],"source":["for year in years:\n","    # Set path\n","    excel_file_year_path = os.path.join(excel_file_folder_path, year)\n","\n","    # Create list of files\n","    excel_files = os.listdir(excel_file_year_path)\n","\n","    # Iterate over files\n","    for excel_file in excel_files:\n","      excel_file_path = os.path.join(excel_file_year_path, excel_file)\n","\n","      try:\n","        comparison_emotions_image_audio(excel_file_path)\n","      except:\n","        print(excel_file)"]},{"cell_type":"markdown","metadata":{"id":"dOSIkq0O2vI5"},"source":["# *3. Acoustic Indices (1.0.1)*\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"gB0pxe3s2-Gd"},"source":["This module aims to extract audio features of the given ads (audio-files)\n","\n","Acoustic Indices: https://github.com/patriceguyot/Acoustic_Indices\n","\n","Pydub: https://github.com/jiaaro/pydub"]},{"cell_type":"markdown","metadata":{"id":"FTjB61sx3Hx3"},"source":["### Set up"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Po2USJPg3WOj"},"outputs":[],"source":["#!/usr/bin/env python"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"9gd3_BUK3a5t","outputId":"2f3e149e-944a-4adc-ea4a-d29acdb9aaef"},"outputs":[],"source":["!pip install numpy\n","!pip install scipy\n","!pip install matplotlib\n","!pip install pyyaml\n","\n","!pip install librosa\n","!pip install python_speech_features\n","\n","!pip install pydub"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"CGxhU3OO3tz8"},"outputs":[],"source":["import yaml\n","from scipy import signal\n","from csv import writer\n","import argparse\n","import os\n","\n","import cv2\n","import librosa\n","import librosa.display\n","from python_speech_features import mfcc\n","import wave\n","import audioop\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","# compression rate\n","from pydub import AudioSegment\n","\n","# Excel Export\n","import pandas as pd\n","from openpyxl.styles import Font"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18683,"status":"ok","timestamp":1708809950260,"user":{"displayName":"Thorben Schlieffen","userId":"06537630395058694731"},"user_tz":-60},"id":"A_uNmIVF3eSl","outputId":"19df4cc5-2f87-4a59-be7f-afd15c8d9044"},"outputs":[],"source":["  # Optional\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"JuvMI44hotqJ"},"source":["### Copy & import relevant files"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dqqAeHkK4BC9"},"outputs":[],"source":["!cp /content/drive/MyDrive/SuperBowl_Project_FUB/Colab_Notebooks/Acoustic_Indices/compute_indice.py .\n","!cp /content/drive/MyDrive/SuperBowl_Project_FUB/Colab_Notebooks/Acoustic_Indices/acoustic_index.py .\n","!cp /content/drive/MyDrive/SuperBowl_Project_FUB/Colab_Notebooks/Acoustic_Indices/yaml/config_014_butter.yaml ."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6OyVSFqEo69Q"},"outputs":[],"source":["# import sys\n","# sys.path.append('/content/drive/MyDrive/SuperBowl_Project_FUB/Colab_Notebooks/Acoustic_Indices')\n","\n","from compute_indice import *\n","from acoustic_index import *"]},{"cell_type":"markdown","metadata":{"id":"HHJzI9D7pJH9"},"source":["### Run the Code"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MHdPXCL64V_m"},"outputs":[],"source":["# If True, only one file \"outputs.xlsx\" is created.\n","# If False, a separate output file is created for each ad.\n","single_output_file = False\n","\n","config_file = f\"/content/drive/MyDrive/SuperBowl_Project_FUB/Colab_Notebooks/Acoustic_Indices/yaml/config_014_butter.yaml\"\n","audio_dir = f\"/content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/Input_audio\"\n","# output_csv_file = f\"/content/drive/MyDrive/SuperBowl_Project_FUB/Colab_Notebooks/Acoustic_Indices-1.0.1/outputs.csv\"\n","output_dir = \"/content/drive/MyDrive/SuperBowl_Project_FUB/Tonanalyse/output_lists/output_lists\""]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":193},"executionInfo":{"elapsed":270,"status":"error","timestamp":1708809965539,"user":{"displayName":"Thorben Schlieffen","userId":"06537630395058694731"},"user_tz":-60},"id":"bD_Pds7h4XrV","outputId":"13cff867-5690-4225-936d-23d10f5d67d0"},"outputs":[],"source":["# Set config file\n","yml_file = config_file\n","print(\"Config file: \", yml_file)\n","with open(yml_file, 'r') as stream:\n","    data_config = yaml.load(stream, Loader=yaml.FullLoader)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":299,"status":"ok","timestamp":1708808977152,"user":{"displayName":"Thorben Schlieffen","userId":"06537630395058694731"},"user_tz":-60},"id":"_Ly6BYmJ4aiF","outputId":"a9b7f926-c3d3-47ed-cc80-23bb0c3de9c5"},"outputs":[],"source":["# Get audio files\n","all_audio_file_path = []\n","for path, subdirs, files in os.walk(audio_dir):\n","    for name in files:\n","        if name.endswith(\".wav\") and not name.startswith(\".\"):\n","            all_audio_file_path.append(os.path.join(path, name))\n","\n","all_audio_file_path = sorted(all_audio_file_path)\n","\n","print(\"-\", len(all_audio_file_path), \"files found in the directory\", audio_dir, ':\\n')"]},{"cell_type":"markdown","metadata":{"id":"ay5j1mBspcaz"},"source":["### In case the ad list needs to be modified, use the list below.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uHSMrIdPpoYu"},"outputs":[],"source":["#all_audio_file_path = []\n","#print(all_audio_file_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z8ps4GcE4eFk"},"outputs":[],"source":["# Initialize an empty DataFrame for all ads\n","all_data = pd.DataFrame()"]},{"cell_type":"markdown","metadata":{"id":"A3um4wUgpsng"},"source":["### additional values"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"69jImfOx4hCI"},"outputs":[],"source":["def calculate_additional_values(y):\n","    duration = librosa.get_duration(y=y)\n","    tempo, _ = librosa.beat.beat_track(y=y)\n","    db_values = librosa.amplitude_to_db(np.abs(librosa.stft(y)), ref=np.max)\n","    avg_db = np.mean(db_values)\n","    min_db = np.min(db_values)\n","    max_db = np.max(db_values)\n","    max_db_value = np.max(db_values)\n","    return duration, tempo, avg_db, min_db, max_db, max_db_value"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EpIplz6wqOYq"},"outputs":[],"source":["def compress_wav_to_mp3(input_wav_path, output_mp3_path, bitrate='192k'):\n","    audio = AudioSegment.from_wav(input_wav_path)\n","    audio.export(output_mp3_path, format='mp3', bitrate=bitrate)\n","\n","def measure_compression_ratio(original_size, compressed_size, original_duration):\n","    compression_ratio = ((original_size - compressed_size) / original_size) * 100\n","    return compression_ratio\n","\n","def measure_compression_ratio_per_second(compression_ratio, original_duration):\n","    compression_ratio_per_second = compression_ratio / original_duration\n","    return compression_ratio_per_second\n","\n","def delete_file(file_path):\n","    \"\"\"\n","    Delete a file if it exists.\n","\n","    Parameters:\n","    - file_path: Path to the file to be deleted.\n","    \"\"\"\n","    if os.path.exists(file_path):\n","        os.remove(file_path)\n","        print(f\"File {file_path} deleted.\")\n","    else:\n","        print(f\"File {file_path} does not exist.\")"]},{"cell_type":"markdown","metadata":{"id":"MNVJCe7tqWYS"},"source":["### Main"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IgEqSfcs4p77"},"outputs":[],"source":["#parser = argparse.ArgumentParser()\n","#parser.add_argument(\"config_file\", help='yaml config file', nargs='?', const='yaml/config_014_butter.yaml', default='yaml/config_014_butter.yaml', type=str)\n","#parser.add_argument(\"audio_dir\", help='audio directory', nargs='?', const='audio_files', default='audio_files', type=str)\n","#parser.add_argument(\"output_csv_file\", help='output csv file', nargs='?', const='dict_all.csv', default='dict_all.csv', type=str)\n","#args =parser.parse_args()\n","\n","if single_output_file:\n","\n","  print(\"audio directory: \", audio_dir)\n","  print(\"output_excel_file: \", output_excel_file)\n","\n","  for idx_file, filename in enumerate(all_audio_file_path):\n","\n","      print(f'###### CURRENT AD: {filename} ######')\n","      print(f'###### - {all_audio_file_path.index(filename)} / {len(all_audio_file_path)} - ######')\n","\n","      # Read signal -------------------------------------\n","      file = AudioFile(filename, verbose=True)\n","\n","      # Pre-processing -----------------------------------------------------------------------------------\n","      if 'Filtering' in data_config:\n","          if data_config['Filtering']['type'] == 'butterworth':\n","              print('- Pre-processing - High-Pass Filtering:', data_config['Filtering'])\n","              freq_filter = data_config['Filtering']['frequency']\n","              Wn = freq_filter/float(file.niquist)\n","              order = data_config['Filtering']['order']\n","              [b,a] = signal.butter(order, Wn, btype='highpass')\n","              # to plot the frequency response\n","              #w, h = signal.freqz(b, a, worN=2000)\n","              #plt.plot((file.sr * 0.5 / np.pi) * w, abs(h))\n","              #plt.show()\n","              file.process_filtering(signal.filtfilt(b, a, file.sig_float))\n","          elif data_config['Filtering']['type'] == 'windowed_sinc':\n","              print('- Pre-processing - High-Pass Filtering:', data_config['Filtering'])\n","              freq_filter = data_config['Filtering']['frequency']\n","              fc = freq_filter / float(file.sr)\n","              roll_off = data_config['Filtering']['roll_off']\n","              b = roll_off / float(file.sr)\n","              N = int(np.ceil((4 / b)))\n","              if not N % 2: N += 1  # Make sure that N is odd.\n","              n = np.arange(N)\n","              # Compute a low-pass filter.\n","              h = np.sinc(2 * fc * (n - (N - 1) / 2.))\n","              w = np.blackman(N)\n","              h = h * w\n","              h = h / np.sum(h)\n","              # Create a high-pass filter from the low-pass filter through spectral inversion.\n","              h = -h\n","              h[(N - 1) / 2] += 1\n","              file.process_filtering(np.convolve(file.sig_float, h))\n","\n","      # Compute Indices -----------------------------------------------------------------------------------\n","      print('- Compute Indices')\n","      ci = data_config['Indices']  # use to simplify the notation\n","      for index_name in ci:  # iterate over the index names (key of dictionary in the yml file)\n","\n","          if index_name == 'Acoustic_Complexity_Index':\n","              print('\\tCompute', index_name)\n","              spectro, _ = compute_spectrogram(file, **ci[index_name]['spectro'])\n","              methodToCall = globals().get(ci[index_name]['function'])\n","              j_bin = int(ci[index_name]['arguments']['j_bin'] * file.sr / ci[index_name]['spectro']['windowHop'])  # transform j_bin in samples\n","              main_value, temporal_values = methodToCall(spectro, j_bin)\n","              file.indices[index_name] = Index(index_name, temporal_values=temporal_values, main_value=main_value)\n","\n","          elif index_name == 'Acoustic_Diversity_Index':\n","              print('\\tCompute', index_name)\n","              methodToCall = globals().get(ci[index_name]['function'])\n","              freq_band_Hz = ci[index_name]['arguments']['max_freq'] / ci[index_name]['arguments']['freq_step']\n","              windowLength = int(file.sr / freq_band_Hz)\n","              spectro, _ = compute_spectrogram(file, windowLength=windowLength, windowHop=windowLength, scale_audio=True, square=False, windowType='hamming', centered=False, normalized=False)\n","              main_value = methodToCall(spectro, freq_band_Hz, **ci[index_name]['arguments'])\n","              file.indices[index_name] = Index(index_name, main_value=main_value)\n","\n","          elif index_name == 'Acoustic_Evenness_Index':\n","              print('\\tCompute', index_name)\n","              methodToCall = globals().get(ci[index_name]['function'])\n","              freq_band_Hz = ci[index_name]['arguments']['max_freq'] / ci[index_name]['arguments']['freq_step']\n","              windowLength = int(file.sr / freq_band_Hz)\n","              spectro, _ = compute_spectrogram(file, windowLength=windowLength, windowHop=windowLength, scale_audio=True, square=False, windowType='hamming', centered=False, normalized=False)\n","              main_value = methodToCall(spectro, freq_band_Hz, **ci[index_name]['arguments'])\n","              file.indices[index_name] = Index(index_name, main_value=main_value)\n","\n","          elif index_name == 'Bio_acoustic_Index':\n","              print('\\tCompute', index_name)\n","              spectro, frequencies = compute_spectrogram(file, **ci[index_name]['spectro'])\n","              methodToCall = globals().get(ci[index_name]['function'])\n","              main_value = methodToCall(spectro, frequencies, **ci[index_name]['arguments'])\n","              file.indices[index_name] = Index(index_name, main_value=main_value)\n","\n","          elif index_name == 'Normalized_Difference_Sound_Index':\n","              print('\\tCompute', index_name)\n","              methodToCall = globals().get(ci[index_name]['function'])\n","              main_value = methodToCall(file, **ci[index_name]['arguments'])\n","              file.indices[index_name] = Index(index_name, main_value=main_value)\n","\n","          elif index_name == 'RMS_energy':\n","              print('\\tCompute', index_name)\n","              methodToCall = globals().get(ci[index_name]['function'])\n","              temporal_values = methodToCall(file, **ci[index_name]['arguments'])\n","              file.indices[index_name] = Index(index_name, temporal_values=temporal_values)\n","\n","          elif index_name == 'Spectral_centroid':\n","              print('\\tCompute', index_name)\n","              spectro, frequencies = compute_spectrogram(file, **ci[index_name]['spectro'])\n","              methodToCall = globals().get(ci[index_name]['function'])\n","              temporal_values = methodToCall(spectro, frequencies)\n","              file.indices[index_name] = Index(index_name, temporal_values=temporal_values)\n","\n","          elif index_name == 'Spectral_Entropy':\n","              print('\\tCompute', index_name)\n","              spectro, _ = compute_spectrogram(file, **ci[index_name]['spectro'])\n","              methodToCall = globals().get(ci[index_name]['function'])\n","              main_value = methodToCall(spectro)\n","              file.indices[index_name] = Index(index_name, main_value=main_value)\n","\n","          elif index_name == 'Temporal_Entropy':\n","              print('\\tCompute', index_name)\n","              methodToCall = globals().get(ci[index_name]['function'])\n","              main_value = methodToCall(file, **ci[index_name]['arguments'])\n","              file.indices[index_name] = Index(index_name, main_value=main_value)\n","\n","          elif index_name == 'ZCR':\n","              print('\\tCompute', index_name)\n","              methodToCall = globals().get(ci[index_name]['function'])\n","              temporal_values = methodToCall(file, **ci[index_name]['arguments'])\n","              file.indices[index_name] = Index(index_name, temporal_values=temporal_values)\n","\n","          elif index_name == 'Wave_SNR':\n","              print('\\tCompute', index_name)\n","              methodToCall = globals().get(ci[index_name]['function'])\n","              values = methodToCall(file, **ci[index_name]['arguments'])\n","              file.indices[index_name] = Index(index_name, values=values)\n","\n","          elif index_name == 'NB_peaks':\n","              print('\\tCompute', index_name)\n","              spectro, frequencies = compute_spectrogram(file, **ci[index_name]['spectro'])\n","              methodToCall = globals().get(ci[index_name]['function'])\n","              main_value = methodToCall(spectro, frequencies, **ci[index_name]['arguments'])\n","              file.indices[index_name] = Index(index_name, main_value=main_value)\n","\n","          elif index_name == 'Acoustic_Diversity_Index_NR': # Acoustic_Diversity_Index with Noise Removed spectrograms\n","              print('\\tCompute', index_name)\n","              methodToCall = globals().get(ci[index_name]['function'])\n","              freq_band_Hz = ci[index_name]['arguments']['max_freq'] / ci[index_name]['arguments']['freq_step']\n","              windowLength = int(file.sr / freq_band_Hz)\n","              spectro,_ = compute_spectrogram(file, windowLength=windowLength, windowHop= windowLength, scale_audio=True, square=False, windowType='hamming', centered=False, normalized= False )\n","              spectro_noise_removed = remove_noiseInSpectro(spectro, **ci[index_name]['remove_noiseInSpectro'])\n","              main_value = methodToCall(spectro_noise_removed, freq_band_Hz, **ci[index_name]['arguments'])\n","              file.indices[index_name] = Index(index_name, main_value=main_value)\n","\n","          elif index_name == 'Acoustic_Evenness_Index_NR': # Acoustic_Evenness_Index with Noise Removed spectrograms\n","              print('\\tCompute', index_name)\n","              methodToCall = globals().get(ci[index_name]['function'])\n","              freq_band_Hz = ci[index_name]['arguments']['max_freq'] / ci[index_name]['arguments']['freq_step']\n","              windowLength = int(file.sr / freq_band_Hz)\n","              spectro,_ = compute_spectrogram(file, windowLength=windowLength, windowHop= windowLength, scale_audio=True, square=False, windowType='hamming', centered=False, normalized= False )\n","              spectro_noise_removed = remove_noiseInSpectro(spectro, **ci[index_name]['remove_noiseInSpectro'])\n","              main_value = methodToCall(spectro_noise_removed, freq_band_Hz, **ci[index_name]['arguments'])\n","              file.indices[index_name] = Index(index_name, main_value=main_value)\n","\n","          elif index_name == 'Bio_acoustic_Index_NR': # Bio_acoustic_Index with Noise Removed spectrograms\n","              print('\\tCompute', index_name)\n","              spectro, frequencies = compute_spectrogram(file, **ci[index_name]['spectro'])\n","              spectro_noise_removed = remove_noiseInSpectro(spectro, **ci[index_name]['remove_noiseInSpectro'])\n","              methodToCall = globals().get(ci[index_name]['function'])\n","              main_value = methodToCall(spectro_noise_removed, frequencies, **ci[index_name]['arguments'])\n","              file.indices[index_name] = Index(index_name, main_value=main_value)\n","\n","          elif index_name == 'Spectral_Entropy_NR': # Spectral_Entropy with Noise Removed spectrograms\n","              print('\\tCompute', index_name)\n","              spectro, _ = compute_spectrogram(file, **ci[index_name]['spectro'])\n","              spectro_noise_removed = remove_noiseInSpectro(spectro, **ci[index_name]['remove_noiseInSpectro'])\n","              methodToCall = globals().get(ci[index_name]['function'])\n","              main_value = methodToCall(spectro_noise_removed)\n","              file.indices[index_name] = Index(index_name, main_value=main_value)\n","\n","      # Output Indices -----------------------------------------------------------------------------------\n","      #if idx_file == 0: # wenn ertse Datei\n","      #    with open(output_csv_file, 'w') as f_object:\n","      #        writer_object = writer(f_object)\n","      #        keys = ['filename']\n","      #        values = [file.file_name]\n","      #        for idx, current_index in file.indices.items():\n","      #            for key, value in current_index.__dict__.items():\n","      #                if key != 'name':\n","      #                    keys.append(idx + '__' + key)\n","      #                    values.append(value)\n","      #        writer_object.writerow(keys)\n","      #        writer_object.writerow(values)\n","      #        f_object.close()\n","      #else: # alles nach der ersten Datei\n","      #    with open(output_csv_file, 'a') as f_object:\n","      #        writer_object = writer(f_object)\n","      #        values = [file.file_name]\n","      #        for idx, current_index in file.indices.items():\n","      #            for key, value in current_index.__dict__.items():\n","      #                if key != 'name':\n","      #                    values.append(value)\n","      #        writer_object.writerow(values)\n","      #        f_object.close()\n","      #print(\"\\n\")\n","      # Create a dictionary to store data for the current file\n","\n","      file_data = {'filename': file.file_name}\n","\n","      for idx, current_index in file.indices.items():\n","          for key, value in current_index.__dict__.items():\n","              if key != 'name':\n","                  file_data[idx + '__' + key] = value\n","\n","      # Calculate additional values\n","      additional_values = calculate_additional_values(file.sig_float)\n","\n","      # compression rate\n","      original_size = os.path.getsize(filename)\n","      audio = AudioSegment.from_wav(filename)\n","      original_duration = audio.duration_seconds\n","      mp3_path = os.path.join(filename.replace(\".wav\", \".mp3\"))\n","      compress_wav_to_mp3(filename, mp3_path)\n","      compressed_size = os.path.getsize(mp3_path)\n","      compression_ratio = measure_compression_ratio(original_size, compressed_size, original_duration)\n","      compression_ratio_per_second = measure_compression_ratio_per_second(compression_ratio, original_duration)\n","      delete_file(mp3_path)\n","\n","      # Append additional values to the file_data dictionary\n","      duration, tempo, avg_db, min_db, max_db, max_db_value = additional_values\n","      file_data['duration'] = duration\n","      file_data['tempo'] = tempo\n","      file_data['avg_db'] = avg_db\n","      file_data['min_db'] = min_db\n","      file_data['max_db'] = max_db\n","      file_data['max_db_value'] = max_db_value\n","      file_data['compression_ratio'] = compression_ratio\n","      file_data['compression_ratio_per_second'] = compression_ratio_per_second\n","\n","      # Append the data for the current file to the DataFrame\n","      all_data = all_data.append(file_data, ignore_index=True)\n","\n","  #To Excel\n","  with pd.ExcelWriter(output_excel_file, engine='openpyxl') as writer:\n","    all_data.to_excel(writer, sheet_name='Acoustic_Indices', index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":765261,"status":"ok","timestamp":1708809758797,"user":{"displayName":"Thorben Schlieffen","userId":"06537630395058694731"},"user_tz":-60},"id":"REsHzfJY4rGb","outputId":"414a3890-bff3-445e-f8ae-da9ca87c79f8"},"outputs":[],"source":["if not single_output_file:\n","\n","  print(\"audio directory: \", audio_dir)\n","  print(\"output_directory: \", output_dir)\n","\n","  for idx_file, filename in enumerate(all_audio_file_path):\n","\n","      ad_name = filename.split('/')[-1]\n","      year_folder_name = filename.split('/')[-2]\n","      #print(ad_name, year_folder_name)\n","\n","      print(f'###### CURRENT AD: {filename} ######')\n","      print(f'###### - {all_audio_file_path.index(filename)} / {len(all_audio_file_path)} - ######')\n","\n","      # Initialize an empty DataFrame for each individual ad\n","      all_data = pd.DataFrame()\n","\n","      # Read signal -------------------------------------\n","      file = AudioFile(filename, verbose=True)\n","\n","      # Pre-processing -----------------------------------------------------------------------------------\n","      if 'Filtering' in data_config:\n","          if data_config['Filtering']['type'] == 'butterworth':\n","              print('- Pre-processing - High-Pass Filtering:', data_config['Filtering'])\n","              freq_filter = data_config['Filtering']['frequency']\n","              Wn = freq_filter/float(file.niquist)\n","              order = data_config['Filtering']['order']\n","              [b,a] = signal.butter(order, Wn, btype='highpass')\n","              # to plot the frequency response\n","              #w, h = signal.freqz(b, a, worN=2000)\n","              #plt.plot((file.sr * 0.5 / np.pi) * w, abs(h))\n","              #plt.show()\n","              file.process_filtering(signal.filtfilt(b, a, file.sig_float))\n","          elif data_config['Filtering']['type'] == 'windowed_sinc':\n","              print('- Pre-processing - High-Pass Filtering:', data_config['Filtering'])\n","              freq_filter = data_config['Filtering']['frequency']\n","              fc = freq_filter / float(file.sr)\n","              roll_off = data_config['Filtering']['roll_off']\n","              b = roll_off / float(file.sr)\n","              N = int(np.ceil((4 / b)))\n","              if not N % 2: N += 1  # Make sure that N is odd.\n","              n = np.arange(N)\n","              # Compute a low-pass filter.\n","              h = np.sinc(2 * fc * (n - (N - 1) / 2.))\n","              w = np.blackman(N)\n","              h = h * w\n","              h = h / np.sum(h)\n","              # Create a high-pass filter from the low-pass filter through spectral inversion.\n","              h = -h\n","              h[(N - 1) / 2] += 1\n","              file.process_filtering(np.convolve(file.sig_float, h))\n","\n","      # Compute Indices -----------------------------------------------------------------------------------\n","      print('- Compute Indices')\n","      ci = data_config['Indices']  # use to simplify the notation\n","      for index_name in ci:  # iterate over the index names (key of dictionary in the yml file)\n","\n","          if index_name == 'Acoustic_Complexity_Index':\n","              print('\\tCompute', index_name)\n","              spectro, _ = compute_spectrogram(file, **ci[index_name]['spectro'])\n","              methodToCall = globals().get(ci[index_name]['function'])\n","              j_bin = int(ci[index_name]['arguments']['j_bin'] * file.sr / ci[index_name]['spectro']['windowHop'])  # transform j_bin in samples\n","              main_value, temporal_values = methodToCall(spectro, j_bin)\n","              file.indices[index_name] = Index(index_name, temporal_values=temporal_values, main_value=main_value)\n","\n","          elif index_name == 'Acoustic_Diversity_Index':\n","              print('\\tCompute', index_name)\n","              methodToCall = globals().get(ci[index_name]['function'])\n","              freq_band_Hz = ci[index_name]['arguments']['max_freq'] / ci[index_name]['arguments']['freq_step']\n","              windowLength = int(file.sr / freq_band_Hz)\n","              spectro, _ = compute_spectrogram(file, windowLength=windowLength, windowHop=windowLength, scale_audio=True, square=False, windowType='hamming', centered=False, normalized=False)\n","              main_value = methodToCall(spectro, freq_band_Hz, **ci[index_name]['arguments'])\n","              file.indices[index_name] = Index(index_name, main_value=main_value)\n","\n","          elif index_name == 'Acoustic_Evenness_Index':\n","              print('\\tCompute', index_name)\n","              methodToCall = globals().get(ci[index_name]['function'])\n","              freq_band_Hz = ci[index_name]['arguments']['max_freq'] / ci[index_name]['arguments']['freq_step']\n","              windowLength = int(file.sr / freq_band_Hz)\n","              spectro, _ = compute_spectrogram(file, windowLength=windowLength, windowHop=windowLength, scale_audio=True, square=False, windowType='hamming', centered=False, normalized=False)\n","              main_value = methodToCall(spectro, freq_band_Hz, **ci[index_name]['arguments'])\n","              file.indices[index_name] = Index(index_name, main_value=main_value)\n","\n","          elif index_name == 'Bio_acoustic_Index':\n","              print('\\tCompute', index_name)\n","              spectro, frequencies = compute_spectrogram(file, **ci[index_name]['spectro'])\n","              methodToCall = globals().get(ci[index_name]['function'])\n","              main_value = methodToCall(spectro, frequencies, **ci[index_name]['arguments'])\n","              file.indices[index_name] = Index(index_name, main_value=main_value)\n","\n","          elif index_name == 'Normalized_Difference_Sound_Index':\n","              print('\\tCompute', index_name)\n","              methodToCall = globals().get(ci[index_name]['function'])\n","              main_value = methodToCall(file, **ci[index_name]['arguments'])\n","              file.indices[index_name] = Index(index_name, main_value=main_value)\n","\n","          elif index_name == 'RMS_energy':\n","              print('\\tCompute', index_name)\n","              methodToCall = globals().get(ci[index_name]['function'])\n","              temporal_values = methodToCall(file, **ci[index_name]['arguments'])\n","              file.indices[index_name] = Index(index_name, temporal_values=temporal_values)\n","\n","          elif index_name == 'Spectral_centroid':\n","              print('\\tCompute', index_name)\n","              spectro, frequencies = compute_spectrogram(file, **ci[index_name]['spectro'])\n","              methodToCall = globals().get(ci[index_name]['function'])\n","              temporal_values = methodToCall(spectro, frequencies)\n","              file.indices[index_name] = Index(index_name, temporal_values=temporal_values)\n","\n","          elif index_name == 'Spectral_Entropy':\n","              print('\\tCompute', index_name)\n","              spectro, _ = compute_spectrogram(file, **ci[index_name]['spectro'])\n","              methodToCall = globals().get(ci[index_name]['function'])\n","              main_value = methodToCall(spectro)\n","              file.indices[index_name] = Index(index_name, main_value=main_value)\n","\n","          elif index_name == 'Temporal_Entropy':\n","              print('\\tCompute', index_name)\n","              methodToCall = globals().get(ci[index_name]['function'])\n","              main_value = methodToCall(file, **ci[index_name]['arguments'])\n","              file.indices[index_name] = Index(index_name, main_value=main_value)\n","\n","          elif index_name == 'ZCR':\n","              print('\\tCompute', index_name)\n","              methodToCall = globals().get(ci[index_name]['function'])\n","              temporal_values = methodToCall(file, **ci[index_name]['arguments'])\n","              file.indices[index_name] = Index(index_name, temporal_values=temporal_values)\n","\n","          elif index_name == 'Wave_SNR':\n","              print('\\tCompute', index_name)\n","              methodToCall = globals().get(ci[index_name]['function'])\n","              values = methodToCall(file, **ci[index_name]['arguments'])\n","              file.indices[index_name] = Index(index_name, values=values)\n","\n","          elif index_name == 'NB_peaks':\n","              print('\\tCompute', index_name)\n","              spectro, frequencies = compute_spectrogram(file, **ci[index_name]['spectro'])\n","              methodToCall = globals().get(ci[index_name]['function'])\n","              main_value = methodToCall(spectro, frequencies, **ci[index_name]['arguments'])\n","              file.indices[index_name] = Index(index_name, main_value=main_value)\n","\n","          elif index_name == 'Acoustic_Diversity_Index_NR': # Acoustic_Diversity_Index with Noise Removed spectrograms\n","              print('\\tCompute', index_name)\n","              methodToCall = globals().get(ci[index_name]['function'])\n","              freq_band_Hz = ci[index_name]['arguments']['max_freq'] / ci[index_name]['arguments']['freq_step']\n","              windowLength = int(file.sr / freq_band_Hz)\n","              spectro,_ = compute_spectrogram(file, windowLength=windowLength, windowHop= windowLength, scale_audio=True, square=False, windowType='hamming', centered=False, normalized= False )\n","              spectro_noise_removed = remove_noiseInSpectro(spectro, **ci[index_name]['remove_noiseInSpectro'])\n","              main_value = methodToCall(spectro_noise_removed, freq_band_Hz, **ci[index_name]['arguments'])\n","              file.indices[index_name] = Index(index_name, main_value=main_value)\n","\n","          elif index_name == 'Acoustic_Evenness_Index_NR': # Acoustic_Evenness_Index with Noise Removed spectrograms\n","              print('\\tCompute', index_name)\n","              methodToCall = globals().get(ci[index_name]['function'])\n","              freq_band_Hz = ci[index_name]['arguments']['max_freq'] / ci[index_name]['arguments']['freq_step']\n","              windowLength = int(file.sr / freq_band_Hz)\n","              spectro,_ = compute_spectrogram(file, windowLength=windowLength, windowHop= windowLength, scale_audio=True, square=False, windowType='hamming', centered=False, normalized= False )\n","              spectro_noise_removed = remove_noiseInSpectro(spectro, **ci[index_name]['remove_noiseInSpectro'])\n","              main_value = methodToCall(spectro_noise_removed, freq_band_Hz, **ci[index_name]['arguments'])\n","              file.indices[index_name] = Index(index_name, main_value=main_value)\n","\n","          elif index_name == 'Bio_acoustic_Index_NR': # Bio_acoustic_Index with Noise Removed spectrograms\n","              print('\\tCompute', index_name)\n","              spectro, frequencies = compute_spectrogram(file, **ci[index_name]['spectro'])\n","              spectro_noise_removed = remove_noiseInSpectro(spectro, **ci[index_name]['remove_noiseInSpectro'])\n","              methodToCall = globals().get(ci[index_name]['function'])\n","              main_value = methodToCall(spectro_noise_removed, frequencies, **ci[index_name]['arguments'])\n","              file.indices[index_name] = Index(index_name, main_value=main_value)\n","\n","          elif index_name == 'Spectral_Entropy_NR': # Spectral_Entropy with Noise Removed spectrograms\n","              print('\\tCompute', index_name)\n","              spectro, _ = compute_spectrogram(file, **ci[index_name]['spectro'])\n","              spectro_noise_removed = remove_noiseInSpectro(spectro, **ci[index_name]['remove_noiseInSpectro'])\n","              methodToCall = globals().get(ci[index_name]['function'])\n","              main_value = methodToCall(spectro_noise_removed)\n","              file.indices[index_name] = Index(index_name, main_value=main_value)\n","\n","      # Output Indices -----------------------------------------------------------------------------------\n","      #if idx_file == 0: # wenn ertse Datei\n","      #    with open(output_csv_file, 'w') as f_object:\n","      #        writer_object = writer(f_object)\n","      #        keys = ['filename']\n","      #        values = [file.file_name]\n","      #        for idx, current_index in file.indices.items():\n","      #            for key, value in current_index.__dict__.items():\n","      #                if key != 'name':\n","      #                    keys.append(idx + '__' + key)\n","      #                    values.append(value)\n","      #        writer_object.writerow(keys)\n","      #        writer_object.writerow(values)\n","      #        f_object.close()\n","      #else: # alles nach der ersten Datei\n","      #    with open(output_csv_file, 'a') as f_object:\n","      #        writer_object = writer(f_object)\n","      #        values = [file.file_name]\n","      #        for idx, current_index in file.indices.items():\n","      #            for key, value in current_index.__dict__.items():\n","      #                if key != 'name':\n","      #                    values.append(value)\n","      #        writer_object.writerow(values)\n","      #        f_object.close()\n","      #print(\"\\n\")\n","      # Create a dictionary to store data for the current file\n","\n","      file_data = {'filename': file.file_name}\n","\n","      for idx, current_index in file.indices.items():\n","          for key, value in current_index.__dict__.items():\n","              if key != 'name':\n","                  file_data[idx + '__' + key] = value\n","\n","      # Calculate additional values\n","      additional_values = calculate_additional_values(file.sig_float)\n","\n","      # compression rate\n","      original_size = os.path.getsize(filename)\n","      audio = AudioSegment.from_wav(filename)\n","      original_duration = audio.duration_seconds\n","      mp3_path = os.path.join(filename.replace(\".wav\", \".mp3\"))\n","      compress_wav_to_mp3(filename, mp3_path)\n","      compressed_size = os.path.getsize(mp3_path)\n","      compression_ratio = measure_compression_ratio(original_size, compressed_size, original_duration)\n","      compression_ratio_per_second = measure_compression_ratio_per_second(compression_ratio, original_duration)\n","      delete_file(mp3_path)\n","\n","      # Append additional values to the file_data dictionary\n","      duration, tempo, avg_db, min_db, max_db, max_db_value = additional_values\n","      file_data['duration'] = duration\n","      file_data['tempo'] = tempo\n","      file_data['avg_db'] = avg_db\n","      file_data['min_db'] = min_db\n","      file_data['max_db'] = max_db\n","      file_data['max_db_value'] = max_db_value\n","      file_data['compression_ratio'] = compression_ratio\n","      file_data['compression_ratio_per_second'] = compression_ratio_per_second\n","\n","      # Append the data for the current file to the DataFrame\n","      all_data = all_data.append(file_data, ignore_index=True)\n","\n","      output_xlsx_file = os.path.join(output_dir, year_folder_name[0:11] + \"/\" + ad_name[0:7] + \"xlsx\")\n","\n","      print(output_xlsx_file)\n","\n","      #To Excel\n","      #with pd.ExcelWriter(output_xlsx_file, engine='openpyxl', mode='a', if_sheet_exists='overlay') as writer:\n","      #  all_data.to_excel(writer, sheet_name='Acoustic_Indices', index=False)\n","      try:\n","          with pd.ExcelWriter(output_xlsx_file, engine='openpyxl', mode='a', if_sheet_exists='overlay') as writer:\n","              all_data.to_excel(writer, sheet_name='Acoustic_Indices', index=False)\n","\n","      except Exception as e:\n","          print(f\"Error: {e}\")\n","\n","          # Versuche, die Datei zu löschen (falls sie existiert)\n","          if os.path.exists(output_xlsx_file):\n","              os.remove(output_xlsx_file)\n","              print(f\"File '{output_xlsx_file}' deleted.\")\n","\n","          # Erstelle eine neue leere Datei\n","          with pd.ExcelWriter(output_xlsx_file, engine='openpyxl', mode='w') as writer:\n","              all_data.to_excel(writer, sheet_name='Acoustic_Indices', index=False)\n","              print(f\"New file '{output_xlsx_file}' created and written.\")"]}],"metadata":{"accelerator":"TPU","colab":{"provenance":[{"file_id":"1e6gtuF1-nQAflcMlP0kbDHvSxYIb0aG3","timestamp":1705574462824},{"file_id":"https://github.com/pyannote/pyannote-audio/blob/develop/tutorials/intro.ipynb","timestamp":1703704769456}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.3"},"widgets":{"application/vnd.jupyter.widget-state+json":{"2d52b4a045c14f44903b010261527ed6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"LabelModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_35ae3d30cf45414a98a606b2c9ba40be","placeholder":"​","style":"IPY_MODEL_504a095f33214bd4b36646d088b49e5e","value":"Token is valid (permission: write)."}},"34222225ba97456694d7298c879b4b19":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"LabelModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a5865557a7474b0ebeed270b592f38ac","placeholder":"​","style":"IPY_MODEL_45d6c82b2ec74893ae2b5a86eeb1daff","value":"Your token has been saved to /root/.cache/huggingface/token"}},"35ae3d30cf45414a98a606b2c9ba40be":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3ff01220d0d449f99775a49369910f63":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"LabelModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a9bf397a62944000ae68a482a8ae20c1","placeholder":"​","style":"IPY_MODEL_aa5d8610fb084fa59991f262fed05bad","value":"Your token has been saved in your configured git credential helpers (store)."}},"45d6c82b2ec74893ae2b5a86eeb1daff":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4cbb11e0ee474f2eaf7ffa7e3583e9f8":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"504a095f33214bd4b36646d088b49e5e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6c2729e3f4b84d349a112da2ccf77b25":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":"center","align_self":null,"border":null,"bottom":null,"display":"flex","flex":null,"flex_flow":"column","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"50%"}},"8a7f1963cc7848a48b2c8a5ab1ff17a3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a5865557a7474b0ebeed270b592f38ac":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a9bf397a62944000ae68a482a8ae20c1":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"aa5d8610fb084fa59991f262fed05bad":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c9ff155a579140e2bb19bfbadc4e032f":{"model_module":"@jupyter-widgets/output","model_module_version":"1.0.0","model_name":"OutputModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/output","_model_module_version":"1.0.0","_model_name":"OutputModel","_view_count":null,"_view_module":"@jupyter-widgets/output","_view_module_version":"1.0.0","_view_name":"OutputView","layout":"IPY_MODEL_4cbb11e0ee474f2eaf7ffa7e3583e9f8","msg_id":"","outputs":[{"data":{"text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">segmentation         <span style=\"color: #729c1f; text-decoration-color: #729c1f\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span> <span style=\"color: #800080; text-decoration-color: #800080\">100%</span> <span style=\"color: #808000; text-decoration-color: #808000\">0:00:00</span>\nspeaker_counting     <span style=\"color: #729c1f; text-decoration-color: #729c1f\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span> <span style=\"color: #800080; text-decoration-color: #800080\">100%</span> <span style=\"color: #808000; text-decoration-color: #808000\">0:00:00</span>\nembeddings           <span style=\"color: #729c1f; text-decoration-color: #729c1f\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span> <span style=\"color: #800080; text-decoration-color: #800080\">100%</span> <span style=\"color: #808000; text-decoration-color: #808000\">0:00:08</span>\ndiscrete_diarization <span style=\"color: #729c1f; text-decoration-color: #729c1f\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span> <span style=\"color: #800080; text-decoration-color: #800080\">100%</span> <span style=\"color: #808000; text-decoration-color: #808000\">0:00:00</span>\n</pre>\n","text/plain":"segmentation         \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m100%\u001b[0m \u001b[33m0:00:00\u001b[0m\nspeaker_counting     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m100%\u001b[0m \u001b[33m0:00:00\u001b[0m\nembeddings           \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m100%\u001b[0m \u001b[33m0:00:08\u001b[0m\ndiscrete_diarization \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m100%\u001b[0m \u001b[33m0:00:00\u001b[0m\n"},"metadata":{},"output_type":"display_data"}]}},"d0647072ab8044c7a1ff53b9e01f7189":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"VBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"VBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"VBoxView","box_style":"","children":["IPY_MODEL_2d52b4a045c14f44903b010261527ed6","IPY_MODEL_3ff01220d0d449f99775a49369910f63","IPY_MODEL_34222225ba97456694d7298c879b4b19","IPY_MODEL_d9bce56fe264426b85e729cf46a3973f"],"layout":"IPY_MODEL_6c2729e3f4b84d349a112da2ccf77b25"}},"d9bce56fe264426b85e729cf46a3973f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"LabelModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e9be5a21ad9046d181c258ea1487de5c","placeholder":"​","style":"IPY_MODEL_8a7f1963cc7848a48b2c8a5ab1ff17a3","value":"Login successful"}},"e9be5a21ad9046d181c258ea1487de5c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}}}},"nbformat":4,"nbformat_minor":0}
